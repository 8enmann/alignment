<!DOCTYPE html><html xmlns:cc="http://creativecommons.org/ns#"><head prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb# medium-com: http://ogp.me/ns/fb/medium-com#"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0, viewport-fit=contain"><title>Efficient and safely scalable – AI Alignment</title><link rel="canonical" href="https://ai-alignment.com/efficient-and-safely-scalable-8218fa8a871f"><meta name="title" content="Efficient and safely scalable – AI Alignment"><meta name="referrer" content="always"><meta name="description" content="Precisely defining the goal of AI control research seems quite difficult. This post gives preliminary definitions of safe scalability and efficiency for AI control protocols, taking a step towards…"><meta name="theme-color" content="#000000"><meta property="og:title" content="Efficient and safely scalable – AI Alignment"><meta property="twitter:title" content="Efficient and safely scalable"><meta property="og:url" content="https://ai-alignment.com/efficient-and-safely-scalable-8218fa8a871f"><meta property="fb:app_id" content="542599432471018"><meta property="og:description" content="A precise but overambitious goal for AI control research."><meta name="twitter:description" content="A precise but overambitious goal for AI control research."><link rel="author" href="https://ai-alignment.com/@paulfchristiano"><meta name="author" content="Paul Christiano"><meta property="og:type" content="article"><meta name="twitter:card" content="summary"><meta property="article:publisher" content="https://www.facebook.com/medium"><meta property="article:author" content="1167284919"><meta name="robots" content="index, follow"><meta property="article:published_time" content="2016-03-24T01:47:00.844Z"><meta name="twitter:site" content="@Medium"><meta property="og:site_name" content="AI Alignment"><meta name="twitter:label1" value="Reading time"><meta name="twitter:data1" value="15 min read"><meta name="twitter:app:name:iphone" content="Medium"><meta name="twitter:app:id:iphone" content="828256236"><meta name="twitter:app:url:iphone" content="medium://p/8218fa8a871f"><meta property="al:ios:app_name" content="Medium"><meta property="al:ios:app_store_id" content="828256236"><meta property="al:android:package" content="com.medium.reader"><meta property="al:android:app_name" content="Medium"><meta property="al:ios:url" content="medium://p/8218fa8a871f"><meta property="al:android:url" content="medium://p/8218fa8a871f"><meta property="al:web:url" content="https://ai-alignment.com/efficient-and-safely-scalable-8218fa8a871f"><link rel="search" type="application/opensearchdescription+xml" title="Medium" href="/osd.xml"><link rel="alternate" href="android-app://com.medium.reader/https/medium.com/p/8218fa8a871f"><script async="" src="https://cdn.branch.io/branch-latest.min.js"></script><script type="application/ld+json">{"@context":"http://schema.org","@type":"NewsArticle","image":{"@type":"ImageObject","width":545,"height":106,"url":"https://cdn-images-1.medium.com/max/545/1*OMF3fSqH8t4xBJ9-6oZDZw.png"},"url":"https://ai-alignment.com/efficient-and-safely-scalable-8218fa8a871f","dateCreated":"2016-03-24T01:47:00.844Z","datePublished":"2016-03-24T01:47:00.844Z","dateModified":"2018-03-14T23:29:35.653Z","headline":"Efficient and safely scalable","name":"Efficient and safely scalable","articleId":"8218fa8a871f","thumbnailUrl":"https://cdn-images-1.medium.com/max/545/1*OMF3fSqH8t4xBJ9-6oZDZw.png","keywords":["Publication:ai-control","LockedPostSource:0","Elevated:false","LayerCake:0"],"author":{"@type":"Person","name":"Paul Christiano","url":"https://ai-alignment.com/@paulfchristiano"},"creator":["Paul Christiano"],"publisher":{"@type":"Organization","name":"AI Alignment","url":"https://ai-alignment.com","logo":{"@type":"ImageObject","width":308,"height":60,"url":"https://cdn-images-1.medium.com/max/308/1*OMF3fSqH8t4xBJ9-6oZDZw.png"}},"mainEntityOfPage":"https://ai-alignment.com/efficient-and-safely-scalable-8218fa8a871f"}</script><meta name="parsely-link" content="https://ai-alignment.com/efficient-and-safely-scalable-8218fa8a871f"><link rel="stylesheet" href="https://cdn-static-1.medium.com/_/fp/css/main-branding-base.sMRbh_65n82B91860QdvTg.css"><script>!function(n,e){var t,o,i,c=[],f={passive:!0,capture:!0},r=new Date,a="pointerup",u="pointercancel";function p(n,c){t||(t=c,o=n,i=new Date,w(e),s())}function s(){o>=0&&o<i-r&&(c.forEach(function(n){n(o,t)}),c=[])}function l(t){if(t.cancelable){var o=(t.timeStamp>1e12?new Date:performance.now())-t.timeStamp;"pointerdown"==t.type?function(t,o){function i(){p(t,o),r()}function c(){r()}function r(){e(a,i,f),e(u,c,f)}n(a,i,f),n(u,c,f)}(o,t):p(o,t)}}function w(n){["click","mousedown","keydown","touchstart","pointerdown"].forEach(function(e){n(e,l,f)})}w(n),self.perfMetrics=self.perfMetrics||{},self.perfMetrics.onFirstInputDelay=function(n){c.push(n),s()}}(addEventListener,removeEventListener);</script><script>if (window.top !== window.self) window.top.location = window.self.location.href;var OB_startTime = new Date().getTime(); var OB_loadErrors = []; function _onerror(e) { OB_loadErrors.push(e) }; if (document.addEventListener) document.addEventListener("error", _onerror, true); else if (document.attachEvent) document.attachEvent("onerror", _onerror); function _asyncScript(u) {var d = document, f = d.getElementsByTagName("script")[0], s = d.createElement("script"); s.type = "text/javascript"; s.async = true; s.src = u; f.parentNode.insertBefore(s, f);}function _asyncStyles(u) {var d = document, f = d.getElementsByTagName("script")[0], s = d.createElement("link"); s.rel = "stylesheet"; s.href = u; f.parentNode.insertBefore(s, f); return s}(new Image()).src = "/_/stat?event=pixel.load&origin=" + encodeURIComponent(location.origin);</script><script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date; ga("create", "UA-24232453-2", "auto", {"allowLinker": true, "legacyCookieDomain": window.location.hostname}); ga("send", "pageview");</script><script async="" src="https://www.google-analytics.com/analytics.js"></script><script>(function () {var height = window.innerHeight || document.documentElement.clientHeight || document.body.clientHeight; var width = window.innerWidth || document.documentElement.clientWidth || document.body.clientWidth; document.write("<style>section.section-image--fullBleed.is-backgrounded {padding-top: " + Math.round(1.1 * height) + "px;}section.section-image--fullScreen.is-backgrounded, section.section-image--coverFade.is-backgrounded {min-height: " + height + "px; padding-top: " + Math.round(0.5 * height) + "px;}.u-height100vh {height: " + height + "px !important;}.u-height110vh {height: " + Math.round(1.1 * height) + "px !important;}.u-minHeight100vh {min-height: " + height + "px !important;}.u-maxHeight100vh {max-height: " + height + "px !important;}section.section-image--coverFade {height: " + height + "px;}.section-aspectRatioViewportPlaceholder, .section-aspectRatioViewportCropPlaceholder {max-height: " + height + "px;}.section-aspectRatioViewportBottomSpacer, .section-aspectRatioViewportBottomPlaceholder {max-height: " + Math.round(0.5 * height) + "px;}.zoomable:before {top: " + (-1 * height) + "px; left: " + (-1 * width) + "px; padding: " + height + "px " + width + "px;}</style>");})()</script><style>section.section-image--fullBleed.is-backgrounded {padding-top: 660px;}section.section-image--fullScreen.is-backgrounded, section.section-image--coverFade.is-backgrounded {min-height: 600px; padding-top: 300px;}.u-height100vh {height: 600px !important;}.u-height110vh {height: 660px !important;}.u-minHeight100vh {min-height: 600px !important;}.u-maxHeight100vh {max-height: 600px !important;}section.section-image--coverFade {height: 600px;}.section-aspectRatioViewportPlaceholder, .section-aspectRatioViewportCropPlaceholder {max-height: 600px;}.section-aspectRatioViewportBottomSpacer, .section-aspectRatioViewportBottomPlaceholder {max-height: 300px;}.zoomable:before {top: -600px; left: -800px; padding: 600px 800px;}</style><!--[if lt IE 9]><script charset="UTF-8" src="https://cdn-static-1.medium.com/_/fp/js/shiv.RI2ePTZ5gFmMgLzG5bEVAA.js"></script><![endif]--><link rel="icon" href="https://cdn-images-1.medium.com/fit/c/128/128/1*cciPf4CUXd_Zyux0Jg0yBQ.png" class="js-favicon"><link rel="apple-touch-icon" sizes="152x152" href="https://cdn-images-1.medium.com/fit/c/152/152/1*N56Qc5-aHTcfGff0scntKQ.png"><link rel="apple-touch-icon" sizes="120x120" href="https://cdn-images-1.medium.com/fit/c/120/120/1*N56Qc5-aHTcfGff0scntKQ.png"><link rel="apple-touch-icon" sizes="76x76" href="https://cdn-images-1.medium.com/fit/c/76/76/1*N56Qc5-aHTcfGff0scntKQ.png"><link rel="apple-touch-icon" sizes="60x60" href="https://cdn-images-1.medium.com/fit/c/60/60/1*N56Qc5-aHTcfGff0scntKQ.png"><link rel="mask-icon" href="https://cdn-static-1.medium.com/_/fp/icons/monogram-mask.KPLCSFEZviQN0jQ7veN2RQ.svg" color="#171717"><meta property="og:image" content=""><style type="text/css">.metabar,.u-fixed,footer { display: none}</style></head><body itemscope="" class="postShowScreen browser-chrome os-mac v-glyph v-glyph--m2 is-js is-resizing" data-action-scope="_actionscope_0"><script>document.body.className = document.body.className.replace(/(^|\s)is-noJs(\s|$)/, "$1is-js$2")</script><div class="site-main surface-container" id="container"><div class="butterBar butterBar--error" data-action-scope="_actionscope_1"></div><div class="surface" id="_obv.shell._surface_1557467314286" style="display: block; visibility: visible;"><div class="screenContent surface-content is-supplementalPostContentLoaded" data-used="true" data-action-scope="_actionscope_2"><canvas class="canvas-renderer" width="800" height="600"></canvas><div class="container u-maxWidth740 u-xs-margin0 notesPositionContainer js-notesPositionContainer"><div class="notesMarkers" data-action-scope="_actionscope_4"></div></div><div class="metabar u-clearfix u-boxShadow4px12pxBlackLightest u-fixed u-backgroundTransparentWhiteDarkest u-xs-sizeFullViewportWidth js-metabar"><div class="branch-journeys-top"></div><div class="js-metabarMiddle metabar-inner u-marginAuto u-maxWidth1032 u-flexCenter u-justifyContentSpaceBetween u-height65 u-xs-height56 u-paddingHorizontal20"><div class="metabar-block u-flex1 u-flexCenter"><div class="u-xs-hide js-metabarLogoLeft"><a href="https://medium.com/" data-log-event="home" class="siteNav-logo u-fillTransparentBlackDarker u-flex0 u-flexCenter u-paddingTop0"><span class="svgIcon svgIcon--logoMonogram svgIcon--45px is-flushLeft u-flex0 u-flexCenter u-paddingTop0"><svg class="svgIcon-use" width="45" height="45"><path d="M5 40V5h35v35H5zm8.56-12.627c0 .555-.027.687-.318 1.03l-2.457 2.985v.396h6.974v-.396l-2.456-2.985c-.291-.343-.344-.502-.344-1.03V18.42l6.127 13.364h.714l5.256-13.364v10.644c0 .29 0 .342-.185.528l-1.848 1.796v.396h9.19v-.396l-1.822-1.796c-.184-.186-.21-.238-.21-.528V15.937c0-.291.026-.344.21-.528l1.823-1.797v-.396h-6.471l-4.622 11.542-5.203-11.542h-6.79v.396l2.14 2.64c.239.292.291.37.291.768v10.353z"></path></svg></span><span class="u-textScreenReader">Homepage</span></a></div><div class="u-xs-show js-metabarLogoLeft"><a href="https://medium.com/" data-log-event="home" class="siteNav-logo u-fillTransparentBlackDarker u-flex0 u-flexCenter u-paddingTop0"><span class="svgIcon svgIcon--logoMonogram svgIcon--45px is-flushLeft u-flex0 u-flexCenter u-paddingTop0"><svg class="svgIcon-use" width="45" height="45"><path d="M5 40V5h35v35H5zm8.56-12.627c0 .555-.027.687-.318 1.03l-2.457 2.985v.396h6.974v-.396l-2.456-2.985c-.291-.343-.344-.502-.344-1.03V18.42l6.127 13.364h.714l5.256-13.364v10.644c0 .29 0 .342-.185.528l-1.848 1.796v.396h9.19v-.396l-1.822-1.796c-.184-.186-.21-.238-.21-.528V15.937c0-.291.026-.344.21-.528l1.823-1.797v-.396h-6.471l-4.622 11.542-5.203-11.542h-6.79v.396l2.14 2.64c.239.292.291.37.291.768v10.353z"></path></svg></span><span class="u-textScreenReader">Homepage</span></a></div><div class="u-flexCenter u-height65 u-xs-height56"><span class="u-inlineBlock u-height28 u-xs-height24 u-verticalAlignTop u-marginRight20 u-marginLeft15 u-borderRightLighter"></span></div><div class="u-flexCenter u-height65 u-xs-height56 u-marginRight18"><div class="u-xs-show"><a class="link u-baseColor--link avatar avatar--roundedRectangle" href="https://ai-alignment.com?source=avatar-lo_JuU2p2EvGepj-624d886c4aa4" title="Go to AI Alignment" aria-label="Go to AI Alignment" data-action-source="avatar-lo_JuU2p2EvGepj-624d886c4aa4" data-collection-slug="ai-control"><img src="https://cdn-images-1.medium.com/fit/c/32/32/1*N56Qc5-aHTcfGff0scntKQ.png" class="avatar-image avatar-image--icon" alt="AI Alignment"></a></div><div class="u-xs-hide"><a href="https://ai-alignment.com?source=logo-lo_JuU2p2EvGepj---624d886c4aa4" class="u-flexCenter js-collectionLogoOrName"><span class="u-noWrapWithEllipsis u-maxWidth1032 u-uiTextBold u-fontSize26 u-textColorDarker">AI Alignment</span></a></div></div></div><div class="metabar-block u-flex0 u-flexCenter"><div class="u-flexCenter u-height65 u-xs-height56"><div class="buttonSet buttonSet--wide u-lineHeightInherit"><a class="button button--primary button--chromeless u-accentColor--buttonNormal is-inSiteNavBar u-xs-hide js-signInButton" href="https://medium.com/m/signin?redirect=https%3A%2F%2Fai-alignment.com%2Fefficient-and-safely-scalable-8218fa8a871f&amp;source=--------------------------nav_reg&amp;operation=login" data-action="sign-in-prompt" data-redirect="https://ai-alignment.com/efficient-and-safely-scalable-8218fa8a871f" data-action-source="--------------------------nav_reg">Sign in</a><a class="button button--primary button--withChrome u-accentColor--buttonNormal is-inSiteNavBar js-signUpButton" href="https://medium.com/m/signin?redirect=https%3A%2F%2Fai-alignment.com%2Fefficient-and-safely-scalable-8218fa8a871f&amp;source=--------------------------nav_reg&amp;operation=register" data-action="sign-up-prompt" data-redirect="https://ai-alignment.com/efficient-and-safely-scalable-8218fa8a871f" data-action-source="--------------------------nav_reg">Get started</a></div></div></div></div></div><div class="metabar metabar--spacer js-metabarSpacer u-height65 u-xs-height56"></div><main role="main"><article class=" u-minHeight100vhOffset65 u-overflowHidden postArticle postArticle--full is-withAccentColors u-marginBottom40" lang="en"><div class="postArticle-content js-postField js-notesSource js-trackPostScrolls" data-post-id="8218fa8a871f" data-source="post_page" data-collection-id="624d886c4aa4" data-tracking-context="postPage" data-scroll="native"><section name="c867" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h1 name="531d" id="531d" class="graf graf--h3 graf--leading graf--title">Efficient and safely&nbsp;scalable</h1><div class="uiScale uiScale-ui--regular uiScale-caption--regular u-flexCenter u-marginVertical24 u-fontSize15 js-postMetaLockup"><div class="u-flex0"><a class="link u-baseColor--link avatar" href="https://ai-alignment.com/@paulfchristiano?source=post_header_lockup" data-action="show-user-card" data-action-source="post_header_lockup" data-action-value="57f1a655a613" data-action-type="hover" data-user-id="57f1a655a613" data-collection-slug="ai-control" dir="auto"><img src="https://cdn-images-1.medium.com/fit/c/50/50/1*BNjZCuQuRfIgcXCBMipuBw.jpeg" class="avatar-image u-size50x50" alt="Go to the profile of Paul Christiano"></a></div><div class="u-flex1 u-paddingLeft15 u-overflowHidden"><div class="u-paddingBottom3"><a class="ds-link ds-link--styleSubtle ui-captionStrong u-inlineBlock link link--darken link--darker" href="https://ai-alignment.com/@paulfchristiano" data-action="show-user-card" data-action-value="57f1a655a613" data-action-type="hover" data-user-id="57f1a655a613" data-collection-slug="ai-control" dir="auto">Paul Christiano</a><span class="followState js-followState" data-user-id="57f1a655a613"><button class="button button--smallest u-noUserSelect button--withChrome u-baseColor--buttonNormal button--withHover button--unblock js-unblockButton u-marginLeft10 u-xs-hide" data-action="sign-up-prompt" data-sign-in-action="toggle-block-user" data-requires-token="true" data-redirect="https://ai-alignment.com/efficient-and-safely-scalable-8218fa8a871f" data-action-source="post_header_lockup"><span class="button-label  button-defaultState">Blocked</span><span class="button-label button-hoverState">Unblock</span></button><button class="button button--primary button--smallest button--dark u-noUserSelect button--withChrome u-accentColor--buttonDark button--follow js-followButton u-marginLeft10 u-xs-hide" data-action="sign-up-prompt" data-sign-in-action="toggle-subscribe-user" data-requires-token="true" data-redirect="https://medium.com/_/subscribe/user/57f1a655a613" data-action-source="post_header_lockup-57f1a655a613-------------------------follow_byline"><span class="button-label  button-defaultState js-buttonLabel">Follow</span><span class="button-label button-activeState">Following</span></button></span></div><div class="ui-caption u-noWrapWithEllipsis js-testPostMetaInlineSupplemental"><time datetime="2016-03-24T01:47:00.844Z">Mar 23, 2016</time><span class="middotDivider u-fontSize12"></span><span class="readingTime" title="15 min read"></span></div></div></div><p name="30e4" id="30e4" class="graf graf--p graf-after--h3">Precisely defining the goal of AI control research seems quite difficult. This post gives preliminary definitions of <strong class="markup--strong markup--p-strong">safe scalability</strong> and <strong class="markup--strong markup--p-strong">efficiency</strong> for AI control protocols, taking a step towards formalization. Roughly, these properties say that “using better machine learning primitives results in better systems” and “the control scheme does not impose significant overhead.”</p><p name="b32d" id="b32d" class="graf graf--p graf-after--p">I think these properties are probably sufficient conditions for success, but they are also probably too ambitious to be realistic goals. I discuss a few possible ways to weaken these definitions.</p><p name="12bf" id="12bf" class="graf graf--p graf-after--p">Both scalability and efficiency are defined with respect to a preference order ≻ᴰ which tells us when one algorithm is “better” another on some distribution D, according to the user’s preferences. I won’t offer any precise definition of ≻ᴰ, but I’ll discuss a few informal candidates.</p><h4 name="2794" id="2794" class="graf graf--h4 graf-after--p">Motivation</h4><p name="caf0" id="caf0" class="graf graf--p graf-after--h4">I’m interested in defining alignment formally for at least three reasons:</p><ul class="postList"><li name="89ca" id="89ca" class="graf graf--li graf-after--p">Having a precise goal makes it easier to do good and well-targeted research. The AI control problem would feel much easier to me (both to work on and to talk to others about) if there were a precise, satisfactory, and achievable goal.</li><li name="0b19" id="0b19" class="graf graf--li graf-after--li">A precise definition of alignment might be helpful when analyzing AI control schemes. For example, <a href="https://medium.com/ai-control/alba-an-explicit-proposal-for-aligned-ai-17a55f60bbcf" data-href="https://medium.com/ai-control/alba-an-explicit-proposal-for-aligned-ai-17a55f60bbcf" class="markup--anchor markup--li-anchor" target="_blank">the analysis of ALBA</a> calls for maintaining alignment as an inductive invariant as the agent becomes more powerful. Right now, there is little hope of making that argument formal.</li><li name="91e1" id="91e1" class="graf graf--li graf-after--li">Trying to formalize alignment may shed light on what the key difficulties are, what assumptions are likely to be necessary, and so on. Trying to pin down slippery concepts is often a good idea&nbsp;.</li></ul><h3 name="14b9" id="14b9" class="graf graf--h3 graf-after--li">Definitions</h3><h4 name="fca7" id="fca7" class="graf graf--h4 graf-after--h3">What is a control protocol?</h4><p name="291d" id="291d" class="graf graf--p graf-after--h4">Our AI control protocols will use machine learning primitives as building blocks, and construct a (hopefully aligned) AI out of them.</p><p name="3894" id="3894" class="graf graf--p graf-after--p">To instantiate a control protocol Aʟɪɢɴ, we provide some set of learning primitives that are required by the protocol. Aʟɪɢɴ then instantiates any number of copies of each of those primitives. Aʟɪɢɴ may choose what inputs to provide to those instances, and may use their outputs however it likes. Aʟɪɢɴ may also interact with the user arbitrarily.</p><p name="8c2d" id="8c2d" class="graf graf--p graf-after--p">For simplicity, throughout the post we will assume that Aʟɪɢɴ is built from an RL algorithm, and write Aʟɪɢɴ(Aᴿᴸ) for the algorithm obtained by using Aᴿᴸ. Note that Aʟɪɢɴ can instantiate any number of distinct instances of Aᴿᴸ, can provide each of them distinct rewards, and so on.</p><p name="61d5" id="61d5" class="graf graf--p graf-after--p">All of our definitions can be easily extended to any set of machine learning primitives, as long as we can define what it means for one implementation of a primitive to “outperform” another on a given distribution. I think that the definitions are most interesting when we can efficiently test whether one implementation outperforms another, and amongst such primitives RL is essentially universal (since we can use the test itself as a reward function).</p><h4 name="4318" id="4318" class="graf graf--h4 graf-after--p">Betterness</h4><p name="a066" id="a066" class="graf graf--p graf-after--h4">What does it mean for one algorithm to be better than another?</p><p name="20f0" id="20f0" class="graf graf--p graf-after--p">We won’t answer that question. Instead, we take as given a family of preorders ≺ᴰ indexed by distributions D. These orders define when one program “outperforms” another on the distribution D, according to the user’s preferences.</p><p name="297f" id="297f" class="graf graf--p graf-after--p">Intuitively, we can imagine some (unobserved) utility function U characterizing the user’s preferences. U takes as input an (<em class="markup--em markup--p-em">x</em>, <em class="markup--em markup--p-em">y</em>) pair, and outputs a real number reflecting how good it is, according to the user’s preferences, for a program to output <em class="markup--em markup--p-em">y</em> given input <em class="markup--em markup--p-em">x</em>.</p><p name="8f6d" id="8f6d" class="graf graf--p graf-after--p">Then we could define:</p><ul class="postList"><li name="ae34" id="ae34" class="graf graf--li graf-after--p">A ≼ᴰ B ⟺ 𝔼[U(<em class="markup--em markup--li-em">x, </em>A(<em class="markup--em markup--li-em">x</em>))] ≤ 𝔼[U(<em class="markup--em markup--li-em">x</em>,<em class="markup--em markup--li-em"> </em>B(<em class="markup--em markup--li-em">x</em>))],</li></ul><p name="bb87" id="bb87" class="graf graf--p graf-after--li">where the expectations are taken over <em class="markup--em markup--p-em">x</em> drawn from D. We could also strengthen the condition by allowing A to score nearly as well on average over many episodes, or by requiring B to do at least as well as A for every utility function in some big set 𝒰. These changes make ≺ᴰ a preorder rather than a total order.</p><p name="c27a" id="c27a" class="graf graf--p graf-after--p">We’ll give a range of more plausible definitions in the final section, but this is a good concrete example to have in mind.</p><h4 name="5254" id="5254" class="graf graf--h4 graf-after--p">Safely scalable and efficient</h4><p name="9d36" id="9d36" class="graf graf--p graf-after--h4">Intuitively, a control scheme is <em class="markup--em markup--p-em">safely scalable </em>if using better RL algorithms never results in worse performance. A control scheme is <em class="markup--em markup--p-em">efficient</em> if it can realize any level of performance with low overhead.</p><ul class="postList"><li name="d3f2" id="d3f2" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Safely scalable (with respect to </strong>≻ᴰ). Given any Aᴿᴸ, Bᴿᴸ, and the ability to sample from some distribution D with Aʟɪɢɴ(Aᴿᴸ) ≻ᴰ Aʟɪɢɴ(Bᴿᴸ), we can sample from a distribution Dᴿᴸ over RL episodes on which Aᴿᴸ outperforms Bᴿᴸ.</li><li name="dc13" id="dc13" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Efficient (with respect to </strong>≻ᴰ). Given any B, we can implement an RL agent Bᴿᴸ such that, for every distribution D from which we can sample, Aʟɪɢɴ(Bᴿᴸ) ⪰ᴰ B.</li></ul><p name="1877" id="1877" class="graf graf--p graf-after--li">Efficiency is quantified by how much more time Aʟɪɢɴ(Bᴿᴸ) takes than B itself, and how often it queries the user. I’ll say that a scheme is “<strong class="markup--strong markup--p-strong">asymptotically efficient</strong>” if the difference in running times, and the number of queries to the user, is sublinear in the number of episodes T.</p><p name="3796" id="3796" class="graf graf--p graf-after--p">In order to be achievable this definition probably needs to be weakened. I think the most plausible way to weaken it is to make additional assumptions about the agent B in <strong class="markup--strong markup--p-strong">efficiency</strong>. For example, we could focus our attention on a particular approach to building AI systems, and assume that B is the kind of agent that might be produced by that approach. Particularly interesting are structural assumptions about how B itself is built out of the same building blocks that are available to Aʟɪɢɴ.</p><h4 name="770f" id="770f" class="graf graf--h4 graf-after--p">Hard to&nbsp;beat</h4><p name="2539" id="2539" class="graf graf--p graf-after--h4">Together efficiency and safe scalability imply a third property:</p><ul class="postList"><li name="1037" id="1037" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Hard to beat</strong>. Given any RL agent Aᴿᴸ, any agent B, and the ability to sample from a distribution D with B ≻ᴰ Aʟɪɢɴ(Aᴿᴸ), we can implement an agent Bᴿᴸ and sample from a distribution Dᴿᴸ over RL episodes on which Bᴿᴸ outperforms Aᴿᴸ.</li></ul><p name="085f" id="085f" class="graf graf--p graf-after--li">If an algorithm is “hard to beat,” then the only way to make it better (according to ≻ᴰ) is to improve the underlying RL algorithms. In some sense this is the strongest form of optimality that we can realistically hope for, since improving our RL algorithms will allow us to build “better” AI systems for any reasonable notion of “better.”</p><p name="a29e" id="a29e" class="graf graf--p graf-after--p">To see that (efficient + scalable → hard to beat), apply efficiency to find an agent Aʟɪɢɴ(Bᴿᴸ) ⪰ᴰ B, use transitivity to infer that Aʟɪɢɴ(Bᴿᴸ) ≻ᴰ Aʟɪɢɴ(Aᴿᴸ), and then to use safe scalability to sample from a distribution where Bᴿᴸ outperforms Aᴿᴸ.</p><p name="7248" id="7248" class="graf graf--p graf-after--p">Being hard to beat is slightly weaker than being efficient + scalable while being almost as comforting. So it might also be useful as an easier goal.</p><h4 name="b895" id="b895" class="graf graf--h4 graf-after--p">Restrictions on the building&nbsp;blocks</h4><p name="fa54" id="fa54" class="graf graf--p graf-after--h4">Rather than working with a generic RL algorithm, we might want to work with an RL algorithm that satisfies some additional property. For example, Aʟɪɢɴ(Aᴿᴸ) might only be safely scalable if Aᴿᴸ is able to make good enough predictions about humans, or if Aᴿᴸ is “transparent” in an appropriate sense.</p><p name="4687" id="4687" class="graf graf--p graf-after--p">These restrictions can be incorporated into the definition of safe scalability and efficiency — in safe scalability, we can strengthen the hypothesis by assuming that the learning algorithms satisfy the restrictions, and in efficiency we can strengthen the conclusion by assuming that the learning algorithms satisfy the restrictions.</p><p name="e051" id="e051" class="graf graf--p graf-after--p">To make the task easiest, we could strengthen the hypothesis of safe scalability <em class="markup--em markup--p-em">without</em> strengthening the conclusion of efficiency. For example, if we want to work with “transparent” RL algorithms, we will probably not be able to strengthen the conclusion of efficiency — we won’t be able to turn a black-box algorithm B into a transparent RL algorithm Bᴿᴸ. So we could instead aim for a scheme that is safely scalable when applied with transparent RL algorithms, and that is efficient when we are allowed to use arbitrary RL algorithms. The resulting control scheme would only be practically efficient to the extent that there are transparent nearly-state-of-the-art RL algorithms.</p><p name="b02c" id="b02c" class="graf graf--p graf-after--p">For now I am interested in schemes that work under <em class="markup--em markup--p-em">any</em> remotely plausible assumptions:</p><ul class="postList"><li name="500c" id="500c" class="graf graf--li graf-after--p">Even a conditional result would be a big advance beyond our current understanding.</li><li name="6125" id="6125" class="graf graf--li graf-after--li">Conditional results could clarify our understanding of when AI control will and won’t succeed.</li><li name="2c45" id="2c45" class="graf graf--li graf-after--li">Conditional results present natural targets for differential AI progress. For example, if we could formulate a transparency condition that was sufficient for building safe+efficient AI, this could help clarify the goals of research on transparency.</li></ul><h4 name="889b" id="889b" class="graf graf--h4 graf-after--li">Related building&nbsp;blocks</h4><p name="3471" id="3471" class="graf graf--p graf-after--h4">We might also work with a number of different building blocks, and require them to have some relationship to each other. For example, we might want to use both a classifier and an RL agent, and assume that the classifier is in some sense more powerful than the RL agent (though it’s <a href="https://medium.com/ai-control/the-informed-oversight-problem-1b51b4f66b35#.dl1zzh3jm" data-href="https://medium.com/ai-control/the-informed-oversight-problem-1b51b4f66b35#.dl1zzh3jm" class="markup--anchor markup--p-anchor" target="_blank">not yet clear to me what assumption of this form would actually be useful</a>). Or we might work with a sequence of RL agents, and <a href="https://medium.com/ai-control/alba-an-explicit-proposal-for-aligned-ai-17a55f60bbcf" data-href="https://medium.com/ai-control/alba-an-explicit-proposal-for-aligned-ai-17a55f60bbcf" class="markup--anchor markup--p-anchor" target="_blank">assume that each is only slightly more powerful than the last</a>.</p><h3 name="60f1" id="60f1" class="graf graf--h3 graf-after--p">Non-solutions</h3><h4 name="570a" id="570a" class="graf graf--h4 graf-after--h3">Plain RL (+counterfactual oversight)</h4><p name="1f5e" id="1f5e" class="graf graf--p graf-after--h4">The simplest behavior for Aʟɪɢɴ is to simply consult the underlying RL agent directly, and then to define a reward that induces aligned behavior.</p><p name="7d30" id="7d30" class="graf graf--p graf-after--p">Perhaps the simplest reward function is a human’s evaluation: a human watches the agent and presses the reward button when the agent behaves well.</p><p name="f7c6" id="f7c6" class="graf graf--p graf-after--p">If the human has to monitor the agent during every episode, then this constitutes a huge efficiency hit and so our algorithm won’t be reasonably efficient.</p><p name="655e" id="655e" class="graf graf--p graf-after--p">More realistically, we could train our agent on a small subset of the data (for which the human does have to monitor the agent and press the reward button) and then use the learned policy to act even when there is no human monitor. If necessary, we could train online as in <a href="https://medium.com/ai-control/counterfactual-human-in-the-loop-a7822e36f399" data-href="https://medium.com/ai-control/counterfactual-human-in-the-loop-a7822e36f399" class="markup--anchor markup--p-anchor" target="_blank">counterfactual oversight</a>.</p><p name="e293" id="e293" class="graf graf--p graf-after--p">This approach is asymptotically efficient (as long as the size of the training set is sublinear in the total number of episodes, as would be typical).</p><p name="7494" id="7494" class="graf graf--p graf-after--p">However, this scheme is <strong class="markup--strong markup--p-strong">not</strong> safely scalable for a reasonable order ≻ᴰ.</p><p name="da66" id="da66" class="graf graf--p graf-after--p">The problem is that the user’s judgments are not exactly aligned with the user’s preferences. For any given input <em class="markup--em markup--p-em">x</em>, there are some outputs <em class="markup--em markup--p-em">y</em> for which the user assigns <em class="markup--em markup--p-em">y</em> an “erroneously” high expected value, whether due to error, to bias, or simply to not having enough time to understand the consequences of a proposed action.</p><p name="cfe4" id="cfe4" class="graf graf--p graf-after--p">So let Aᴿᴸ be an RL agent which outputs actions which the user mistakenly rates highly, and let Bᴿᴸ be an RL agent which outputs actions that are better than Aᴿᴸ’s outputs but which are rated lower by the user. As long as the user’s judgments don’t perfectly preserve the ordering of outcomes, then we can construct such a pair. Then we can have Aʟɪɢɴ(Aᴿᴸ) ≻ᴰ Aʟɪɢɴ(Bᴿᴸ) for most distributions D, while Bᴿᴸ outperforms Aᴿᴸ on <em class="markup--em markup--p-em">every</em> distribution Dᴿᴸ</p><h4 name="8379" id="8379" class="graf graf--h4 graf-after--p">Plain imitation learning (+counterfactual oversight)</h4><p name="eebc" id="eebc" class="graf graf--p graf-after--h4">Given observations of a human expert, we can give Aᴿᴸ the problem “predict what output the human expert will produce on input <em class="markup--em markup--p-em">x</em>.”</p><p name="645e" id="645e" class="graf graf--p graf-after--p">After Aᴿᴸ produces a prediction, with small probability we ask the human to actually produce an output <em class="markup--em markup--p-em">y</em>, and then we provide Aᴿᴸ a payoff depending on the quality of its prediction. (There are a number of possible ways to score Aᴿᴸ, most involving two separate learners. For example, we could use a generative adversarial model, with a second instance of Aᴿᴸ trying to distinguish the predicted output from the real human output. Or we could score Aᴿᴸ based on a variational lower bound on the log probability it assigned to the particular prediction made by the user.)</p><p name="cdc5" id="cdc5" class="graf graf--p graf-after--p">This kind of imitation learning is not safely scalable: an agent which chooses superhuman actions will become strictly worse if we make them a perfect human-predictor, yet their performance in the RL game will improve.</p><p name="0d86" id="0d86" class="graf graf--p graf-after--p">(Intuitively, imitation <em class="markup--em markup--p-em">is</em> safely scalable but <em class="markup--em markup--p-em">is not </em>efficient — this example highlights a way in which our definitions fail to map well to the intuitive definitions of these concepts.)</p><h4 name="7557" id="7557" class="graf graf--h4 graf-after--p">ALBA</h4><p name="3dbf" id="3dbf" class="graf graf--p graf-after--h4">The <a href="https://medium.com/ai-control/alba-an-explicit-proposal-for-aligned-ai-17a55f60bbcf" data-href="https://medium.com/ai-control/alba-an-explicit-proposal-for-aligned-ai-17a55f60bbcf" class="markup--anchor markup--p-anchor" target="_blank">current version of ALBA</a> is not safely scalable + efficient, even under the generous assumptions made in the post. The key problem is its reliance on a sequence of throttled RL subagents.</p><p name="8b3d" id="8b3d" class="graf graf--p graf-after--p">The missing ingredients, roughly speaking, are the following:</p><ul class="postList"><li name="5d17" id="5d17" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Speedup</strong>. Given black-box access to the agent B, we can produce a <em class="markup--em markup--li-em">strictly less capable</em> agent B′ such that Bootstrap(B′) ⪰ᴰ B.</li><li name="3f28" id="3f28" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Informed oversight</strong>. <a href="https://medium.com/ai-control/the-informed-oversight-problem-1b51b4f66b35" data-href="https://medium.com/ai-control/the-informed-oversight-problem-1b51b4f66b35" class="markup--anchor markup--li-anchor" target="_blank">See here</a>.</li></ul><p name="8445" id="8445" class="graf graf--p graf-after--li">Speedup is impossible without some additional assumptions on B, and informed oversight probably also requires some additional assumptions (in order to make transparency possible). It’s not clear what kind of assumptions on B might be sufficient to carry out the analysis, and overall it looks like a daunting project.</p><h3 name="2641" id="2641" class="graf graf--h3 graf-after--p">Limitations</h3><h4 name="3ecb" id="3ecb" class="graf graf--h4 graf-after--h3">Too strong</h4><p name="5bc3" id="5bc3" class="graf graf--p graf-after--h4">I think the biggest problem with this definition is that it is too strong.</p><p name="a248" id="a248" class="graf graf--p graf-after--p">It’s not so strong as to be <em class="markup--em markup--p-em">obviously</em> impossible. But it looks <em class="markup--em markup--p-em">almost </em>obviously impossible. The discussions of RL and ALBA illustrate why the definition is so strong:</p><ul class="postList"><li name="69cf" id="69cf" class="graf graf--li graf-after--p">In order to turn a black box agent B into an agent Bᴿᴸ with Aʟɪɢɴ(Bᴿᴸ) ⪰ᴰ B, we are essentially forced to take Bᴿᴸ = B (since we can’t produce other derivative agents using black-box access to B).</li><li name="0da5" id="0da5" class="graf graf--li graf-after--li">Then Aʟɪɢɴ is essentially forced to be a training scheme for RL agents.</li><li name="b356" id="b356" class="graf graf--li graf-after--li">So in order to be safely scalable, Aʟɪɢɴ needs to evaluate of the quality of the agent’s decisions “well enough” that optimizing its evaluations optimizes ≻ᴰ.</li><li name="4d4a" id="4d4a" class="graf graf--li graf-after--li">Moreover, Aʟɪɢɴ can’t really use the RL agent’s help to make those decisions — if Aʟɪɢɴ is merely a training procedure, the RL agent need not output anything except on the support of D, and so we can’t get any useful work out of the agent. Thus Aʟɪɢɴ is using the same evaluations for every agent.</li><li name="3329" id="3329" class="graf graf--li graf-after--li">If Aʟɪɢɴ evaluates the agent’s behavior “well enough” for an arbitrary agent, then Aʟɪɢɴ must be evaluating the agent’s behavior perfectly.</li><li name="afbe" id="afbe" class="graf graf--li graf-after--li">It seems infeasible to produce such a perfect evaluations for any interesting ≻ᴰ.</li></ul><p name="c4cb" id="c4cb" class="graf graf--p graf-after--li">How might we weaken the definition?</p><ul class="postList"><li name="cf97" id="cf97" class="graf graf--li graf-after--p">Place some restriction on the set of agents B that we consider in <strong class="markup--strong markup--li-strong">efficiency</strong>. For example, we may restrict attention to the kinds of agents that could be produced by some particular AI research project in AI. I think that this is by far the most promising approach.</li><li name="49cb" id="49cb" class="graf graf--li graf-after--li">As discussed in the section <strong class="markup--strong markup--li-strong">Restrictions on building blocks</strong>, we could only require safe scalability for a certain class of RL agents, thus moving some of the work to ensuring that state-of-the-art RL agents have the required properties.</li><li name="e3c1" id="e3c1" class="graf graf--li graf-after--li">We could use relations ≻ᴰ that evaluate agents holistically in terms of a <em class="markup--em markup--li-em">description</em> of the distribution D (see below). For example, we might say that “A ≻ᴰ B if the human believes that A would outperform B on the distribution D.” I don’t really see a way to make this work, but it might be worth thinking about.</li><li name="2af0" id="2af0" class="graf graf--li graf-after--li">We could settle for an agent which is hard to beat instead of both efficient and safely scalable. I don’t think this really addresses the difficulty described above, but it does give us a tiny bit more traction.</li><li name="5702" id="5702" class="graf graf--li graf-after--li">We could swap the quantifier order, giving us access to B and D when trying to construct an agent Bᴿᴸ with Aʟɪɢɴ(Bᴿᴸ) ⪰ᴰ B. I don’t think this will help.</li></ul><p name="c65a" id="c65a" class="graf graf--p graf-after--li">I expect there are many other ways to weaken the definition, and of course we could pursue some combination of the above.</p><h4 name="5ef5" id="5ef5" class="graf graf--h4 graf-after--p">Improving RL algorithms is quite&nbsp;broad</h4><p name="00c5" id="00c5" class="graf graf--p graf-after--h4">Even if Aʟɪɢɴ is efficient and safely scalable, Aʟɪɢɴ(Aᴿᴸ) isn’t necessarily <em class="markup--em markup--p-em">good</em> even according to ≻ᴰ. In order to make Aʟɪɢɴ(Aᴿᴸ) actually be good, we may need to improve Aᴿᴸ. In some sense this is obvious and inevitable — it’s like saying that even if we solve the control problem, AI progress will still make our AI systems work better.</p><p name="6e07" id="6e07" class="graf graf--p graf-after--p">But in particular, the alignment of Aʟɪɢɴ(Aᴿᴸ) may depend on how Aᴿᴸ performs on some very unnatural distribution over RL problems (e.g. on how well Aᴿᴸ is able to predict the results of human deliberation about moral questions).</p><p name="f21b" id="f21b" class="graf graf--p graf-after--p">Given how strong safe scalability and efficiency are, I don’t think this is a problem for this particular definition. That is, any such “unnatural” distribution over RL problems would be necessary to achieving good behavior, even for very weak agents:</p><ul class="postList"><li name="a0c4" id="a0c4" class="graf graf--li graf-after--p">Given any aligned agent B we can apply efficiency to obtain an RL agent Bᴿᴸ.</li><li name="5f54" id="5f54" class="graf graf--li graf-after--li">If Bᴿᴸ always does well on these unnatural instances, then they were in some strong sense <em class="markup--em markup--li-em">necessary</em> in order to get good behavior. But note that B might be quite weak, so these “unnatural” instances were necessary even to getting good behavior out of weak AI systems.</li><li name="69f4" id="69f4" class="graf graf--li graf-after--li">If Bᴿᴸ does poorly on these unnatural instances, then we can consider a very powerful agent Aᴿᴸ that does equally poorly.</li><li name="22b4" id="22b4" class="graf graf--li graf-after--li">By safe scalability, Aʟɪɢɴ(Aᴿᴸ) remains at-least-as-good-as-B for that RL agent Aᴿᴸ.</li></ul><p name="0697" id="0697" class="graf graf--p graf-after--li">This argument would still go through if we restricted B to the kinds of AI’s that we might actually produce. (The conclusion would be that the unnatural RL instances are in fact necessary, <em class="markup--em markup--p-em">for the kinds of AI that we might actually build</em>, which is just as good.)</p><p name="2a5b" id="2a5b" class="graf graf--p graf-after--p">If we weakened the definition enough we might encounter a more troubling version of this unnatural-RL-instances problem, but even then I think that reducing the alignment problem to a concrete RL problem would probably represent significant progress in and of itself. So overall I’m not concerned about this potential bug.</p><h4 name="23f4" id="23f4" class="graf graf--h4 graf-after--p">Missing definition of&nbsp;≻ᴰ</h4><p name="9f3d" id="9f3d" class="graf graf--p graf-after--h4">We don’t provide any definition of ≻ᴰ; that definition needs to be quite subtle and is doing a lot of the work of formalizing our goals. See the discussion in the next section.</p><h3 name="c8ed" id="c8ed" class="graf graf--h3 graf-after--p">Defining ≻ᴰ</h3><h4 name="df5a" id="df5a" class="graf graf--h4 graf-after--h3">Human judgment</h4><p name="e9c4" id="e9c4" class="graf graf--p graf-after--h4">For sufficiently subhuman agents we could define:</p><ul class="postList"><li name="be0f" id="be0f" class="graf graf--li graf-after--p">Draw a random <em class="markup--em markup--li-em">x</em> from D, compute A(<em class="markup--em markup--li-em">x</em>) and B(<em class="markup--em markup--li-em">x</em>), and give them both to a human judge (along with the input x) to decide which is better. A ≻ᴰ B if the human prefers A(<em class="markup--em markup--li-em">x</em>) with probability at least 2/3.</li></ul><p name="8b92" id="8b92" class="graf graf--p graf-after--li">For this definition of ≺, the RL agent described above is in fact efficient and safely scalable.</p><p name="f2e9" id="f2e9" class="graf graf--p graf-after--p">But being safely scalable in this sense doesn’t seem very useful, since the human’s judgment about “which action is good” need not be correct, and the human would substantially prefer an agent that chooses actions which are <em class="markup--em markup--p-em">actually</em> good.</p><h4 name="f3c7" id="f3c7" class="graf graf--h4 graf-after--p">Human judgment on reflection</h4><p name="cdc7" id="cdc7" class="graf graf--p graf-after--h4">We could try to fix this definition with an alternative informal definition:</p><ul class="postList"><li name="5dac" id="5dac" class="graf graf--li graf-after--p">Draw a random <em class="markup--em markup--li-em">x</em> from D, compute A(<em class="markup--em markup--li-em">x</em>) and B(<em class="markup--em markup--li-em">x</em>). Give a human judge <a href="https://ordinaryideas.wordpress.com/2012/04/21/indirect-normativity-write-up/" data-href="https://ordinaryideas.wordpress.com/2012/04/21/indirect-normativity-write-up/" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">an extremely long time to reflect, and access to extremely powerful tools</a>. A ≻ᴰ B if, upon extensive reflection, the human prefers A(<em class="markup--em markup--li-em">x</em>) with probability at least 2/3.</li></ul><p name="6ddd" id="6ddd" class="graf graf--p graf-after--li">This definition leads to a very strong notion of safe scalability. Unfortunately that strong notion is <em class="markup--em markup--p-em">too</em> strong, and it is clearly intractable to build a hard-to-beat agent in this sense.</p><p name="4794" id="4794" class="graf graf--p graf-after--p">For example, consider some hard question that bears on what strategies will have good long-term effects. This question is beyond our abilities currently, but we will be able to answer it with sufficient reflection. If B guesses the correct answer to that question and A does not, then A ≺ᴰ B, where D is the a point distribution concentrated on a decision where that question matters. If B guesses the wrong answer, then B ≺ᴰ A. But by efficiency + safe scalability, we can essentially produce a pair of agents Aᴿᴸ and Bᴿᴸ and sample from a distribution Dᴿᴸ such that A ≺ᴰ B iff Bᴿᴸ outperforms Aᴿᴸ on D ᴿᴸ. But then we can test which of A and B is correct about the hard question, contradicting its supposed hardness.</p><h4 name="96d8" id="96d8" class="graf graf--h4 graf-after--p">Holistic judgment</h4><p name="1854" id="1854" class="graf graf--p graf-after--h4">A very different approach would be to directly elicit the human’s judgment about the relative merits of A and B. That is:</p><ul class="postList"><li name="e159" id="e159" class="graf graf--li graf-after--p">Give a human judge a description of A and B, and the sampling procedure for D, and ask them which they expect to perform better. A ≻ᴰ B if the human thinks that A would make significantly better decisions than B, on inputs sampled from D.</li></ul><p name="48e2" id="48e2" class="graf graf--p graf-after--li">In some sense this definition gets very directly at what we care about. Suppose that some alternative AI control research program produced B, and that we would be happier using B than A. Then we are precisely satisfying this definition of A ≺ᴰ B.</p><p name="e008" id="e008" class="graf graf--p graf-after--p">An obvious problem with this approach is that determining which of A and B is better may itself be a very hard problem — indeed, it’s easy to believe that it includes all of the difficulty of the alignment problem.</p><p name="628c" id="628c" class="graf graf--p graf-after--p">The definition of efficiency<strong class="markup--strong markup--p-strong"> </strong>still has teeth if we restrict our attention to algorithms B that “explain themselves,” e.g. that are accompanied with significant documentation and explanation. The description of the agent B could embody the entire output of the alternative research program that produced it.</p><p name="edbf" id="edbf" class="graf graf--p graf-after--p">So this definition of betterness is adequate if we think that there are efficient and good decision-making protocols which are <em class="markup--em markup--p-em">demonstrably</em> good to existing humans. This definition is very unsatisfying if we think that evaluating a possible proposal, even given the best available arguments, is the core difficulty of AI control. This might be either because those arguments are necessarily extremely complex, or because there will be many bad proposals that are also supported by extremely convincing-looking arguments.</p><h4 name="08c8" id="08c8" class="graf graf--h4 graf-after--p">What we really&nbsp;want</h4><p name="5df8" id="5df8" class="graf graf--p graf-after--h4">Intuitively, I would like a definition along the lines of:</p><ul class="postList"><li name="0415" id="0415" class="graf graf--li graf-after--p">Draw a random <em class="markup--em markup--li-em">x</em> from D, compute A(<em class="markup--em markup--li-em">x</em>) and B(<em class="markup--em markup--li-em">x</em>). Give a human judge the same information, abilities, and insights, that A and B used to compute these quantities. A ≻ᴰ B if the human prefers A(<em class="markup--em markup--li-em">x</em>) with probability at least 2/3.</li></ul><p name="aebe" id="aebe" class="graf graf--p graf-after--li">This definition smuggles all of the complexity into imagining that the human has the same “information, abilities, and insights” as the AI they are evaluating. I don’t have any candidate formalization of this idea, nor am I especially optimistic about being able to formalize it.</p><p name="4b26" id="4b26" class="graf graf--p graf-after--p">I do feel like I can reason about this definition intuitively and that it roughly captures my intuitive desiderata. This makes me more optimistic that there is <em class="markup--em markup--p-em">some</em> satisfactory definition of ≻.</p><p name="5d9a" id="5d9a" class="graf graf--p graf-after--p">Note that this definition is closely related to the goal in <a href="https://medium.com/ai-control/the-informed-oversight-problem-1b51b4f66b35" data-href="https://medium.com/ai-control/the-informed-oversight-problem-1b51b4f66b35" class="markup--anchor markup--p-anchor" target="_blank">the informed oversight problem</a>, which is roughly to ensure that the overseer “knows everything the AI knows.” In the informed oversight problem we are willing to assume that the overseer is significantly more powerful than the system they are overseeing. That may well be a necessary assumption to actually ensure that the overseer “knows everything the AI knows,” but it probably isn’t needed to define what it would mean for the overseer to “know everything the AI knows.”</p><h3 name="daa8" id="daa8" class="graf graf--h3 graf-after--p">Conclusion</h3><p name="075d" id="075d" class="graf graf--p graf-after--h3">We can try to define the goals of AI control by thinking about how AI systems relate to the underlying machine learning primitives. Such a framework wouldn’t cover all possible approaches to AI control, but where applicable it could be a great way to organize research and a useful analysis tool.</p><p name="e937" id="e937" class="graf graf--p graf-after--p graf--trailing">This post gave a step in that direction, but did not yet succeed. I would love to see other attempts, and I think there is a good chance that it will be possible to find a satisfying problem statement for AI control.</p></div></div></section></div><footer class="u-paddingTop10"><div class="container u-maxWidth740"><div class="row"><div class="col u-size12of12"></div></div><div class="row"><div class="col u-size12of12 js-postTags"><div class="u-paddingBottom10"><ul class="tags tags--postTags tags--borderless"></ul></div></div></div><div class="postActions js-postActionsFooter "><div class="u-flexCenter"><div class="u-flex1"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="8218fa8a871f" data-is-icon-29px="true" data-is-circle="true" data-has-recommend-list="true" data-source="post_actions_footer-----8218fa8a871f---------------------clap_footer" data-clap-string-singular="clap" data-clap-string-plural="claps"><div class="u-relative u-foreground"><button class="button button--large button--circle button--withChrome u-baseColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--darker clapButton--largePill u-relative u-foreground u-xs-paddingLeft13 u-width60 u-height60 u-accentColor--textNormal u-accentColor--buttonNormal clap-onboardingcollection" data-action="sign-up-prompt" data-sign-in-action="multivote" data-requires-token="true" data-redirect="https://medium.com/_/vote/p/8218fa8a871f" data-action-source="post_actions_footer-----8218fa8a871f---------------------clap_footer" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--33px u-relative u-topNegative2 u-xs-top0"><svg class="svgIcon-use" width="33" height="33"><path d="M28.86 17.342l-3.64-6.402c-.292-.433-.712-.729-1.163-.8a1.124 1.124 0 0 0-.889.213c-.63.488-.742 1.181-.33 2.061l1.222 2.587 1.4 2.46c2.234 4.085 1.511 8.007-2.145 11.663-.26.26-.526.49-.797.707 1.42-.084 2.881-.683 4.292-2.094 3.822-3.823 3.565-7.876 2.05-10.395zm-6.252 11.075c3.352-3.35 3.998-6.775 1.978-10.469l-3.378-5.945c-.292-.432-.712-.728-1.163-.8a1.122 1.122 0 0 0-.89.213c-.63.49-.742 1.182-.33 2.061l1.72 3.638a.502.502 0 0 1-.806.568l-8.91-8.91a1.335 1.335 0 0 0-1.887 1.886l5.292 5.292a.5.5 0 0 1-.707.707l-5.292-5.292-1.492-1.492c-.503-.503-1.382-.505-1.887 0a1.337 1.337 0 0 0 0 1.886l1.493 1.492 5.292 5.292a.499.499 0 0 1-.353.854.5.5 0 0 1-.354-.147L5.642 13.96a1.338 1.338 0 0 0-1.887 0 1.338 1.338 0 0 0 0 1.887l2.23 2.228 3.322 3.324a.499.499 0 0 1-.353.853.502.502 0 0 1-.354-.146l-3.323-3.324a1.333 1.333 0 0 0-1.886 0 1.325 1.325 0 0 0-.39.943c0 .356.138.691.39.943l6.396 6.397c3.528 3.53 8.86 5.313 12.821 1.353zM12.73 9.26l5.68 5.68-.49-1.037c-.518-1.107-.426-2.13.224-2.89l-3.303-3.304a1.337 1.337 0 0 0-1.886 0 1.326 1.326 0 0 0-.39.944c0 .217.067.42.165.607zm14.787 19.184c-1.599 1.6-3.417 2.392-5.353 2.392-.349 0-.7-.03-1.058-.082a7.922 7.922 0 0 1-3.667.887c-3.049 0-6.115-1.626-8.359-3.87l-6.396-6.397A2.315 2.315 0 0 1 2 19.724a2.327 2.327 0 0 1 1.923-2.296l-.875-.875a2.339 2.339 0 0 1 0-3.3 2.33 2.33 0 0 1 1.24-.647l-.139-.139c-.91-.91-.91-2.39 0-3.3.884-.884 2.421-.882 3.301 0l.138.14a2.335 2.335 0 0 1 3.948-1.24l.093.092c.091-.423.291-.828.62-1.157a2.336 2.336 0 0 1 3.3 0l3.384 3.386a2.167 2.167 0 0 1 1.271-.173c.534.086 1.03.354 1.441.765.11-.549.415-1.034.911-1.418a2.12 2.12 0 0 1 1.661-.41c.727.117 1.385.565 1.853 1.262l3.652 6.423c1.704 2.832 2.025 7.377-2.205 11.607zM13.217.484l-1.917.882 2.37 2.837-.454-3.719zm8.487.877l-1.928-.86-.44 3.697 2.368-2.837zM16.5 3.293L15.478-.005h2.044L16.5 3.293z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--33px u-relative u-topNegative2 u-xs-top0"><svg class="svgIcon-use" width="33" height="33"><g fill-rule="evenodd"><path d="M29.58 17.1l-3.854-6.78c-.365-.543-.876-.899-1.431-.989a1.491 1.491 0 0 0-1.16.281c-.42.327-.65.736-.7 1.207v.001l3.623 6.367c2.46 4.498 1.67 8.802-2.333 12.807-.265.265-.536.505-.81.728 1.973-.222 3.474-1.286 4.45-2.263 4.166-4.165 3.875-8.6 2.215-11.36zm-4.831.82l-3.581-6.3c-.296-.439-.725-.742-1.183-.815a1.105 1.105 0 0 0-.89.213c-.647.502-.755 1.188-.33 2.098l1.825 3.858a.601.601 0 0 1-.197.747.596.596 0 0 1-.77-.067L10.178 8.21c-.508-.506-1.393-.506-1.901 0a1.335 1.335 0 0 0-.393.95c0 .36.139.698.393.95v.001l5.61 5.61a.599.599 0 1 1-.848.847l-5.606-5.606c-.001 0-.002 0-.003-.002L5.848 9.375a1.349 1.349 0 0 0-1.902 0 1.348 1.348 0 0 0 0 1.901l1.582 1.582 5.61 5.61a.6.6 0 0 1-.848.848l-5.61-5.61c-.51-.508-1.393-.508-1.9 0a1.332 1.332 0 0 0-.394.95c0 .36.139.697.393.952l2.363 2.362c.002.001.002.002.002.003l3.52 3.52a.6.6 0 0 1-.848.847l-3.522-3.523h-.001a1.336 1.336 0 0 0-.95-.393 1.345 1.345 0 0 0-.949 2.295l6.779 6.78c3.715 3.713 9.327 5.598 13.49 1.434 3.527-3.528 4.21-7.13 2.086-11.015zM11.817 7.727c.06-.328.213-.64.466-.893.64-.64 1.755-.64 2.396 0l3.232 3.232c-.82.783-1.09 1.833-.764 2.992l-5.33-5.33z"></path><path d="M13.285.48l-1.916.881 2.37 2.837z"></path><path d="M21.719 1.361L19.79.501l-.44 3.697z"></path><path d="M16.502 3.298L15.481 0h2.043z"></path></g></svg></span></span></button><div class="clapUndo u-width60 u-round u-height32 u-absolute u-borderBox u-paddingRight5 u-transition--transform200Springu-backgroundGrayLighter js-clapUndo" style="top: 14px; padding: 2px;"><button class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon u-floatRight" data-action="multivote-undo" data-action-value="8218fa8a871f"><span class="svgIcon svgIcon--removeThin svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M20.13 8.11l-5.61 5.61-5.609-5.61-.801.801 5.61 5.61-5.61 5.61.801.8 5.61-5.609 5.61 5.61.8-.801-5.609-5.61 5.61-5.61" fill-rule="evenodd"></path></svg></span></button></div></div><span class="u-relative u-background js-actionMultirecommendCount u-marginLeft16"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-textColorDarker" data-action="show-recommends" data-action-value="8218fa8a871f">4 claps</button><span class="u-xs-hide"></span></span></div></div><div class="buttonSet u-flex0"><a class="button button--dark button--chromeless u-baseColor--buttonDark button--withIcon button--withSvgIcon button--dark button--chromeless u-xs-hide u-marginRight12" href="https://medium.com/p/8218fa8a871f/share/twitter" title="Share on Twitter" aria-label="Share on Twitter" target="_blank" data-action-source="post_actions_footer"><span class="button-defaultState"><span class="svgIcon svgIcon--twitterFilled svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M22.053 7.54a4.474 4.474 0 0 0-3.31-1.455 4.526 4.526 0 0 0-4.526 4.524c0 .35.04.7.082 1.05a12.9 12.9 0 0 1-9.3-4.77c-.39.69-.61 1.46-.65 2.26.03 1.6.83 2.99 2.02 3.79-.72-.02-1.41-.22-2.02-.57-.01.02-.01.04 0 .08-.01 2.17 1.55 4 3.63 4.44-.39.08-.79.13-1.21.16-.28-.03-.57-.05-.81-.08.54 1.77 2.21 3.08 4.2 3.15a9.564 9.564 0 0 1-5.66 1.94c-.34-.03-.7-.06-1.05-.08 2 1.27 4.38 2.02 6.94 2.02 8.31 0 12.86-6.9 12.84-12.85.02-.24.01-.43 0-.65.89-.62 1.65-1.42 2.26-2.34-.82.38-1.69.62-2.59.72a4.37 4.37 0 0 0 1.94-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></span></span></a><a class="button button--dark button--chromeless u-baseColor--buttonDark button--withIcon button--withSvgIcon button--dark button--chromeless u-xs-hide u-marginRight12" href="https://medium.com/p/8218fa8a871f/share/facebook" title="Share on Facebook" aria-label="Share on Facebook" target="_blank" data-action-source="post_actions_footer"><span class="button-defaultState"><span class="svgIcon svgIcon--facebookSquare svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M23.209 5H5.792A.792.792 0 0 0 5 5.791V23.21c0 .437.354.791.792.791h9.303v-7.125H12.72v-2.968h2.375v-2.375c0-2.455 1.553-3.662 3.741-3.662 1.049 0 1.95.078 2.213.112v2.565h-1.517c-1.192 0-1.469.567-1.469 1.397v1.963h2.969l-.594 2.968h-2.375L18.11 24h5.099a.791.791 0 0 0 .791-.791V5.79a.791.791 0 0 0-.791-.79"></path></svg></span></span></a><button class="button button--large button--dark button--chromeless u-baseColor--buttonDark button--withIcon button--withSvgIcon u-xs-show u-marginRight10" title="Share this story on Twitter or Facebook" aria-label="Share this story on Twitter or Facebook" data-action="show-share-popover" data-action-source="post_actions_footer"><span class="svgIcon svgIcon--share svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M20.385 8H19a.5.5 0 1 0 .011 1h1.39c.43 0 .84.168 1.14.473.31.305.48.71.48 1.142v10.77c0 .43-.17.837-.47 1.142-.3.305-.71.473-1.14.473H8.62c-.43 0-.84-.168-1.144-.473a1.603 1.603 0 0 1-.473-1.142v-10.77c0-.43.17-.837.48-1.142A1.599 1.599 0 0 1 8.62 9H10a.502.502 0 0 0 0-1H8.615c-.67 0-1.338.255-1.85.766-.51.51-.765 1.18-.765 1.85v10.77c0 .668.255 1.337.766 1.848.51.51 1.18.766 1.85.766h11.77c.668 0 1.337-.255 1.848-.766.51-.51.766-1.18.766-1.85v-10.77c0-.668-.255-1.337-.766-1.848A2.61 2.61 0 0 0 20.384 8zm-8.67-2.508L14 3.207v8.362c0 .27.224.5.5.5s.5-.23.5-.5V3.2l2.285 2.285a.49.49 0 0 0 .704-.001.511.511 0 0 0 0-.708l-3.14-3.14a.504.504 0 0 0-.71 0L11 4.776a.501.501 0 0 0 .71.706" fill-rule="evenodd"></path></svg></span></button><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon" data-action="scroll-to-responses" data-action-source="post_actions_footer"><span class="svgIcon svgIcon--response svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M21.27 20.058c1.89-1.826 2.754-4.17 2.754-6.674C24.024 8.21 19.67 4 14.1 4 8.53 4 4 8.21 4 13.384c0 5.175 4.53 9.385 10.1 9.385 1.007 0 2-.14 2.95-.41.285.25.592.49.918.7 1.306.87 2.716 1.31 4.19 1.31.276-.01.494-.14.6-.36a.625.625 0 0 0-.052-.65c-.61-.84-1.042-1.71-1.282-2.58a5.417 5.417 0 0 1-.154-.75zm-3.85 1.324l-.083-.28-.388.12a9.72 9.72 0 0 1-2.85.424c-4.96 0-8.99-3.706-8.99-8.262 0-4.556 4.03-8.263 8.99-8.263 4.95 0 8.77 3.71 8.77 8.27 0 2.25-.75 4.35-2.5 5.92l-.24.21v.32c0 .07 0 .19.02.37.03.29.1.6.19.92.19.7.49 1.4.89 2.08-.93-.14-1.83-.49-2.67-1.06-.34-.22-.88-.48-1.16-.74z"></path></svg></span></button><button class="button button--large button--dark button--chromeless is-touchIconFadeInPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="sign-up-prompt" data-sign-in-action="add-to-bookmarks" data-requires-token="true" data-redirect="https://medium.com/_/bookmark/p/8218fa8a871f" data-action-source="post_actions_footer"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--29px u-marginRight4"><svg class="svgIcon-use" width="29" height="29"><path d="M19.385 4h-9.77A2.623 2.623 0 0 0 7 6.615V23.01a1.022 1.022 0 0 0 1.595.847l5.905-4.004 5.905 4.004A1.022 1.022 0 0 0 22 23.011V6.62A2.625 2.625 0 0 0 19.385 4zM21 23l-5.91-3.955-.148-.107a.751.751 0 0 0-.884 0l-.147.107L8 23V6.615C8 5.725 8.725 5 9.615 5h9.77C20.275 5 21 5.725 21 6.615V23z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--29px u-marginRight4"><svg class="svgIcon-use" width="29" height="29"><path d="M19.385 4h-9.77A2.623 2.623 0 0 0 7 6.615V23.01a1.022 1.022 0 0 0 1.595.847l5.905-4.004 5.905 4.004A1.022 1.022 0 0 0 22 23.011V6.62A2.625 2.625 0 0 0 19.385 4z" fill-rule="evenodd"></path></svg></span></span></button><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon js-moreActionsButton" title="More actions" aria-label="More actions" data-action="more-actions"><span class="svgIcon svgIcon--more svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="-480.5 272.5 21 21"><path d="M-463 284.6c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5z"></path></svg></span></button></div></div></div></div><div class="u-maxWidth740 u-paddingTop20 u-marginTop20 u-borderTopLightest container u-paddingBottom20 u-xs-paddingBottom10 js-postAttributionFooterContainer"><div class="row js-postFooterInfo"><div class="col u-size6of12 u-xs-size12of12"><li class="uiScale uiScale-ui--small uiScale-caption--regular u-block u-paddingBottom18 js-cardUser"><div class="u-marginLeft20 u-floatRight"><span class="followState js-followState" data-user-id="57f1a655a613"><button class="button button--small u-noUserSelect button--withChrome u-baseColor--buttonNormal button--withHover button--unblock js-unblockButton" data-action="sign-up-prompt" data-sign-in-action="toggle-block-user" data-requires-token="true" data-redirect="https://ai-alignment.com/efficient-and-safely-scalable-8218fa8a871f" data-action-source="footer_card"><span class="button-label  button-defaultState">Blocked</span><span class="button-label button-hoverState">Unblock</span></button><button class="button button--primary button--small u-noUserSelect button--withChrome u-accentColor--buttonNormal button--follow js-followButton" data-action="sign-up-prompt" data-sign-in-action="toggle-subscribe-user" data-requires-token="true" data-redirect="https://medium.com/_/subscribe/user/57f1a655a613" data-action-source="footer_card-57f1a655a613-------------------------follow_footer"><span class="button-label  button-defaultState js-buttonLabel">Follow</span><span class="button-label button-activeState">Following</span></button></span></div><div class="u-tableCell"><a class="link u-baseColor--link avatar" href="https://ai-alignment.com/@paulfchristiano?source=footer_card" title="Go to the profile of Paul Christiano" aria-label="Go to the profile of Paul Christiano" data-action-source="footer_card" data-user-id="57f1a655a613" data-collection-slug="ai-control" dir="auto"><img src="https://cdn-images-1.medium.com/fit/c/60/60/1*BNjZCuQuRfIgcXCBMipuBw.jpeg" class="avatar-image avatar-image--small" alt="Go to the profile of Paul Christiano"></a></div><div class="u-tableCell u-verticalAlignMiddle u-breakWord u-paddingLeft15"><h3 class="ui-h3 u-fontSize18 u-lineHeightTighter u-marginBottom4"><a class="link link--primary u-accentColor--hoverTextNormal" href="https://ai-alignment.com/@paulfchristiano" property="cc:attributionName" title="Go to the profile of Paul Christiano" aria-label="Go to the profile of Paul Christiano" rel="author cc:attributionUrl" data-user-id="57f1a655a613" data-collection-slug="ai-control" dir="auto">Paul Christiano</a></h3><p class="ui-body u-fontSize14 u-lineHeightBaseSans u-textColorDark u-marginBottom4">OpenAI</p></div></li></div><div class="col u-size6of12 u-xs-size12of12 u-xs-marginTop30"><li class="uiScale uiScale-ui--small uiScale-caption--regular u-block u-paddingBottom18 js-cardCollection"><div class="u-marginLeft20 u-floatRight"><button class="button button--primary button--small u-noUserSelect button--withChrome u-accentColor--buttonNormal js-relationshipButton" data-action="sign-up-prompt" data-sign-in-action="toggle-follow-collection" data-requires-token="true" data-redirect="https://medium.com/_/subscribe/collection/ai-control" data-action-source="footer_card----624d886c4aa4----------------------follow_footer" data-collection-id="624d886c4aa4"><span class="button-label  js-buttonLabel">Follow</span></button></div><div class="u-tableCell "><a class="link u-baseColor--link avatar avatar--roundedRectangle" href="https://ai-alignment.com?source=footer_card" title="Go to AI Alignment" aria-label="Go to AI Alignment" data-action-source="footer_card" data-collection-slug="ai-control"><img src="https://cdn-images-1.medium.com/fit/c/60/60/1*N56Qc5-aHTcfGff0scntKQ.png" class="avatar-image u-size60x60" alt="AI Alignment"></a></div><div class="u-tableCell u-verticalAlignMiddle u-breakWord u-paddingLeft15"><h3 class="ui-h3 u-fontSize18 u-lineHeightTighter u-marginBottom4"><a class="link link--primary u-accentColor--hoverTextNormal" href="https://ai-alignment.com?source=footer_card" rel="collection" data-action-source="footer_card" data-collection-slug="ai-control">AI Alignment</a></h3><p class="ui-body u-fontSize14 u-lineHeightBaseSans u-textColorDark u-marginBottom4">Aligning AI systems with human interests.</p><div class="buttonSet"></div></div></li></div></div></div><div class="js-postFooterPlacements" data-post-id="8218fa8a871f" data-collection-id="624d886c4aa4" data-scroll="native"><div class="streamItem streamItem--placementCardGrid js-streamItem"><div class="u-clearfix u-backgroundGrayLightest"><div class="row u-marginAuto u-maxWidth1032 u-paddingTop30 u-paddingBottom40"><div class="col u-padding8 u-xs-size12of12 u-size4of12"><div class="uiScale uiScale-ui--small uiScale-caption--regular u-height280 u-width100pct u-backgroundWhite u-borderCardBorder u-boxShadow u-borderBox u-borderRadius4 js-trackPostPresentation" data-post-id="c0bee00365bd" data-source="placement_card_footer_grid---------0-41" data-tracking-context="placement"><div class="u-padding15 u-borderBox u-flexColumn u-sizeFull"><a class="link link--noUnderline u-baseColor--link u-flex1 u-flexColumn" href="https://ai-alignment.com/universality-and-consequentialism-within-hch-c0bee00365bd?source=placement_card_footer_grid---------0-41" data-action-source="placement_card_footer_grid---------0-41"><div class="uiScale uiScale-ui--regular uiScale-caption--small u-textColorNormal u-marginBottom7">More from AI Alignment</div><div class="ui-h3 ui-clamp2 u-textColorDarkest u-contentSansBold u-fontSize24 u-maxHeight2LineHeightTighter u-lineClamp2 u-textOverflowEllipsis u-letterSpacingTight u-paddingBottom2">Universality and consequentialism within HCH</div><div class="ui-body ui-clamp2 u-lineClamp2 u-textOverflowEllipsis u-maxHeight2LineHeightTighter">One exotic reason HCH can fail to be universal is the emergence of malicious patterns of behavior; universality may help address this risk.</div></a><div class="u-paddingBottom10 u-flex0 u-flexCenter"><div class="u-flex1 u-minWidth0 u-marginRight10"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://ai-alignment.com/@paulfchristiano" data-action="show-user-card" data-action-value="57f1a655a613" data-action-type="hover" data-user-id="57f1a655a613" data-collection-slug="ai-control" dir="auto"><img src="https://cdn-images-1.medium.com/fit/c/36/36/1*BNjZCuQuRfIgcXCBMipuBw.jpeg" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Paul Christiano"></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--darker" href="https://ai-alignment.com/@paulfchristiano?source=placement_card_footer_grid---------0-41" data-action="show-user-card" data-action-source="placement_card_footer_grid---------0-41" data-action-value="57f1a655a613" data-action-type="hover" data-user-id="57f1a655a613" data-collection-slug="ai-control" dir="auto">Paul Christiano</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><a class="link link--darken" href="https://ai-alignment.com/universality-and-consequentialism-within-hch-c0bee00365bd?source=placement_card_footer_grid---------0-41" data-action="open-post" data-action-value="https://ai-alignment.com/universality-and-consequentialism-within-hch-c0bee00365bd?source=placement_card_footer_grid---------0-41" data-action-source="preview-listing"><time datetime="2019-01-10T03:50:05.776Z">Jan 9</time></a><span class="middotDivider u-fontSize12"></span><span class="readingTime" title="9 min read"></span></div></div></div></div><div class="u-flex0 u-flexCenter"><div class="buttonSet"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="c0bee00365bd" data-is-label-padded="true" data-source="placement_card_footer_grid-----c0bee00365bd----0-41----------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--darker" data-action="sign-up-prompt" data-sign-in-action="multivote" data-requires-token="true" data-redirect="https://medium.com/_/vote/p/c0bee00365bd" data-action-source="placement_card_footer_grid-----c0bee00365bd----0-41----------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-relative u-background js-actionMultirecommendCount u-marginLeft5"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents u-marginLeft4" data-action="show-recommends" data-action-value="c0bee00365bd">18</button></span></div></div><div class="u-height20 u-borderRightLighter u-inlineBlock u-relative u-marginRight10 u-marginLeft12"></div><div class="buttonSet"><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="sign-up-prompt" data-sign-in-action="add-to-bookmarks" data-requires-token="true" data-redirect="https://medium.com/_/bookmark/p/c0bee00365bd" data-action-source="placement_card_footer_grid-----c0bee00365bd----0-41----------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button></div></div></div></div></div></div><div class="col u-padding8 u-xs-size12of12 u-size4of12"><div class="uiScale uiScale-ui--small uiScale-caption--regular u-height280 u-width100pct u-backgroundWhite u-borderCardBorder u-boxShadow u-borderBox u-borderRadius4 js-trackPostPresentation" data-post-id="b6ffa82198ee" data-source="placement_card_footer_grid---------1-14" data-tracking-context="placement"><a class="link link--noUnderline u-baseColor--link" href="https://medium.com/forwardtick/how-google-collapsed-b6ffa82198ee?source=placement_card_footer_grid---------1-14" data-action-source="placement_card_footer_grid---------1-14"><div class="u-backgroundCover u-backgroundColorGrayLight u-height100 u-width100pct u-borderBottomLight u-borderRadiusTop4" style="background-image: url(&quot;https://cdn-images-1.medium.com/fit/c/400/120/1*BGD9g1PoepfxaPn1Uxyzfg.jpeg&quot;); background-position: 50% 50% !important;"></div></a><div class="u-padding15 u-borderBox u-flexColumn u-height180"><a class="link link--noUnderline u-baseColor--link u-flex1" href="https://medium.com/forwardtick/how-google-collapsed-b6ffa82198ee?source=placement_card_footer_grid---------1-14" data-action-source="placement_card_footer_grid---------1-14"><div class="uiScale uiScale-ui--regular uiScale-caption--small u-textColorNormal u-marginBottom7"><div class="u-floatRight u-textColorNormal"><span class="svgIcon svgIcon--star svgIcon--15px"><svg class="svgIcon-use" width="15" height="15"><path d="M7.438 2.324c.034-.099.09-.099.123 0l1.2 3.53a.29.29 0 0 0 .26.19h3.884c.11 0 .127.049.038.111L9.8 8.327a.271.271 0 0 0-.099.291l1.2 3.53c.034.1-.011.131-.098.069l-3.142-2.18a.303.303 0 0 0-.32 0l-3.145 2.182c-.087.06-.132.03-.099-.068l1.2-3.53a.271.271 0 0 0-.098-.292L2.056 6.146c-.087-.06-.071-.112.038-.112h3.884a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></div><div class="u-noWrapWithEllipsis u-marginRight40">Top on Medium</div></div><div class="ui-h3 ui-clamp2 u-textColorDarkest u-contentSansBold u-fontSize24 u-maxHeight2LineHeightTighter u-lineClamp2 u-textOverflowEllipsis u-letterSpacingTight u-paddingBottom2">This Is How Google Will Collapse</div></a><div class="u-paddingBottom10 u-flex0 u-flexCenter"><div class="u-flex1 u-minWidth0 u-marginRight10"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://medium.com/@dacoja" data-action="show-user-card" data-action-value="6502e16a569a" data-action-type="hover" data-user-id="6502e16a569a" dir="auto"><div class="u-relative u-inlineBlock u-flex0"><img src="https://cdn-images-1.medium.com/fit/c/36/36/1*XDy5OzyjY4cHWYxsFcJX_Q.png" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Daniel Colin James"><div class="avatar-halo u-absolute u-textColorGreenNormal svgIcon" style="width: calc(100% + 10px); height: calc(100% + 10px); top:-5px; left:-5px"><svg viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M3.44615311,11.6601601 C6.57294867,5.47967718 12.9131553,1.5 19.9642857,1.5 C27.0154162,1.5 33.3556228,5.47967718 36.4824183,11.6601601 L37.3747245,11.2087295 C34.0793076,4.69494641 27.3961457,0.5 19.9642857,0.5 C12.5324257,0.5 5.84926381,4.69494641 2.55384689,11.2087295 L3.44615311,11.6601601 Z"></path><path d="M36.4824183,28.2564276 C33.3556228,34.4369105 27.0154162,38.4165876 19.9642857,38.4165876 C12.9131553,38.4165876 6.57294867,34.4369105 3.44615311,28.2564276 L2.55384689,28.7078582 C5.84926381,35.2216412 12.5324257,39.4165876 19.9642857,39.4165876 C27.3961457,39.4165876 34.0793076,35.2216412 37.3747245,28.7078582 L36.4824183,28.2564276 Z"></path></svg></div></div></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--darker" href="https://medium.com/@dacoja?source=placement_card_footer_grid---------1-14" data-action="show-user-card" data-action-source="placement_card_footer_grid---------1-14" data-action-value="6502e16a569a" data-action-type="hover" data-user-id="6502e16a569a" dir="auto">Daniel Colin James</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><a class="link link--darken" href="https://medium.com/forwardtick/how-google-collapsed-b6ffa82198ee?source=placement_card_footer_grid---------1-14" data-action="open-post" data-action-value="https://medium.com/forwardtick/how-google-collapsed-b6ffa82198ee?source=placement_card_footer_grid---------1-14" data-action-source="preview-listing"><time datetime="2017-04-24T19:14:43.112Z">Apr 24, 2017</time></a><span class="middotDivider u-fontSize12"></span><span class="readingTime" title="9 min read"></span><span class="u-paddingLeft4"><span class="svgIcon svgIcon--star svgIcon--15px"><svg class="svgIcon-use" width="15" height="15"><path d="M7.438 2.324c.034-.099.09-.099.123 0l1.2 3.53a.29.29 0 0 0 .26.19h3.884c.11 0 .127.049.038.111L9.8 8.327a.271.271 0 0 0-.099.291l1.2 3.53c.034.1-.011.131-.098.069l-3.142-2.18a.303.303 0 0 0-.32 0l-3.145 2.182c-.087.06-.132.03-.099-.068l1.2-3.53a.271.271 0 0 0-.098-.292L2.056 6.146c-.087-.06-.071-.112.038-.112h3.884a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></span></div></div></div></div><div class="u-flex0 u-flexCenter"><div class="buttonSet"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="b6ffa82198ee" data-is-label-padded="true" data-source="placement_card_footer_grid-----b6ffa82198ee----1-14----------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--darker" data-action="sign-up-prompt" data-sign-in-action="multivote" data-requires-token="true" data-redirect="https://medium.com/_/vote/p/b6ffa82198ee" data-action-source="placement_card_footer_grid-----b6ffa82198ee----1-14----------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-relative u-background js-actionMultirecommendCount u-marginLeft5"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents u-marginLeft4" data-action="show-recommends" data-action-value="b6ffa82198ee">86K</button></span></div></div><div class="u-height20 u-borderRightLighter u-inlineBlock u-relative u-marginRight10 u-marginLeft12"></div><div class="buttonSet"><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="sign-up-prompt" data-sign-in-action="add-to-bookmarks" data-requires-token="true" data-redirect="https://medium.com/_/bookmark/p/b6ffa82198ee" data-action-source="placement_card_footer_grid-----b6ffa82198ee----1-14----------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button></div></div></div></div></div></div><div class="col u-padding8 u-xs-size12of12 u-size4of12"><div class="uiScale uiScale-ui--small uiScale-caption--regular u-height280 u-width100pct u-backgroundWhite u-borderCardBorder u-boxShadow u-borderBox u-borderRadius4 js-trackPostPresentation" data-post-id="e4510b6d7975" data-source="placement_card_footer_grid---------2-14" data-tracking-context="placement"><a class="link link--noUnderline u-baseColor--link" href="https://onezero.medium.com/whats-on-your-home-screen-erica-joy-baker-e4510b6d7975?source=placement_card_footer_grid---------2-14" data-action-source="placement_card_footer_grid---------2-14"><div class="u-backgroundCover u-backgroundColorGrayLight u-height100 u-width100pct u-borderBottomLight u-borderRadiusTop4" style="background-image: url(&quot;https://cdn-images-1.medium.com/focal/400/120/52/36/1*202HQdP0MHQMLL17O1ryXw.png&quot;); background-position: 52% 36% !important;"></div></a><div class="u-padding15 u-borderBox u-flexColumn u-height180"><a class="link link--noUnderline u-baseColor--link u-flex1" href="https://onezero.medium.com/whats-on-your-home-screen-erica-joy-baker-e4510b6d7975?source=placement_card_footer_grid---------2-14" data-action-source="placement_card_footer_grid---------2-14"><div class="uiScale uiScale-ui--regular uiScale-caption--small u-textColorNormal u-marginBottom7"><div class="u-floatRight u-textColorNormal"><span class="svgIcon svgIcon--star svgIcon--15px"><svg class="svgIcon-use" width="15" height="15"><path d="M7.438 2.324c.034-.099.09-.099.123 0l1.2 3.53a.29.29 0 0 0 .26.19h3.884c.11 0 .127.049.038.111L9.8 8.327a.271.271 0 0 0-.099.291l1.2 3.53c.034.1-.011.131-.098.069l-3.142-2.18a.303.303 0 0 0-.32 0l-3.145 2.182c-.087.06-.132.03-.099-.068l1.2-3.53a.271.271 0 0 0-.098-.292L2.056 6.146c-.087-.06-.071-.112.038-.112h3.884a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></div><div class="u-noWrapWithEllipsis u-marginRight40">Top on Medium</div></div><div class="ui-h3 ui-clamp2 u-textColorDarkest u-contentSansBold u-fontSize24 u-maxHeight2LineHeightTighter u-lineClamp2 u-textOverflowEllipsis u-letterSpacingTight u-paddingBottom2">What’s on Your Home Screen, Erica Joy Baker?</div></a><div class="u-paddingBottom10 u-flex0 u-flexCenter"><div class="u-flex1 u-minWidth0 u-marginRight10"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://onezero.medium.com/@dlberes" data-action="show-user-card" data-action-value="1ccc714607f0" data-action-type="hover" data-user-id="1ccc714607f0" data-collection-slug="one-zero" dir="auto"><div class="u-relative u-inlineBlock u-flex0"><img src="https://cdn-images-1.medium.com/fit/c/36/36/1*lLRGoD_Y3uJ78dOHSHrhMg.jpeg" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Damon Beres"><div class="avatar-halo u-absolute u-textColorGreenNormal svgIcon" style="width: calc(100% + 10px); height: calc(100% + 10px); top:-5px; left:-5px"><svg viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M3.44615311,11.6601601 C6.57294867,5.47967718 12.9131553,1.5 19.9642857,1.5 C27.0154162,1.5 33.3556228,5.47967718 36.4824183,11.6601601 L37.3747245,11.2087295 C34.0793076,4.69494641 27.3961457,0.5 19.9642857,0.5 C12.5324257,0.5 5.84926381,4.69494641 2.55384689,11.2087295 L3.44615311,11.6601601 Z"></path><path d="M36.4824183,28.2564276 C33.3556228,34.4369105 27.0154162,38.4165876 19.9642857,38.4165876 C12.9131553,38.4165876 6.57294867,34.4369105 3.44615311,28.2564276 L2.55384689,28.7078582 C5.84926381,35.2216412 12.5324257,39.4165876 19.9642857,39.4165876 C27.3961457,39.4165876 34.0793076,35.2216412 37.3747245,28.7078582 L36.4824183,28.2564276 Z"></path></svg></div></div></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--darker" href="https://onezero.medium.com/@dlberes?source=placement_card_footer_grid---------2-14" data-action="show-user-card" data-action-source="placement_card_footer_grid---------2-14" data-action-value="1ccc714607f0" data-action-type="hover" data-user-id="1ccc714607f0" data-collection-slug="one-zero" dir="auto">Damon Beres</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><a class="link link--darken" href="https://onezero.medium.com/whats-on-your-home-screen-erica-joy-baker-e4510b6d7975?source=placement_card_footer_grid---------2-14" data-action="open-post" data-action-value="https://onezero.medium.com/whats-on-your-home-screen-erica-joy-baker-e4510b6d7975?source=placement_card_footer_grid---------2-14" data-action-source="preview-listing"><time datetime="2019-03-15T15:26:28.206Z">Mar 15</time></a><span class="middotDivider u-fontSize12"></span><span class="readingTime" title="7 min read"></span><span class="u-paddingLeft4"><span class="svgIcon svgIcon--star svgIcon--15px"><svg class="svgIcon-use" width="15" height="15"><path d="M7.438 2.324c.034-.099.09-.099.123 0l1.2 3.53a.29.29 0 0 0 .26.19h3.884c.11 0 .127.049.038.111L9.8 8.327a.271.271 0 0 0-.099.291l1.2 3.53c.034.1-.011.131-.098.069l-3.142-2.18a.303.303 0 0 0-.32 0l-3.145 2.182c-.087.06-.132.03-.099-.068l1.2-3.53a.271.271 0 0 0-.098-.292L2.056 6.146c-.087-.06-.071-.112.038-.112h3.884a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></span></div></div></div></div><div class="u-flex0 u-flexCenter"><div class="buttonSet"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="e4510b6d7975" data-is-label-padded="true" data-source="placement_card_footer_grid-----e4510b6d7975----2-14----------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--darker" data-action="sign-up-prompt" data-sign-in-action="multivote" data-requires-token="true" data-redirect="https://medium.com/_/vote/p/e4510b6d7975" data-action-source="placement_card_footer_grid-----e4510b6d7975----2-14----------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-relative u-background js-actionMultirecommendCount u-marginLeft5"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents u-marginLeft4" data-action="show-recommends" data-action-value="e4510b6d7975">809</button></span></div></div><div class="u-height20 u-borderRightLighter u-inlineBlock u-relative u-marginRight10 u-marginLeft12"></div><div class="buttonSet"><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="sign-up-prompt" data-sign-in-action="add-to-bookmarks" data-requires-token="true" data-redirect="https://medium.com/_/bookmark/p/e4510b6d7975" data-action-source="placement_card_footer_grid-----e4510b6d7975----2-14----------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button></div></div></div></div></div></div></div></div></div></div><div class="u-padding0 u-clearfix u-backgroundGrayLightest u-print-hide supplementalPostContent js-responsesWrapper" data-action-scope="_actionscope_5"><div class="container u-maxWidth740"><div class="responsesStreamWrapper u-maxWidth640 u-hide js-responsesStreamWrapper"><div class="container responsesStream-title u-paddingTop15"><div class="row"><header class="heading"><div class="u-clearfix"><div class="heading-content u-floatLeft"><span class="heading-title heading-title--semibold">Responses</span></div></div></header></div></div><div class="responsesStream js-responsesStream"></div><div class="container u-hide js-showOtherResponses"><div class="row"><button class="button button--primary button--withChrome u-accentColor--buttonNormal responsesStream-showOtherResponses cardChromeless u-width100pct u-marginVertical20 u-heightAuto" data-action="show-other-responses">Show all responses</button></div></div><div class="responsesStream js-responsesStreamOther"></div></div></div></div><div class="supplementalPostContent js-heroPromo"></div></footer></article></main><aside class="u-marginAuto u-maxWidth1032 js-postLeftSidebar"><div class="u-foreground u-top0 u-transition--fadeOut300 u-fixed u-sm-hide js-postShareWidget" data-scroll="fixed" style="transform: translateY(150px);"><div class="u-breakWord u-md-hide u-width131"><div class="u-width131 collection-title u-fontWeightBold u-fontSize18 u-lineHeightTight"><a href="https://ai-alignment.com?source=logo-lo_JuU2p2EvGepj">AI Alignment</a></div><div class="u-width131 u-multiline-clamp u-textColorNormal u-fontSize14 u-lineHeightTight u-paddingTop3">Aligning AI systems with human interests.</div><div class="u-paddingTop15 u-paddingBottom30 u-borderBottomLight u-marginBottom30"><button class="button button--primary button--small u-noUserSelect button--withChrome u-accentColor--buttonNormal js-relationshipButton" data-action="sign-up-prompt" data-sign-in-action="toggle-follow-collection" data-requires-token="true" data-redirect="https://medium.com/_/subscribe/collection/ai-control" data-action-source="post_sidebar----624d886c4aa4----------------------post_sidebar" data-collection-id="624d886c4aa4"><span class="button-label  js-buttonLabel">Follow</span></button></div></div><ul><li class="u-marginVertical10"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="8218fa8a871f" data-is-icon-29px="true" data-has-recommend-list="true" data-source="post_share_widget-----8218fa8a871f---------------------clap_sidebar"><div class="u-relative u-foreground"><button class="button button--primary button--large button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--darker" data-action="sign-up-prompt" data-sign-in-action="multivote" data-requires-token="true" data-redirect="https://medium.com/_/vote/p/8218fa8a871f" data-action-source="post_share_widget-----8218fa8a871f---------------------clap_sidebar" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><g fill-rule="evenodd"><path d="M13.739 1l.761 2.966L15.261 1z"></path><path d="M16.815 4.776l1.84-2.551-1.43-.471z"></path><path d="M10.378 2.224l1.84 2.551-.408-3.022z"></path><path d="M22.382 22.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L6.11 15.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L8.43 9.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L20.628 15c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM12.99 6.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><g fill-rule="evenodd"><path d="M13.738 1l.762 2.966L15.262 1z"></path><path d="M18.634 2.224l-1.432-.47-.408 3.022z"></path><path d="M11.79 1.754l-1.431.47 1.84 2.552z"></path><path d="M24.472 14.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M14.58 10.887c-.156-.83.096-1.569.692-2.142L12.78 6.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M17.812 10.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L9.2 7.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L7.046 9.54 5.802 8.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394l1.241 1.241 4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L4.89 11.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C21.74 20.8 22.271 18 20.62 14.982l-2.809-4.942z"></path></g></svg></span></span></button></div><span class="u-relative u-background js-actionMultirecommendCount u-marginLeft5"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton" data-action="show-recommends" data-action-value="8218fa8a871f">4</button></span></div></li><li class="u-marginVertical10 u-marginLeft3"><button class="button button--large button--dark button--chromeless is-touchIconFadeInPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="sign-up-prompt" data-sign-in-action="add-to-bookmarks" data-requires-token="true" data-redirect="https://medium.com/_/bookmark/p/8218fa8a871f" data-action-source="post_share_widget-----8218fa8a871f---------------------bookmark_sidebar"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M19.385 4h-9.77A2.623 2.623 0 0 0 7 6.615V23.01a1.022 1.022 0 0 0 1.595.847l5.905-4.004 5.905 4.004A1.022 1.022 0 0 0 22 23.011V6.62A2.625 2.625 0 0 0 19.385 4zM21 23l-5.91-3.955-.148-.107a.751.751 0 0 0-.884 0l-.147.107L8 23V6.615C8 5.725 8.725 5 9.615 5h9.77C20.275 5 21 5.725 21 6.615V23z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M19.385 4h-9.77A2.623 2.623 0 0 0 7 6.615V23.01a1.022 1.022 0 0 0 1.595.847l5.905-4.004 5.905 4.004A1.022 1.022 0 0 0 22 23.011V6.62A2.625 2.625 0 0 0 19.385 4z" fill-rule="evenodd"></path></svg></span></span></button></li><li class="u-marginVertical10 u-marginLeft3"><a class="button button--dark button--chromeless u-baseColor--buttonDark button--withIcon button--withSvgIcon button--dark button--chromeless" href="https://medium.com/p/8218fa8a871f/share/twitter" title="Share on Twitter" aria-label="Share on Twitter" target="_blank" data-action-source="post_share_widget"><span class="button-defaultState"><span class="svgIcon svgIcon--twitterFilled svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M22.053 7.54a4.474 4.474 0 0 0-3.31-1.455 4.526 4.526 0 0 0-4.526 4.524c0 .35.04.7.082 1.05a12.9 12.9 0 0 1-9.3-4.77c-.39.69-.61 1.46-.65 2.26.03 1.6.83 2.99 2.02 3.79-.72-.02-1.41-.22-2.02-.57-.01.02-.01.04 0 .08-.01 2.17 1.55 4 3.63 4.44-.39.08-.79.13-1.21.16-.28-.03-.57-.05-.81-.08.54 1.77 2.21 3.08 4.2 3.15a9.564 9.564 0 0 1-5.66 1.94c-.34-.03-.7-.06-1.05-.08 2 1.27 4.38 2.02 6.94 2.02 8.31 0 12.86-6.9 12.84-12.85.02-.24.01-.43 0-.65.89-.62 1.65-1.42 2.26-2.34-.82.38-1.69.62-2.59.72a4.37 4.37 0 0 0 1.94-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></span></span></a></li><li class="u-marginVertical10 u-marginLeft3"><a class="button button--dark button--chromeless u-baseColor--buttonDark button--withIcon button--withSvgIcon button--dark button--chromeless" href="https://medium.com/p/8218fa8a871f/share/facebook" title="Share on Facebook" aria-label="Share on Facebook" target="_blank" data-action-source="post_share_widget"><span class="button-defaultState"><span class="svgIcon svgIcon--facebookSquare svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M23.209 5H5.792A.792.792 0 0 0 5 5.791V23.21c0 .437.354.791.792.791h9.303v-7.125H12.72v-2.968h2.375v-2.375c0-2.455 1.553-3.662 3.741-3.662 1.049 0 1.95.078 2.213.112v2.565h-1.517c-1.192 0-1.469.567-1.469 1.397v1.963h2.969l-.594 2.968h-2.375L18.11 24h5.099a.791.791 0 0 0 .791-.791V5.79a.791.791 0 0 0-.791-.79"></path></svg></span></span></a></li></ul></div></aside><div class="u-fixed u-bottom0 u-width100pct u-backgroundWhite u-boxShadowTop u-borderBox u-paddingTop10 u-paddingBottom10 u-zIndexMetabar u-xs-hide js-stickyFooter"><div class="u-maxWidth700 u-marginAuto u-flexCenter"><div class="u-fontSize16 u-flex1 u-flexCenter"><div class="u-flex0 u-inlineBlock u-paddingRight20"><a class="link u-baseColor--link avatar avatar--roundedRectangle" href="https://ai-alignment.com" title="Go to AI Alignment" aria-label="Go to AI Alignment" data-collection-slug="ai-control"><img src="https://cdn-images-1.medium.com/fit/c/40/40/1*N56Qc5-aHTcfGff0scntKQ.png" class="avatar-image avatar-image--smaller" alt="AI Alignment"></a></div><div class="u-flex1 u-inlineBlock">Never miss a story from<strong> AI Alignment</strong>, when you sign up for Medium. <a class="link u-baseColor--link link--accent u-accentColor--textNormal u-accentColor--textDarken" href="https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg" data-action-source="sticky_footer">Learn more</a></div></div><div class="u-marginLeft50"><button class="button button--primary button--dark is-active u-noUserSelect button--withChrome u-accentColor--buttonDark u-uiTextSemibold u-textUppercase u-fontSize12 button--followCollection js-followCollectionButton" data-action="sign-up-prompt" data-sign-in-action="toggle-subscribe-collection" data-requires-token="true" data-redirect="https://medium.com/_/subscribe/collection/ai-control" data-action-source="sticky_footer----624d886c4aa4----------------------follow_metabar"><span class="button-label  button-defaultState js-buttonLabel">Get updates</span><span class="button-label button-activeState">Get updates</span></button></div></div></div><style class="js-collectionStyle">
.u-accentColor--borderLight {border-color: #02B875 !important;}
.u-accentColor--borderNormal {border-color: #02B875 !important;}
.u-accentColor--borderDark {border-color: #1C9963 !important;}
.u-accentColor--iconLight .svgIcon,.u-accentColor--iconLight.svgIcon {fill: #02B875 !important;}
.u-accentColor--iconNormal .svgIcon,.u-accentColor--iconNormal.svgIcon {fill: #02B875 !important;}
.u-accentColor--iconDark .svgIcon,.u-accentColor--iconDark.svgIcon {fill: #1C9963 !important;}
.u-accentColor--textNormal {color: #1C9963 !important;}
.u-accentColor--hoverTextNormal:hover {color: #1C9963 !important;}
.u-accentColor--textNormal.u-accentColor--textDarken:hover {color: #1C9963 !important;}
.u-accentColor--textDark {color: #1C9963 !important;}
.u-accentColor--backgroundLight {background-color: #02B875 !important;}
.u-accentColor--backgroundNormal {background-color: #02B875 !important;}
.u-accentColor--backgroundDark {background-color: #1C9963 !important;}
.u-accentColor--buttonDark {border-color: #1C9963 !important; color: #1C9963 !important;}
.u-accentColor--buttonDark:hover {border-color: #1C9963 !important;}
.u-accentColor--buttonDark .icon:before,.u-accentColor--buttonDark .svgIcon{color: #1C9963 !important; fill: #1C9963 !important;}
.u-accentColor--buttonNormal:not(.clapButton--largePill) {border-color: #02B875 !important; color: #1C9963 !important;}
.u-accentColor--buttonNormal:hover {border-color: #1C9963 !important;}
.u-accentColor--buttonNormal .icon:before,.u-accentColor--buttonNormal .svgIcon{color: #02B875 !important; fill: #02B875 !important;}
.u-accentColor--buttonNormal.button--filled .icon:before,.u-accentColor--buttonNormal.button--filled .svgIcon{color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-accentColor--buttonDark.button--filled,.u-accentColor--buttonDark.button--withChrome.is-active,.u-accentColor--fillWhenActive.is-active {background-color: #1C9963 !important; border-color: #1C9963 !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-accentColor--buttonNormal.button--filled:not(.clapButton--largePill),.u-accentColor--buttonNormal.button--withChrome.is-active:not(.clapButton--largePill) {background-color: #02B875 !important; border-color: #02B875 !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.postArticle.is-withAccentColors .markup--user,.postArticle.is-withAccentColors .markup--query {color: #1C9963 !important;}
.u-accentColor--highlightFaint {background-color: rgba(233, 253, 240, 1) !important;}
.u-accentColor--highlightStrong.is-active .svgIcon {fill: rgba(125, 255, 179, 1) !important;}
.postArticle.is-withAccentColors .markup--quote.is-other {background-color: rgba(233, 253, 240, 1) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .markup--quote.is-other {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(233, 253, 240, 1), rgba(233, 253, 240, 1));}
.postArticle.is-withAccentColors .markup--quote.is-me {background-color: rgba(173, 255, 207, 1) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .markup--quote.is-me {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(173, 255, 207, 1), rgba(173, 255, 207, 1));}
.postArticle.is-withAccentColors .markup--quote.is-targeted {background-color: rgba(125, 255, 179, 1) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .markup--quote.is-targeted {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(125, 255, 179, 1), rgba(125, 255, 179, 1));}
.postArticle.is-withAccentColors .markup--quote.is-selected {background-color: rgba(125, 255, 179, 1) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .markup--quote.is-selected {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(125, 255, 179, 1), rgba(125, 255, 179, 1));}
.postArticle.is-withAccentColors .markup--highlight {background-color: rgba(125, 255, 179, 1) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .markup--highlight {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(125, 255, 179, 1), rgba(125, 255, 179, 1));}.u-baseColor--iconNormal.avatar-halo {fill: rgba(0, 0, 0, 0.4980392156862745) !important;}</style><style class="js-collectionStyleConstant">.u-imageBgColor {background-color: rgba(0, 0, 0, 0.24705882352941178);}
.u-imageSpectrum .u-baseColor--borderLight {border-color: rgba(255, 255, 255, 0.6980392156862745) !important;}
.u-imageSpectrum .u-baseColor--borderNormal {border-color: rgba(255, 255, 255, 0.8980392156862745) !important;}
.u-imageSpectrum .u-baseColor--borderDark {border-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--iconLight .svgIcon,.u-imageSpectrum .u-baseColor--iconLight.svgIcon {fill: rgba(255, 255, 255, 0.8) !important;}
.u-imageSpectrum .u-baseColor--iconNormal .svgIcon,.u-imageSpectrum .u-baseColor--iconNormal.svgIcon {fill: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--iconDark .svgIcon,.u-imageSpectrum .u-baseColor--iconDark.svgIcon {fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--textNormal {color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--textNormal.u-baseColor--textDarken:hover {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--textDark {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--textDarker {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--backgroundLight {background-color: rgba(255, 255, 255, 0.8980392156862745) !important;}
.u-imageSpectrum .u-baseColor--backgroundNormal {background-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--backgroundDark {background-color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--buttonLight {border-color: rgba(255, 255, 255, 0.6980392156862745) !important; color: rgba(255, 255, 255, 0.8) !important;}
.u-imageSpectrum .u-baseColor--buttonLight:hover {border-color: rgba(255, 255, 255, 0.6980392156862745) !important;}
.u-imageSpectrum .u-baseColor--buttonLight .icon:before,.u-imageSpectrum .u-baseColor--buttonLight .svgIcon {color: rgba(255, 255, 255, 0.8) !important; fill: rgba(255, 255, 255, 0.8) !important;}
.u-imageSpectrum .u-baseColor--buttonDark {border-color: rgba(255, 255, 255, 0.9490196078431372) !important; color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--buttonDark:hover {border-color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--buttonDark .icon:before,.u-imageSpectrum .u-baseColor--buttonDark .svgIcon {color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--buttonNormal {border-color: rgba(255, 255, 255, 0.8980392156862745) !important; color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--buttonNormal:hover {border-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--buttonNormal .icon:before,.u-imageSpectrum .u-baseColor--buttonNormal .svgIcon {color: rgba(255, 255, 255, 0.9490196078431372) !important; fill: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--buttonDark.button--filled,.u-imageSpectrum .u-baseColor--buttonDark.button--withChrome.is-active {background-color: rgba(255, 255, 255, 1) !important; border-color: rgba(255, 255, 255, 1) !important; color: rgba(0, 0, 0, 0.24705882352941178) !important; fill: rgba(0, 0, 0, 0.24705882352941178) !important;}
.u-imageSpectrum .u-baseColor--buttonNormal.button--filled,.u-imageSpectrum .u-baseColor--buttonNormal.button--withChrome.is-active {background-color: rgba(255, 255, 255, 0.9490196078431372) !important; border-color: rgba(255, 255, 255, 0.9490196078431372) !important; color: rgba(0, 0, 0, 0.24705882352941178) !important; fill: rgba(0, 0, 0, 0.24705882352941178) !important;}
.u-imageSpectrum .u-baseColor--link {color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--link.link--darkenOnHover:hover {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--link.link--darken:hover,.u-imageSpectrum .u-baseColor--link.link--darken:focus,.u-imageSpectrum .u-baseColor--link.link--darken:active {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--link.link--dark {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--link.link--dark.link--darken:hover,.u-imageSpectrum .u-baseColor--link.link--dark.link--darken:focus,.u-imageSpectrum .u-baseColor--link.link--dark.link--darken:active {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--link.link--darker {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--placeholderNormal ::-webkit-input-placeholder {color: rgba(255, 255, 255, 0.8);}
.u-imageSpectrum .u-baseColor--placeholderNormal ::-moz-placeholder {color: rgba(255, 255, 255, 0.8);}
.u-imageSpectrum .u-baseColor--placeholderNormal :-ms-input-placeholder {color: rgba(255, 255, 255, 0.8);}
.u-imageSpectrum .svgIcon--logoWordmark {stroke: none !important; fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .svgIcon--logoMonogram {stroke: none !important; fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum  .ui-h1,.u-imageSpectrum  .ui-h2,.u-imageSpectrum  .ui-h3,.u-imageSpectrum  .ui-h4,.u-imageSpectrum  .ui-brand1,.u-imageSpectrum  .ui-brand2,.u-imageSpectrum  .ui-captionStrong {color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum  .ui-body,.u-imageSpectrum  .ui-caps {color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum  .ui-summary,.u-imageSpectrum  .ui-caption {color: rgba(255, 255, 255, 0.8) !important; fill: rgba(255, 255, 255, 0.8) !important;}
.u-imageSpectrum .u-accentColor--borderLight {border-color: rgba(255, 255, 255, 0.6980392156862745) !important;}
.u-imageSpectrum .u-accentColor--borderNormal {border-color: rgba(255, 255, 255, 0.8980392156862745) !important;}
.u-imageSpectrum .u-accentColor--borderDark {border-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--iconLight .svgIcon,.u-imageSpectrum .u-accentColor--iconLight.svgIcon {fill: rgba(255, 255, 255, 0.8) !important;}
.u-imageSpectrum .u-accentColor--iconNormal .svgIcon,.u-imageSpectrum .u-accentColor--iconNormal.svgIcon {fill: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--iconDark .svgIcon,.u-imageSpectrum .u-accentColor--iconDark.svgIcon {fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--textNormal {color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--hoverTextNormal:hover {color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--textNormal.u-accentColor--textDarken:hover {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--textDark {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--backgroundLight {background-color: rgba(255, 255, 255, 0.8980392156862745) !important;}
.u-imageSpectrum .u-accentColor--backgroundNormal {background-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--backgroundDark {background-color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--buttonDark {border-color: rgba(255, 255, 255, 0.9490196078431372) !important; color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--buttonDark:hover {border-color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--buttonDark .icon:before,.u-imageSpectrum .u-accentColor--buttonDark .svgIcon{color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--buttonNormal:not(.clapButton--largePill) {border-color: rgba(255, 255, 255, 0.8980392156862745) !important; color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--buttonNormal:hover {border-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--buttonNormal .icon:before,.u-imageSpectrum .u-accentColor--buttonNormal .svgIcon{color: rgba(255, 255, 255, 0.9490196078431372) !important; fill: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--buttonNormal.button--filled .icon:before,.u-imageSpectrum .u-accentColor--buttonNormal.button--filled .svgIcon{color: rgba(0, 0, 0, 0.24705882352941178) !important; fill: rgba(0, 0, 0, 0.24705882352941178) !important;}
.u-imageSpectrum .u-accentColor--buttonDark.button--filled,.u-imageSpectrum .u-accentColor--buttonDark.button--withChrome.is-active,.u-imageSpectrum .u-accentColor--fillWhenActive.is-active {background-color: rgba(255, 255, 255, 1) !important; border-color: rgba(255, 255, 255, 1) !important; color: rgba(0, 0, 0, 0.24705882352941178) !important; fill: rgba(0, 0, 0, 0.24705882352941178) !important;}
.u-imageSpectrum .u-accentColor--buttonNormal.button--filled:not(.clapButton--largePill),.u-imageSpectrum .u-accentColor--buttonNormal.button--withChrome.is-active:not(.clapButton--largePill) {background-color: rgba(255, 255, 255, 0.9490196078431372) !important; border-color: rgba(255, 255, 255, 0.9490196078431372) !important; color: rgba(0, 0, 0, 0.24705882352941178) !important; fill: rgba(0, 0, 0, 0.24705882352941178) !important;}
.u-imageSpectrum .postArticle.is-withAccentColors .markup--user,.u-imageSpectrum .postArticle.is-withAccentColors .markup--query {color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--highlightFaint {background-color: rgba(255, 255, 255, 0.2) !important;}
.u-imageSpectrum .u-accentColor--highlightStrong.is-active .svgIcon {fill: rgba(255, 255, 255, 0.6) !important;}
.postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-other {background-color: rgba(255, 255, 255, 0.2) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-other {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 0.2));}
.postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-me {background-color: rgba(255, 255, 255, 0.4) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-me {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(255, 255, 255, 0.4), rgba(255, 255, 255, 0.4));}
.postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-targeted {background-color: rgba(255, 255, 255, 0.6) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-targeted {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(255, 255, 255, 0.6), rgba(255, 255, 255, 0.6));}
.postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-selected {background-color: rgba(255, 255, 255, 0.6) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-selected {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(255, 255, 255, 0.6), rgba(255, 255, 255, 0.6));}
.postArticle.is-withAccentColors .u-imageSpectrum .markup--highlight {background-color: rgba(255, 255, 255, 0.6) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .u-imageSpectrum .markup--highlight {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(255, 255, 255, 0.6), rgba(255, 255, 255, 0.6));}.u-resetSpectrum .u-tintBgColor {background-color: rgba(255, 255, 255, 1) !important;}.u-resetSpectrum .u-tintBgColor .u-fadeLeft:before {background-image: linear-gradient(to right, rgba(255, 255, 255, 1) 0%, rgba(255, 255, 255, 0) 100%) !important;}.u-resetSpectrum .u-tintBgColor .u-fadeRight:after {background-image: linear-gradient(to right, rgba(255, 255, 255, 0) 0%, rgba(255, 255, 255, 1) 100%) !important;}
.u-resetSpectrum .u-baseColor--borderLight {border-color: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-baseColor--borderNormal {border-color: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--borderDark {border-color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--iconLight .svgIcon,.u-resetSpectrum .u-baseColor--iconLight.svgIcon {fill: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-baseColor--iconNormal .svgIcon,.u-resetSpectrum .u-baseColor--iconNormal.svgIcon {fill: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--iconDark .svgIcon,.u-resetSpectrum .u-baseColor--iconDark.svgIcon {fill: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--textNormal {color: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--textNormal.u-baseColor--textDarken:hover {color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--textDark {color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--textDarker {color: rgba(0, 0, 0, 0.8) !important;}
.u-resetSpectrum .u-baseColor--backgroundLight {background-color: rgba(0, 0, 0, 0.09803921568627451) !important;}
.u-resetSpectrum .u-baseColor--backgroundNormal {background-color: rgba(0, 0, 0, 0.2) !important;}
.u-resetSpectrum .u-baseColor--backgroundDark {background-color: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-baseColor--buttonLight {border-color: rgba(0, 0, 0, 0.2980392156862745) !important; color: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-baseColor--buttonLight:hover {border-color: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-baseColor--buttonLight .icon:before,.u-resetSpectrum .u-baseColor--buttonLight .svgIcon {color: rgba(0, 0, 0, 0.2980392156862745) !important; fill: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-baseColor--buttonDark {border-color: rgba(0, 0, 0, 0.6) !important; color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--buttonDark:hover {border-color: rgba(0, 0, 0, 0.8) !important;}
.u-resetSpectrum .u-baseColor--buttonDark .icon:before,.u-resetSpectrum .u-baseColor--buttonDark .svgIcon {color: rgba(0, 0, 0, 0.6) !important; fill: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--buttonNormal {border-color: rgba(0, 0, 0, 0.4980392156862745) !important; color: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--buttonNormal:hover {border-color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--buttonNormal .icon:before,.u-resetSpectrum .u-baseColor--buttonNormal .svgIcon {color: rgba(0, 0, 0, 0.4980392156862745) !important; fill: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--buttonDark.button--filled,.u-resetSpectrum .u-baseColor--buttonDark.button--withChrome.is-active {background-color: rgba(0, 0, 0, 0.2980392156862745) !important; border-color: rgba(0, 0, 0, 0.2980392156862745) !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-resetSpectrum .u-baseColor--buttonNormal.button--filled,.u-resetSpectrum .u-baseColor--buttonNormal.button--withChrome.is-active {background-color: rgba(0, 0, 0, 0.2) !important; border-color: rgba(0, 0, 0, 0.2) !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-resetSpectrum .u-baseColor--link {color: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--link.link--darkenOnHover:hover {color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--link.link--darken:hover,.u-resetSpectrum .u-baseColor--link.link--darken:focus,.u-resetSpectrum .u-baseColor--link.link--darken:active {color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--link.link--dark {color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--link.link--dark.link--darken:hover,.u-resetSpectrum .u-baseColor--link.link--dark.link--darken:focus,.u-resetSpectrum .u-baseColor--link.link--dark.link--darken:active {color: rgba(0, 0, 0, 0.8) !important;}
.u-resetSpectrum .u-baseColor--link.link--darker {color: rgba(0, 0, 0, 0.8) !important;}
.u-resetSpectrum .u-baseColor--placeholderNormal ::-webkit-input-placeholder {color: rgba(0, 0, 0, 0.2980392156862745);}
.u-resetSpectrum .u-baseColor--placeholderNormal ::-moz-placeholder {color: rgba(0, 0, 0, 0.2980392156862745);}
.u-resetSpectrum .u-baseColor--placeholderNormal :-ms-input-placeholder {color: rgba(0, 0, 0, 0.2980392156862745);}
.u-resetSpectrum .svgIcon--logoWordmark {stroke: none !important; fill: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .svgIcon--logoMonogram {stroke: none !important; fill: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum  .ui-h1,.u-resetSpectrum  .ui-h2,.u-resetSpectrum  .ui-h3,.u-resetSpectrum  .ui-h4,.u-resetSpectrum  .ui-brand1,.u-resetSpectrum  .ui-brand2,.u-resetSpectrum  .ui-captionStrong {color: rgba(0, 0, 0, 0.8) !important; fill: rgba(0, 0, 0, 0.8) !important;}
.u-resetSpectrum  .ui-body,.u-resetSpectrum  .ui-caps {color: rgba(0, 0, 0, 0.6) !important; fill: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum  .ui-summary,.u-resetSpectrum  .ui-caption {color: rgba(0, 0, 0, 0.2980392156862745) !important; fill: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-accentColor--borderLight {border-color: rgba(2, 184, 117, 1) !important;}
.u-resetSpectrum .u-accentColor--borderNormal {border-color: rgba(2, 184, 117, 1) !important;}
.u-resetSpectrum .u-accentColor--borderDark {border-color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--iconLight .svgIcon,.u-resetSpectrum .u-accentColor--iconLight.svgIcon {fill: rgba(2, 184, 117, 1) !important;}
.u-resetSpectrum .u-accentColor--iconNormal .svgIcon,.u-resetSpectrum .u-accentColor--iconNormal.svgIcon {fill: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--iconDark .svgIcon,.u-resetSpectrum .u-accentColor--iconDark.svgIcon {fill: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--textNormal {color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--hoverTextNormal:hover {color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--textNormal.u-accentColor--textDarken:hover {color: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--textDark {color: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--backgroundLight {background-color: rgba(2, 184, 117, 1) !important;}
.u-resetSpectrum .u-accentColor--backgroundNormal {background-color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--backgroundDark {background-color: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonDark {border-color: rgba(0, 171, 107, 1) !important; color: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonDark:hover {border-color: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonDark .icon:before,.u-resetSpectrum .u-accentColor--buttonDark .svgIcon{color: rgba(28, 153, 99, 1) !important; fill: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonNormal:not(.clapButton--largePill) {border-color: rgba(2, 184, 117, 1) !important; color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonNormal:hover {border-color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonNormal .icon:before,.u-resetSpectrum .u-accentColor--buttonNormal .svgIcon{color: rgba(0, 171, 107, 1) !important; fill: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonNormal.button--filled .icon:before,.u-resetSpectrum .u-accentColor--buttonNormal.button--filled .svgIcon{color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonDark.button--filled,.u-resetSpectrum .u-accentColor--buttonDark.button--withChrome.is-active,.u-resetSpectrum .u-accentColor--fillWhenActive.is-active {background-color: rgba(28, 153, 99, 1) !important; border-color: rgba(28, 153, 99, 1) !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonNormal.button--filled:not(.clapButton--largePill),.u-resetSpectrum .u-accentColor--buttonNormal.button--withChrome.is-active:not(.clapButton--largePill) {background-color: rgba(0, 171, 107, 1) !important; border-color: rgba(0, 171, 107, 1) !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-resetSpectrum .postArticle.is-withAccentColors .markup--user,.u-resetSpectrum .postArticle.is-withAccentColors .markup--query {color: rgba(0, 171, 107, 1) !important;}</style><div class="highlightMenu" data-action-scope="_actionscope_3"><div class="highlightMenu-inner"><div class="buttonSet"><a class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--chromeless button--highlightMenu js-highlightMenuTwitterShare" href="https://medium.com/p/8218fa8a871f/share/twitter" title="Share on Twitter" aria-label="Share on Twitter" target="_blank" data-action="twitter"><span class="button-defaultState"><span class="svgIcon svgIcon--twitterFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M21.725 5.338c-.744.47-1.605.804-2.513 1.006a3.978 3.978 0 0 0-2.942-1.293c-2.22 0-4.02 1.81-4.02 4.02 0 .32.034.63.07.94-3.31-.18-6.27-1.78-8.255-4.23a4.544 4.544 0 0 0-.574 2.01c.04 1.43.74 2.66 1.8 3.38-.63-.01-1.25-.19-1.79-.5v.08c0 1.93 1.38 3.56 3.23 3.95-.34.07-.7.12-1.07.14-.25-.02-.5-.04-.72-.07.49 1.58 1.97 2.74 3.74 2.8a8.49 8.49 0 0 1-5.02 1.72c-.3-.03-.62-.04-.93-.07A11.447 11.447 0 0 0 8.88 21c7.386 0 11.43-6.13 11.414-11.414.015-.21.01-.38 0-.578a7.604 7.604 0 0 0 2.01-2.08 7.27 7.27 0 0 1-2.297.645 3.856 3.856 0 0 0 1.72-2.23"></path></svg></span></span></a><div class="buttonSet-separator"></div><button class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--highlightMenu" data-action="sign-up-prompt" data-sign-in-action="highlight" data-redirect="https://ai-alignment.com/efficient-and-safely-scalable-8218fa8a871f" data-skip-onboarding="true" data-action-source="quote_menu--------------------------privatenote_text"><span class="svgIcon svgIcon--privatenoteFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M17.662 4.552H7.346A4.36 4.36 0 0 0 3 8.898v5.685c0 2.168 1.614 3.962 3.697 4.28v2.77c0 .303.35.476.59.29l3.904-2.994h6.48c2.39 0 4.35-1.96 4.35-4.35V8.9c0-2.39-1.95-4.346-4.34-4.346zM16 14.31a.99.99 0 0 1-1.003.99h-4.994C9.45 15.3 9 14.85 9 14.31v-3.02a.99.99 0 0 1 1-.99v-.782a2.5 2.5 0 0 1 2.5-2.51c1.38 0 2.5 1.13 2.5 2.51v.782c.552.002 1 .452 1 .99v3.02z"></path><path d="M14 9.81c0-.832-.674-1.68-1.5-1.68-.833 0-1.5.84-1.5 1.68v.49h3v-.49z"></path></g></svg></span></button></div></div><div class="highlightMenu-arrowClip"><span class="highlightMenu-arrow"></span></div></div></div></div></div><div class="loadingBar"></div><script>// <![CDATA[
window["obvInit"] = function (opt_embedded) {window["obvInit"]["embedded"] = opt_embedded; window["obvInit"]["ready"] = true;}
// ]]></script><script>// <![CDATA[
var GLOBALS = {"audioUrl":"https://d1fcbxp97j4nb2.cloudfront.net","baseUrl":"https://ai-alignment.com","buildLabel":"37496-7f20643","currentUser":{"userId":"lo_JuU2p2EvGepj","isVerified":false,"subscriberEmail":"","hasPastMemberships":false,"isEnrolledInHightower":false,"isEligibleForHightower":false,"hightowerLastLockedAt":0,"isWriterProgramEnrolled":true,"isWriterProgramInvited":false,"isWriterProgramOptedOut":false,"writerProgramVersion":0,"writerProgramEnrolledAt":0,"friendLinkOnboarding":0,"hasAdditionalUnlocks":false,"hasApiAccess":false,"isQuarantined":false,"writerProgramDistributionSettingOptedIn":false},"currentUserHasUnverifiedEmail":false,"isAuthenticated":false,"isCurrentUserVerified":false,"miroUrl":"https://cdn-images-1.medium.com","moduleUrls":{"base":"https://cdn-static-1.medium.com/_/fp/gen-js/main-base.bundle.h-S66qYGELCwGMjo-I5sGg.js","common-async":"https://cdn-static-1.medium.com/_/fp/gen-js/main-common-async.bundle.qFZkgzLZ5TYXIerh_w9awQ.js","hightower":"https://cdn-static-1.medium.com/_/fp/gen-js/main-hightower.bundle.YYDLiHqz4VuEAfIKbpgHlA.js","home-screens":"https://cdn-static-1.medium.com/_/fp/gen-js/main-home-screens.bundle.KOsn4BMHvTHwO7kChpWnwQ.js","misc-screens":"https://cdn-static-1.medium.com/_/fp/gen-js/main-misc-screens.bundle.9HQt5flvjhqNbC82Kz7w3A.js","notes":"https://cdn-static-1.medium.com/_/fp/gen-js/main-notes.bundle.V05mXLtyLz2Mj5DzEML26A.js","payments":"https://cdn-static-1.medium.com/_/fp/gen-js/main-payments.bundle.4s7BX_pcnrqTR9pXdg44lw.js","posters":"https://cdn-static-1.medium.com/_/fp/gen-js/main-posters.bundle.oAF5QtbeXBsEy2D-ZeKzOA.js","power-readers":"https://cdn-static-1.medium.com/_/fp/gen-js/main-power-readers.bundle.242jbfrxhns9kmBC8hYbGA.js","pubs":"https://cdn-static-1.medium.com/_/fp/gen-js/main-pubs.bundle.OVIBk1ifJgWYBiuHy2mNlA.js","stats":"https://cdn-static-1.medium.com/_/fp/gen-js/main-stats.bundle.FRAzB3YCXktr7d19iN9skg.js"},"previewConfig":{"weightThreshold":1,"weightImageParagraph":0.51,"weightIframeParagraph":0.8,"weightTextParagraph":0.08,"weightEmptyParagraph":0,"weightP":0.003,"weightH":0.005,"weightBq":0.003,"minPTextLength":60,"truncateBoundaryChars":20,"detectTitle":true,"detectTitleLevThreshold":0.15},"productName":"Medium","supportsEdit":false,"termsUrl":"//medium.com/policy/9db0094a1e0f","textshotHost":"textshot.medium.com","transactionId":"1557467313753:bf6a757159e7","useragent":{"browser":"headlesschrome","family":"chrome","os":"mac","version":0,"supportsDesktopEdit":false,"supportsInteract":false,"supportsView":true,"isMobile":false,"isTablet":false,"isNative":false,"supportsFileAPI":false,"isTier1":false,"clientVersion":"","unknownParagraphsBad":false,"clientChannel":"","supportsRealScrollEvents":false,"supportsVhUnits":false,"ruinsViewportSections":false,"supportsHtml5Video":false,"supportsMagicUnderlines":false,"isWebView":false,"isFacebookWebView":false,"supportsProgressiveMedia":false,"supportsPromotedPosts":true,"isBot":false,"isNativeIphone":false,"supportsCssVariables":false,"supportsVideoSections":true,"emojiSupportLevel":5,"isSearchBot":false,"isSyndicationBot":false,"isNativeAndroid":false,"isNativeIos":false,"supportsScrollableMetabar":false},"variants":{"allow_access":true,"allow_signup":true,"allow_test_auth":"disallow","signin_services":"twitter,facebook,google,email,google-fastidv,google-one-tap","signup_services":"twitter,facebook,google,email,google-fastidv,google-one-tap","google_sign_in_android":true,"reengagement_notification_duration":3,"browsable_stream_config_bucket":"curated-topics","enable_dedicated_series_tab_api_ios":true,"enable_post_import":true,"available_monthly_plan":"60e220181034","available_annual_plan":"2c754bcc2995","disable_ios_resume_reading_toast":true,"is_not_medium_subscriber":true,"glyph_font_set":"m2","enable_branding":true,"enable_branding_fonts":true,"max_premium_content_per_user_under_metering":3,"enable_automated_mission_control_triggers":true,"enable_lite_profile":true,"enable_marketing_emails":true,"enable_topic_lifecycle_email":true,"enable_parsely":true,"enable_branch_io":true,"enable_ios_post_stats":true,"enable_lite_topics":true,"enable_lite_stories":true,"redis_read_write_splitting":true,"enable_tipalti_onboarding":true,"enable_annual_renewal_reminder_email":true,"enable_janky_spam_rules":"users,posts","enable_new_collaborative_filtering_data":true,"android_rating_prompt_stories_read_threshold":2,"stripe_v3":true,"enable_google_one_tap":true,"enable_email_sign_in_captcha":true,"enable_rito_with_viewer_query":true,"enable_rito_with_flag_query":true,"enable_rito_post_handler":true,"enable_rito_sequence_post_recirc_query":true,"enable_rito_post_recirc_query":true,"enable_rito_topic_handler":true,"enable_rito_stats_post_handler":true,"enable_rito_stats_post_chart":true,"enable_rito_lifetime_earnings_tooltip":true,"enable_rito_stats_post_referrers_container":true,"enable_rito_post_feature_mutation":true,"enable_rito_post_unfeature_mutation":true,"enable_rito_quote_delete_mutation":true,"enable_rito_user_block_mutation":true,"enable_rito_user_unblock_mutation":true,"enable_rito_report_user_link":true,"enable_rito_bookmark_post_default":true,"enable_rito_unbookmark_post_default":true,"enable_rito_archive_post_default":true,"enable_rito_unarchive_post_default":true,"enable_rito_clap":true,"enable_rito_subscribe_series":true,"enable_rito_unsubscribe_series":true,"enable_rito_follow_topic":true,"enable_rito_unfollow_topic":true,"enable_rito_follow_user":true,"enable_rito_unfollow_user":true,"enable_rito_your_story_delete_mutation":true,"enable_rito_update_last_read_section":true,"editorial_push_notifications":true,"enable_primary_topic_for_mobile":true,"enable_rito_sequence_post_handler":true,"enable_todays_highlights_ios":true,"enable_logged_out_homepage_signup":true,"use_new_admin_topic_backend":true,"enable_quarantine_rules":true,"enable_lite_privacy_banner":true,"enable_patronus_on_kubernetes":true,"pub_sidebar":true,"disable_mobile_featured_chunk":true,"enable_rito_user_profile_overview_handler":true,"enable_rito_user_stream_overview":true,"enable_rito_user_profile_latest_handler":true,"enable_rito_user_stream_latest":true,"enable_rito_user_profile_actions":true,"enable_rito_post_actions":true,"enable_rito_user_profile_highlights_handler":true,"enable_rito_user_stream_highlights":true,"enable_rito_user_profile_series_handler":true,"enable_rito_user_stream_series":true,"enable_rito_user_profile_claps_handler":true,"enable_rito_user_stream_claps":true,"enable_rito_user_profile_responses_handler":true,"enable_rito_user_stream_responses":true,"enable_rito_billing_history_handler":true,"enable_rito_your_stories_handler":true,"enable_rito_sequence_library_handler":true,"enable_rito_series_handler":true,"enable_rito_amppost_handler":true,"enable_rito_follow_collection_mutation":true,"enable_rito_unfollow_collection_mutation":true,"enable_pub_newsletters":true,"enable_may_meter_email_test":true,"enable_rex_app_highlights":true,"enable_new_user_avatar_dropdown_menu":true,"enable_mobile_pubcrawl_home_feed":true,"enable_draft_in_post_cotent":true},"xsrfToken":"","iosAppId":"828256236","supportEmail":"yourfriends@medium.com","fp":{"/icons/monogram-mask.svg":"https://cdn-static-1.medium.com/_/fp/icons/monogram-mask.KPLCSFEZviQN0jQ7veN2RQ.svg","/icons/favicon-dev-editor.ico":"https://cdn-static-1.medium.com/_/fp/icons/favicon-dev-editor.YKKRxBO8EMvIqhyCwIiJeQ.ico","/icons/favicon-hatch-editor.ico":"https://cdn-static-1.medium.com/_/fp/icons/favicon-hatch-editor.BuEyHIqlyh2s_XEk4Rl32Q.ico","/icons/favicon-medium-editor.ico":"https://cdn-static-1.medium.com/_/fp/icons/favicon-medium-editor.PiakrZWB7Yb80quUVQWM6g.ico"},"authBaseUrl":"https://medium.com","imageUploadSizeMb":25,"isAuthDomainRequest":false,"domainCollectionSlug":"ai-control","algoliaApiEndpoint":"https://MQ57UUUQZ2-dsn.algolia.net","algoliaAppId":"MQ57UUUQZ2","algoliaSearchOnlyApiKey":"394474ced050e3911ae2249ecc774921","iosAppStoreUrl":"https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8","iosAppLinkBaseUrl":"medium:","algoliaIndexPrefix":"medium_","androidPlayStoreUrl":"https://play.google.com/store/apps/details?id=com.medium.reader","googleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","androidPackage":"com.medium.reader","androidPlayStoreMarketScheme":"market://details?id=com.medium.reader","googleAuthUri":"https://accounts.google.com/o/oauth2/auth","androidScheme":"medium","layoutData":{"useDynamicScripts":false,"googleAnalyticsTrackingCode":"UA-24232453-2","jsShivUrl":"https://cdn-static-1.medium.com/_/fp/js/shiv.RI2ePTZ5gFmMgLzG5bEVAA.js","useDynamicCss":false,"faviconUrl":"https://cdn-static-1.medium.com/_/fp/icons/favicon-rebrand-medium.3Y6xpZ-0FSdWDnPM3hSBIA.ico","faviconImageId":"1*8I-HPL0bfoIzGied-dzOvA.png","fontSets":[{"id":8,"url":"https://glyph.medium.com/css/e/sr/latin/e/ssr/latin/e/ssb/latin/m2.css"},{"id":11,"url":"https://glyph.medium.com/css/m2.css"},{"id":9,"url":"https://glyph.medium.com/css/mkt.css"}],"editorFaviconUrl":"https://cdn-static-1.medium.com/_/fp/icons/favicon-rebrand-medium-editor.3Y6xpZ-0FSdWDnPM3hSBIA.ico","glyphUrl":"https://glyph.medium.com"},"authBaseUrlRev":"moc.muidem//:sptth","isDnt":false,"stripePublishableKey":"pk_live_7FReX44VnNIInZwrIIx6ghjl","archiveUploadSizeMb":100,"paymentData":{"currencies":{"1":{"label":"US Dollar","external":"usd"}},"countries":{"1":{"label":"United States of America","external":"US"}},"accountTypes":{"1":{"label":"Individual","external":"individual"},"2":{"label":"Company","external":"company"}}},"previewConfig2":{"weightThreshold":1,"weightImageParagraph":0.05,"raiseImage":true,"enforceHeaderHierarchy":true,"isImageInsetRight":true},"isAmp":false,"iosScheme":"medium","isSwBoot":false,"lightstep":{"accessToken":"ce5be895bef60919541332990ac9fef2","carrier":"{\"ot-tracer-spanid\":\"60500aaf5f5feb52\",\"ot-tracer-traceid\":\"160f6b435c6b1158\",\"ot-tracer-sampled\":\"true\"}","host":"collector-medium.lightstep.com"},"facebook":{"key":"542599432471018","namespace":"medium-com","scope":{"default":["public_profile","email"],"connect":["public_profile","email"],"login":["public_profile","email"],"share":["public_profile","email"]}},"editorsPicksTopicId":"3985d2a191c5","popularOnMediumTopicId":"9d34e48ecf94","memberContentTopicId":"13d7efd82fb2","audioContentTopicId":"3792abbd134","brandedSequenceId":"7d337ddf1941","isDoNotAuth":false,"buggle":{"url":"https://buggle.medium.com","videoUrl":"https://cdn-videos-1.medium.com","audioUrl":"https://cdn-audio-1.medium.com"},"referrerType":5,"isMeteredOut":false,"meterConfig":{"maxUnlockCount":3,"windowLength":"MONTHLY"},"partnerProgramEmail":"partnerprogram@medium.com","userResearchPrompts":[{"promptId":"lo_post_page_4","type":0,"url":"www.calendly.com"},{"promptId":"lo_home_page","type":1,"url":"www.calendly.com"},{"promptId":"lo_profile_page","type":2,"url":"www.calendly.com"}],"recaptchaKey":"6LdAokEUAAAAAC7seICd4vtC8chDb3jIXDQulyUJ","signinWallCustomDomainCollectionIds":["3a8144eabfe3","336d898217ee","61061eb0c96b","138adf9c44c","819cc2aaeee0"],"countryCode":"US","bypassMeter":false,"branchKey":"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm","paypal":{"clientMode":"production","oneYearGift":{"name":"Medium Membership (1 Year, Digital Gift Code)","description":"Unlimited access to the best and brightest stories on Medium. Gift codes can be redeemed at medium.com/redeem.","price":"50.00","currency":"USD","sku":"membership-gift-1-yr"}},"collectionConfig":{"mediumOwnedAndOperatedCollectionIds":["544c7006046e // Human Parts","bcc38c8f6edf // Matter","444d13b52878 // OneZero","8d6b8a439e32 // Elemental","92d2092dc598 // Gay Mag","1285ba81cada // Heated"]}}
// ]]></script><script charset="UTF-8" src="https://cdn-static-1.medium.com/_/fp/gen-js/main-base.bundle.h-S66qYGELCwGMjo-I5sGg.js" async=""></script><script>// <![CDATA[
window["obvInit"]({"value":{"id":"8218fa8a871f","versionId":"77580728ea52","creatorId":"57f1a655a613","creator":{"userId":"57f1a655a613","name":"Paul Christiano","username":"paulfchristiano","createdAt":1417286353352,"imageId":"1*BNjZCuQuRfIgcXCBMipuBw.jpeg","backgroundImageId":"","bio":"OpenAI","twitterScreenName":"","socialStats":{"userId":"57f1a655a613","usersFollowedCount":93,"usersFollowedByCount":821,"type":"SocialStats"},"social":{"userId":"lo_1HnsOvgcVrD4","targetUserId":"57f1a655a613","type":"Social"},"facebookAccountId":"1167284919","allowNotes":1,"mediumMemberAt":0,"isNsfw":false,"isWriterProgramEnrolled":true,"isQuarantined":false,"type":"User"},"homeCollection":{"id":"624d886c4aa4","name":"AI Alignment","slug":"ai-control","tags":[],"creatorId":"57f1a655a613","description":"Aligning AI systems with human interests.","shortDescription":"Aligning AI systems with human interests.","image":{"imageId":"1*N56Qc5-aHTcfGff0scntKQ.png","filter":"","backgroundSize":"","originalWidth":512,"originalHeight":512,"strategy":"resample","height":0,"width":0},"metadata":{"followerCount":2834,"activeAt":1548040822588},"virtuals":{"permissions":{"canPublish":false,"canPublishAll":false,"canRepublish":false,"canRemove":false,"canManageAll":false,"canSubmit":false,"canEditPosts":false,"canAddWriters":false,"canViewStats":false,"canSendNewsletter":false,"canViewLockedPosts":false,"canViewCloaked":false,"canEditOwnPosts":false,"canBeAssignedAuthor":false,"canEnrollInHightower":false,"canLockPostsForMediumMembers":false,"canLockOwnPostsForMediumMembers":false},"isSubscribed":false,"isNewsletterSubscribed":false,"isEnrolledInHightower":false,"isEligibleForHightower":false},"logo":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"collectionMastheadId":"29f3dcc2e4","domain":"ai-alignment.com","sections":[{"type":2,"collectionHeaderMetadata":{"title":"AI Alignment","backgroundImage":{},"logoImage":{},"alignment":2,"layout":4}},{"type":1,"postListMetadata":{"source":3,"layout":4,"number":3,"postIds":["157debfd1616","b49ad992940b","b959644d79c2"]}},{"type":1,"postListMetadata":{"source":1,"layout":4,"number":24,"postIds":[],"sectionHeader":"Latest"}}],"favicon":{"imageId":"1*cciPf4CUXd_Zyux0Jg0yBQ.png","filter":"","backgroundSize":"","originalWidth":400,"originalHeight":400,"strategy":"resample","height":0,"width":0},"colorPalette":{"defaultBackgroundSpectrum":{"colorPoints":[{"color":"#FF02B875","point":0},{"color":"#FF00AB6B","point":0.1},{"color":"#FF1C9963","point":0.2},{"color":"#FF092E20","point":1}],"backgroundColor":"#FFFFFFFF"},"highlightSpectrum":{"colorPoints":[{"color":"#FFFFFFFF","point":0},{"color":"#FFE9FDF0","point":0.1},{"color":"#FFE2FAEE","point":0.2},{"color":"#FFADFFCF","point":0.6},{"color":"#FF7DFFB3","point":1}],"backgroundColor":"#FFFFFFFF"}},"navItems":[],"colorBehavior":1,"instantArticlesState":0,"acceleratedMobilePagesState":0,"ampLogo":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"header":{"title":"AI Alignment","backgroundImage":{},"logoImage":{},"alignment":2,"layout":4},"paidForDomainAt":1490733089988,"type":"Collection"},"homeCollectionId":"624d886c4aa4","title":"Efficient and safely scalable","detectedLanguage":"en","latestVersion":"77580728ea52","latestPublishedVersion":"77580728ea52","hasUnpublishedEdits":false,"latestRev":1957,"createdAt":1458430565192,"updatedAt":1521070175653,"acceptedAt":0,"firstPublishedAt":1458784020844,"latestPublishedAt":1458784020844,"vote":false,"experimentalCss":"","displayAuthor":"","content":{"subtitle":"A precise but overambitious goal for AI control research.","bodyModel":{"paragraphs":[{"name":"531d","type":3,"text":"Efficient and safely scalable","markups":[]},{"name":"30e4","type":1,"text":"Precisely defining the goal of AI control research seems quite difficult. This post gives preliminary definitions of safe scalability and efficiency for AI control protocols, taking a step towards formalization. Roughly, these properties say that “using better machine learning primitives results in better systems” and “the control scheme does not impose significant overhead.”","markups":[{"type":1,"start":117,"end":133},{"type":1,"start":138,"end":148}]},{"name":"b32d","type":1,"text":"I think these properties are probably sufficient conditions for success, but they are also probably too ambitious to be realistic goals. I discuss a few possible ways to weaken these definitions.","markups":[]},{"name":"12bf","type":1,"text":"Both scalability and efficiency are defined with respect to a preference order ≻ᴰ which tells us when one algorithm is “better” another on some distribution D, according to the user’s preferences. I won’t offer any precise definition of ≻ᴰ, but I’ll discuss a few informal candidates.","markups":[]},{"name":"2794","type":13,"text":"Motivation","markups":[]},{"name":"caf0","type":1,"text":"I’m interested in defining alignment formally for at least three reasons:","markups":[]},{"name":"89ca","type":9,"text":"Having a precise goal makes it easier to do good and well-targeted research. The AI control problem would feel much easier to me (both to work on and to talk to others about) if there were a precise, satisfactory, and achievable goal.","markups":[]},{"name":"0b19","type":9,"text":"A precise definition of alignment might be helpful when analyzing AI control schemes. For example, the analysis of ALBA calls for maintaining alignment as an inductive invariant as the agent becomes more powerful. Right now, there is little hope of making that argument formal.","markups":[{"type":3,"start":99,"end":119,"href":"https://medium.com/ai-control/alba-an-explicit-proposal-for-aligned-ai-17a55f60bbcf","title":"","rel":"","anchorType":0}]},{"name":"91e1","type":9,"text":"Trying to formalize alignment may shed light on what the key difficulties are, what assumptions are likely to be necessary, and so on. Trying to pin down slippery concepts is often a good idea .","markups":[]},{"name":"14b9","type":3,"text":"Definitions","markups":[]},{"name":"fca7","type":13,"text":"What is a control protocol?","markups":[]},{"name":"291d","type":1,"text":"Our AI control protocols will use machine learning primitives as building blocks, and construct a (hopefully aligned) AI out of them.","markups":[]},{"name":"3894","type":1,"text":"To instantiate a control protocol Aʟɪɢɴ, we provide some set of learning primitives that are required by the protocol. Aʟɪɢɴ then instantiates any number of copies of each of those primitives. Aʟɪɢɴ may choose what inputs to provide to those instances, and may use their outputs however it likes. Aʟɪɢɴ may also interact with the user arbitrarily.","markups":[]},{"name":"8c2d","type":1,"text":"For simplicity, throughout the post we will assume that Aʟɪɢɴ is built from an RL algorithm, and write Aʟɪɢɴ(Aᴿᴸ) for the algorithm obtained by using Aᴿᴸ. Note that Aʟɪɢɴ can instantiate any number of distinct instances of Aᴿᴸ, can provide each of them distinct rewards, and so on.","markups":[]},{"name":"61d5","type":1,"text":"All of our definitions can be easily extended to any set of machine learning primitives, as long as we can define what it means for one implementation of a primitive to “outperform” another on a given distribution. I think that the definitions are most interesting when we can efficiently test whether one implementation outperforms another, and amongst such primitives RL is essentially universal (since we can use the test itself as a reward function).","markups":[]},{"name":"4318","type":13,"text":"Betterness","markups":[]},{"name":"a066","type":1,"text":"What does it mean for one algorithm to be better than another?","markups":[]},{"name":"20f0","type":1,"text":"We won’t answer that question. Instead, we take as given a family of preorders ≺ᴰ indexed by distributions D. These orders define when one program “outperforms” another on the distribution D, according to the user’s preferences.","markups":[]},{"name":"297f","type":1,"text":"Intuitively, we can imagine some (unobserved) utility function U characterizing the user’s preferences. U takes as input an (x, y) pair, and outputs a real number reflecting how good it is, according to the user’s preferences, for a program to output y given input x.","markups":[{"type":2,"start":125,"end":126},{"type":2,"start":128,"end":129},{"type":2,"start":251,"end":252},{"type":2,"start":265,"end":266}]},{"name":"8f6d","type":1,"text":"Then we could define:","markups":[]},{"name":"ae34","type":9,"text":"A ≼ᴰ B ⟺ 𝔼[U(x, A(x))] ≤ 𝔼[U(x, B(x))],","markups":[{"type":2,"start":14,"end":17},{"type":2,"start":19,"end":20},{"type":2,"start":31,"end":32},{"type":2,"start":33,"end":34},{"type":2,"start":36,"end":37}]},{"name":"bb87","type":1,"text":"where the expectations are taken over x drawn from D. We could also strengthen the condition by allowing A to score nearly as well on average over many episodes, or by requiring B to do at least as well as A for every utility function in some big set 𝒰. These changes make ≺ᴰ a preorder rather than a total order.","markups":[{"type":2,"start":38,"end":39}]},{"name":"c27a","type":1,"text":"We’ll give a range of more plausible definitions in the final section, but this is a good concrete example to have in mind.","markups":[]},{"name":"5254","type":13,"text":"Safely scalable and efficient","markups":[]},{"name":"9d36","type":1,"text":"Intuitively, a control scheme is safely scalable if using better RL algorithms never results in worse performance. A control scheme is efficient if it can realize any level of performance with low overhead.","markups":[{"type":2,"start":33,"end":49},{"type":2,"start":135,"end":144}]},{"name":"d3f2","type":9,"text":"Safely scalable (with respect to ≻ᴰ). Given any Aᴿᴸ, Bᴿᴸ, and the ability to sample from some distribution D with Aʟɪɢɴ(Aᴿᴸ) ≻ᴰ Aʟɪɢɴ(Bᴿᴸ), we can sample from a distribution Dᴿᴸ over RL episodes on which Aᴿᴸ outperforms Bᴿᴸ.","markups":[{"type":1,"start":0,"end":33}]},{"name":"dc13","type":9,"text":"Efficient (with respect to ≻ᴰ). Given any B, we can implement an RL agent Bᴿᴸ such that, for every distribution D from which we can sample, Aʟɪɢɴ(Bᴿᴸ) ⪰ᴰ B.","markups":[{"type":1,"start":0,"end":27}]},{"name":"1877","type":1,"text":"Efficiency is quantified by how much more time Aʟɪɢɴ(Bᴿᴸ) takes than B itself, and how often it queries the user. I’ll say that a scheme is “asymptotically efficient” if the difference in running times, and the number of queries to the user, is sublinear in the number of episodes T.","markups":[{"type":1,"start":141,"end":165}]},{"name":"3796","type":1,"text":"In order to be achievable this definition probably needs to be weakened. I think the most plausible way to weaken it is to make additional assumptions about the agent B in efficiency. For example, we could focus our attention on a particular approach to building AI systems, and assume that B is the kind of agent that might be produced by that approach. Particularly interesting are structural assumptions about how B itself is built out of the same building blocks that are available to Aʟɪɢɴ.","markups":[{"type":1,"start":172,"end":182}]},{"name":"770f","type":13,"text":"Hard to beat","markups":[]},{"name":"2539","type":1,"text":"Together efficiency and safe scalability imply a third property:","markups":[]},{"name":"1037","type":9,"text":"Hard to beat. Given any RL agent Aᴿᴸ, any agent B, and the ability to sample from a distribution D with B ≻ᴰ Aʟɪɢɴ(Aᴿᴸ), we can implement an agent Bᴿᴸ and sample from a distribution Dᴿᴸ over RL episodes on which Bᴿᴸ outperforms Aᴿᴸ.","markups":[{"type":1,"start":0,"end":12}]},{"name":"085f","type":1,"text":"If an algorithm is “hard to beat,” then the only way to make it better (according to ≻ᴰ) is to improve the underlying RL algorithms. In some sense this is the strongest form of optimality that we can realistically hope for, since improving our RL algorithms will allow us to build “better” AI systems for any reasonable notion of “better.”","markups":[]},{"name":"a29e","type":1,"text":"To see that (efficient + scalable → hard to beat), apply efficiency to find an agent Aʟɪɢɴ(Bᴿᴸ) ⪰ᴰ B, use transitivity to infer that Aʟɪɢɴ(Bᴿᴸ) ≻ᴰ Aʟɪɢɴ(Aᴿᴸ), and then to use safe scalability to sample from a distribution where Bᴿᴸ outperforms Aᴿᴸ.","markups":[]},{"name":"7248","type":1,"text":"Being hard to beat is slightly weaker than being efficient + scalable while being almost as comforting. So it might also be useful as an easier goal.","markups":[]},{"name":"b895","type":13,"text":"Restrictions on the building blocks","markups":[]},{"name":"fa54","type":1,"text":"Rather than working with a generic RL algorithm, we might want to work with an RL algorithm that satisfies some additional property. For example, Aʟɪɢɴ(Aᴿᴸ) might only be safely scalable if Aᴿᴸ is able to make good enough predictions about humans, or if Aᴿᴸ is “transparent” in an appropriate sense.","markups":[]},{"name":"4687","type":1,"text":"These restrictions can be incorporated into the definition of safe scalability and efficiency — in safe scalability, we can strengthen the hypothesis by assuming that the learning algorithms satisfy the restrictions, and in efficiency we can strengthen the conclusion by assuming that the learning algorithms satisfy the restrictions.","markups":[]},{"name":"e051","type":1,"text":"To make the task easiest, we could strengthen the hypothesis of safe scalability without strengthening the conclusion of efficiency. For example, if we want to work with “transparent” RL algorithms, we will probably not be able to strengthen the conclusion of efficiency — we won’t be able to turn a black-box algorithm B into a transparent RL algorithm Bᴿᴸ. So we could instead aim for a scheme that is safely scalable when applied with transparent RL algorithms, and that is efficient when we are allowed to use arbitrary RL algorithms. The resulting control scheme would only be practically efficient to the extent that there are transparent nearly-state-of-the-art RL algorithms.","markups":[{"type":2,"start":81,"end":88}]},{"name":"b02c","type":1,"text":"For now I am interested in schemes that work under any remotely plausible assumptions:","markups":[{"type":2,"start":51,"end":54}]},{"name":"500c","type":9,"text":"Even a conditional result would be a big advance beyond our current understanding.","markups":[]},{"name":"6125","type":9,"text":"Conditional results could clarify our understanding of when AI control will and won’t succeed.","markups":[]},{"name":"2c45","type":9,"text":"Conditional results present natural targets for differential AI progress. For example, if we could formulate a transparency condition that was sufficient for building safe+efficient AI, this could help clarify the goals of research on transparency.","markups":[]},{"name":"889b","type":13,"text":"Related building blocks","markups":[]},{"name":"3471","type":1,"text":"We might also work with a number of different building blocks, and require them to have some relationship to each other. For example, we might want to use both a classifier and an RL agent, and assume that the classifier is in some sense more powerful than the RL agent (though it’s not yet clear to me what assumption of this form would actually be useful). Or we might work with a sequence of RL agents, and assume that each is only slightly more powerful than the last.","markups":[{"type":3,"start":283,"end":356,"href":"https://medium.com/ai-control/the-informed-oversight-problem-1b51b4f66b35#.dl1zzh3jm","title":"","rel":"","anchorType":0},{"type":3,"start":410,"end":471,"href":"https://medium.com/ai-control/alba-an-explicit-proposal-for-aligned-ai-17a55f60bbcf","title":"","rel":"","anchorType":0}]},{"name":"60f1","type":3,"text":"Non-solutions","markups":[]},{"name":"570a","type":13,"text":"Plain RL (+counterfactual oversight)","markups":[]},{"name":"1f5e","type":1,"text":"The simplest behavior for Aʟɪɢɴ is to simply consult the underlying RL agent directly, and then to define a reward that induces aligned behavior.","markups":[]},{"name":"7d30","type":1,"text":"Perhaps the simplest reward function is a human’s evaluation: a human watches the agent and presses the reward button when the agent behaves well.","markups":[]},{"name":"f7c6","type":1,"text":"If the human has to monitor the agent during every episode, then this constitutes a huge efficiency hit and so our algorithm won’t be reasonably efficient.","markups":[]},{"name":"655e","type":1,"text":"More realistically, we could train our agent on a small subset of the data (for which the human does have to monitor the agent and press the reward button) and then use the learned policy to act even when there is no human monitor. If necessary, we could train online as in counterfactual oversight.","markups":[{"type":3,"start":274,"end":298,"href":"https://medium.com/ai-control/counterfactual-human-in-the-loop-a7822e36f399","title":"","rel":"","anchorType":0}]},{"name":"e293","type":1,"text":"This approach is asymptotically efficient (as long as the size of the training set is sublinear in the total number of episodes, as would be typical).","markups":[]},{"name":"7494","type":1,"text":"However, this scheme is not safely scalable for a reasonable order ≻ᴰ.","markups":[{"type":1,"start":24,"end":27}]},{"name":"da66","type":1,"text":"The problem is that the user’s judgments are not exactly aligned with the user’s preferences. For any given input x, there are some outputs y for which the user assigns y an “erroneously” high expected value, whether due to error, to bias, or simply to not having enough time to understand the consequences of a proposed action.","markups":[{"type":2,"start":114,"end":115},{"type":2,"start":140,"end":141},{"type":2,"start":169,"end":170}]},{"name":"cfe4","type":1,"text":"So let Aᴿᴸ be an RL agent which outputs actions which the user mistakenly rates highly, and let Bᴿᴸ be an RL agent which outputs actions that are better than Aᴿᴸ’s outputs but which are rated lower by the user. As long as the user’s judgments don’t perfectly preserve the ordering of outcomes, then we can construct such a pair. Then we can have Aʟɪɢɴ(Aᴿᴸ) ≻ᴰ Aʟɪɢɴ(Bᴿᴸ) for most distributions D, while Bᴿᴸ outperforms Aᴿᴸ on every distribution Dᴿᴸ","markups":[{"type":2,"start":426,"end":431}]},{"name":"8379","type":13,"text":"Plain imitation learning (+counterfactual oversight)","markups":[]},{"name":"eebc","type":1,"text":"Given observations of a human expert, we can give Aᴿᴸ the problem “predict what output the human expert will produce on input x.”","markups":[{"type":2,"start":126,"end":127}]},{"name":"645e","type":1,"text":"After Aᴿᴸ produces a prediction, with small probability we ask the human to actually produce an output y, and then we provide Aᴿᴸ a payoff depending on the quality of its prediction. (There are a number of possible ways to score Aᴿᴸ, most involving two separate learners. For example, we could use a generative adversarial model, with a second instance of Aᴿᴸ trying to distinguish the predicted output from the real human output. Or we could score Aᴿᴸ based on a variational lower bound on the log probability it assigned to the particular prediction made by the user.)","markups":[{"type":2,"start":103,"end":104}]},{"name":"cdc5","type":1,"text":"This kind of imitation learning is not safely scalable: an agent which chooses superhuman actions will become strictly worse if we make them a perfect human-predictor, yet their performance in the RL game will improve.","markups":[]},{"name":"0d86","type":1,"text":"(Intuitively, imitation is safely scalable but is not efficient — this example highlights a way in which our definitions fail to map well to the intuitive definitions of these concepts.)","markups":[{"type":2,"start":24,"end":26},{"type":2,"start":47,"end":54}]},{"name":"7557","type":13,"text":"ALBA","markups":[]},{"name":"3dbf","type":1,"text":"The current version of ALBA is not safely scalable + efficient, even under the generous assumptions made in the post. The key problem is its reliance on a sequence of throttled RL subagents.","markups":[{"type":3,"start":4,"end":27,"href":"https://medium.com/ai-control/alba-an-explicit-proposal-for-aligned-ai-17a55f60bbcf","title":"","rel":"","anchorType":0}]},{"name":"8b3d","type":1,"text":"The missing ingredients, roughly speaking, are the following:","markups":[]},{"name":"5d17","type":9,"text":"Speedup. Given black-box access to the agent B, we can produce a strictly less capable agent B′ such that Bootstrap(B′) ⪰ᴰ B.","markups":[{"type":1,"start":0,"end":7},{"type":2,"start":65,"end":86}]},{"name":"3f28","type":9,"text":"Informed oversight. See here.","markups":[{"type":3,"start":20,"end":28,"href":"https://medium.com/ai-control/the-informed-oversight-problem-1b51b4f66b35","title":"","rel":"","anchorType":0},{"type":1,"start":0,"end":18}]},{"name":"8445","type":1,"text":"Speedup is impossible without some additional assumptions on B, and informed oversight probably also requires some additional assumptions (in order to make transparency possible). It’s not clear what kind of assumptions on B might be sufficient to carry out the analysis, and overall it looks like a daunting project.","markups":[]},{"name":"2641","type":3,"text":"Limitations","markups":[]},{"name":"3ecb","type":13,"text":"Too strong","markups":[]},{"name":"5bc3","type":1,"text":"I think the biggest problem with this definition is that it is too strong.","markups":[]},{"name":"a248","type":1,"text":"It’s not so strong as to be obviously impossible. But it looks almost obviously impossible. The discussions of RL and ALBA illustrate why the definition is so strong:","markups":[{"type":2,"start":28,"end":37},{"type":2,"start":63,"end":70}]},{"name":"69cf","type":9,"text":"In order to turn a black box agent B into an agent Bᴿᴸ with Aʟɪɢɴ(Bᴿᴸ) ⪰ᴰ B, we are essentially forced to take Bᴿᴸ = B (since we can’t produce other derivative agents using black-box access to B).","markups":[]},{"name":"0da5","type":9,"text":"Then Aʟɪɢɴ is essentially forced to be a training scheme for RL agents.","markups":[]},{"name":"b356","type":9,"text":"So in order to be safely scalable, Aʟɪɢɴ needs to evaluate of the quality of the agent’s decisions “well enough” that optimizing its evaluations optimizes ≻ᴰ.","markups":[]},{"name":"4d4a","type":9,"text":"Moreover, Aʟɪɢɴ can’t really use the RL agent’s help to make those decisions — if Aʟɪɢɴ is merely a training procedure, the RL agent need not output anything except on the support of D, and so we can’t get any useful work out of the agent. Thus Aʟɪɢɴ is using the same evaluations for every agent.","markups":[]},{"name":"3329","type":9,"text":"If Aʟɪɢɴ evaluates the agent’s behavior “well enough” for an arbitrary agent, then Aʟɪɢɴ must be evaluating the agent’s behavior perfectly.","markups":[]},{"name":"afbe","type":9,"text":"It seems infeasible to produce such a perfect evaluations for any interesting ≻ᴰ.","markups":[]},{"name":"c4cb","type":1,"text":"How might we weaken the definition?","markups":[]},{"name":"cf97","type":9,"text":"Place some restriction on the set of agents B that we consider in efficiency. For example, we may restrict attention to the kinds of agents that could be produced by some particular AI research project in AI. I think that this is by far the most promising approach.","markups":[{"type":1,"start":66,"end":76}]},{"name":"49cb","type":9,"text":"As discussed in the section Restrictions on building blocks, we could only require safe scalability for a certain class of RL agents, thus moving some of the work to ensuring that state-of-the-art RL agents have the required properties.","markups":[{"type":1,"start":28,"end":59}]},{"name":"e3c1","type":9,"text":"We could use relations ≻ᴰ that evaluate agents holistically in terms of a description of the distribution D (see below). For example, we might say that “A ≻ᴰ B if the human believes that A would outperform B on the distribution D.” I don’t really see a way to make this work, but it might be worth thinking about.","markups":[{"type":2,"start":74,"end":85}]},{"name":"2af0","type":9,"text":"We could settle for an agent which is hard to beat instead of both efficient and safely scalable. I don’t think this really addresses the difficulty described above, but it does give us a tiny bit more traction.","markups":[]},{"name":"5702","type":9,"text":"We could swap the quantifier order, giving us access to B and D when trying to construct an agent Bᴿᴸ with Aʟɪɢɴ(Bᴿᴸ) ⪰ᴰ B. I don’t think this will help.","markups":[]},{"name":"c65a","type":1,"text":"I expect there are many other ways to weaken the definition, and of course we could pursue some combination of the above.","markups":[]},{"name":"5ef5","type":13,"text":"Improving RL algorithms is quite broad","markups":[]},{"name":"00c5","type":1,"text":"Even if Aʟɪɢɴ is efficient and safely scalable, Aʟɪɢɴ(Aᴿᴸ) isn’t necessarily good even according to ≻ᴰ. In order to make Aʟɪɢɴ(Aᴿᴸ) actually be good, we may need to improve Aᴿᴸ. In some sense this is obvious and inevitable — it’s like saying that even if we solve the control problem, AI progress will still make our AI systems work better.","markups":[{"type":2,"start":77,"end":81}]},{"name":"6e07","type":1,"text":"But in particular, the alignment of Aʟɪɢɴ(Aᴿᴸ) may depend on how Aᴿᴸ performs on some very unnatural distribution over RL problems (e.g. on how well Aᴿᴸ is able to predict the results of human deliberation about moral questions).","markups":[]},{"name":"f21b","type":1,"text":"Given how strong safe scalability and efficiency are, I don’t think this is a problem for this particular definition. That is, any such “unnatural” distribution over RL problems would be necessary to achieving good behavior, even for very weak agents:","markups":[]},{"name":"a0c4","type":9,"text":"Given any aligned agent B we can apply efficiency to obtain an RL agent Bᴿᴸ.","markups":[]},{"name":"5f54","type":9,"text":"If Bᴿᴸ always does well on these unnatural instances, then they were in some strong sense necessary in order to get good behavior. But note that B might be quite weak, so these “unnatural” instances were necessary even to getting good behavior out of weak AI systems.","markups":[{"type":2,"start":90,"end":99}]},{"name":"69f4","type":9,"text":"If Bᴿᴸ does poorly on these unnatural instances, then we can consider a very powerful agent Aᴿᴸ that does equally poorly.","markups":[]},{"name":"22b4","type":9,"text":"By safe scalability, Aʟɪɢɴ(Aᴿᴸ) remains at-least-as-good-as-B for that RL agent Aᴿᴸ.","markups":[]},{"name":"0697","type":1,"text":"This argument would still go through if we restricted B to the kinds of AI’s that we might actually produce. (The conclusion would be that the unnatural RL instances are in fact necessary, for the kinds of AI that we might actually build, which is just as good.)","markups":[{"type":2,"start":189,"end":237}]},{"name":"2a5b","type":1,"text":"If we weakened the definition enough we might encounter a more troubling version of this unnatural-RL-instances problem, but even then I think that reducing the alignment problem to a concrete RL problem would probably represent significant progress in and of itself. So overall I’m not concerned about this potential bug.","markups":[]},{"name":"23f4","type":13,"text":"Missing definition of ≻ᴰ","markups":[]},{"name":"9f3d","type":1,"text":"We don’t provide any definition of ≻ᴰ; that definition needs to be quite subtle and is doing a lot of the work of formalizing our goals. See the discussion in the next section.","markups":[]},{"name":"c8ed","type":3,"text":"Defining ≻ᴰ","markups":[]},{"name":"df5a","type":13,"text":"Human judgment","markups":[]},{"name":"e9c4","type":1,"text":"For sufficiently subhuman agents we could define:","markups":[]},{"name":"be0f","type":9,"text":"Draw a random x from D, compute A(x) and B(x), and give them both to a human judge (along with the input x) to decide which is better. A ≻ᴰ B if the human prefers A(x) with probability at least 2/3.","markups":[{"type":2,"start":14,"end":15},{"type":2,"start":34,"end":35},{"type":2,"start":43,"end":44},{"type":2,"start":165,"end":166}]},{"name":"8b92","type":1,"text":"For this definition of ≺, the RL agent described above is in fact efficient and safely scalable.","markups":[]},{"name":"f2e9","type":1,"text":"But being safely scalable in this sense doesn’t seem very useful, since the human’s judgment about “which action is good” need not be correct, and the human would substantially prefer an agent that chooses actions which are actually good.","markups":[{"type":2,"start":224,"end":232}]},{"name":"f3c7","type":13,"text":"Human judgment on reflection","markups":[]},{"name":"cdc7","type":1,"text":"We could try to fix this definition with an alternative informal definition:","markups":[]},{"name":"5dac","type":9,"text":"Draw a random x from D, compute A(x) and B(x). Give a human judge an extremely long time to reflect, and access to extremely powerful tools. A ≻ᴰ B if, upon extensive reflection, the human prefers A(x) with probability at least 2/3.","markups":[{"type":3,"start":66,"end":139,"href":"https://ordinaryideas.wordpress.com/2012/04/21/indirect-normativity-write-up/","title":"","rel":"","anchorType":0},{"type":2,"start":14,"end":15},{"type":2,"start":34,"end":35},{"type":2,"start":43,"end":44},{"type":2,"start":199,"end":200}]},{"name":"6ddd","type":1,"text":"This definition leads to a very strong notion of safe scalability. Unfortunately that strong notion is too strong, and it is clearly intractable to build a hard-to-beat agent in this sense.","markups":[{"type":2,"start":103,"end":106}]},{"name":"4794","type":1,"text":"For example, consider some hard question that bears on what strategies will have good long-term effects. This question is beyond our abilities currently, but we will be able to answer it with sufficient reflection. If B guesses the correct answer to that question and A does not, then A ≺ᴰ B, where D is the a point distribution concentrated on a decision where that question matters. If B guesses the wrong answer, then B ≺ᴰ A. But by efficiency + safe scalability, we can essentially produce a pair of agents Aᴿᴸ and Bᴿᴸ and sample from a distribution Dᴿᴸ such that A ≺ᴰ B iff Bᴿᴸ outperforms Aᴿᴸ on D ᴿᴸ. But then we can test which of A and B is correct about the hard question, contradicting its supposed hardness.","markups":[]},{"name":"96d8","type":13,"text":"Holistic judgment","markups":[]},{"name":"1854","type":1,"text":"A very different approach would be to directly elicit the human’s judgment about the relative merits of A and B. That is:","markups":[]},{"name":"e159","type":9,"text":"Give a human judge a description of A and B, and the sampling procedure for D, and ask them which they expect to perform better. A ≻ᴰ B if the human thinks that A would make significantly better decisions than B, on inputs sampled from D.","markups":[]},{"name":"48e2","type":1,"text":"In some sense this definition gets very directly at what we care about. Suppose that some alternative AI control research program produced B, and that we would be happier using B than A. Then we are precisely satisfying this definition of A ≺ᴰ B.","markups":[]},{"name":"e008","type":1,"text":"An obvious problem with this approach is that determining which of A and B is better may itself be a very hard problem — indeed, it’s easy to believe that it includes all of the difficulty of the alignment problem.","markups":[]},{"name":"628c","type":1,"text":"The definition of efficiency still has teeth if we restrict our attention to algorithms B that “explain themselves,” e.g. that are accompanied with significant documentation and explanation. The description of the agent B could embody the entire output of the alternative research program that produced it.","markups":[{"type":1,"start":28,"end":29}]},{"name":"edbf","type":1,"text":"So this definition of betterness is adequate if we think that there are efficient and good decision-making protocols which are demonstrably good to existing humans. This definition is very unsatisfying if we think that evaluating a possible proposal, even given the best available arguments, is the core difficulty of AI control. This might be either because those arguments are necessarily extremely complex, or because there will be many bad proposals that are also supported by extremely convincing-looking arguments.","markups":[{"type":2,"start":127,"end":139}]},{"name":"08c8","type":13,"text":"What we really want","markups":[]},{"name":"5df8","type":1,"text":"Intuitively, I would like a definition along the lines of:","markups":[]},{"name":"0415","type":9,"text":"Draw a random x from D, compute A(x) and B(x). Give a human judge the same information, abilities, and insights, that A and B used to compute these quantities. A ≻ᴰ B if the human prefers A(x) with probability at least 2/3.","markups":[{"type":2,"start":14,"end":15},{"type":2,"start":34,"end":35},{"type":2,"start":43,"end":44},{"type":2,"start":190,"end":191}]},{"name":"aebe","type":1,"text":"This definition smuggles all of the complexity into imagining that the human has the same “information, abilities, and insights” as the AI they are evaluating. I don’t have any candidate formalization of this idea, nor am I especially optimistic about being able to formalize it.","markups":[]},{"name":"4b26","type":1,"text":"I do feel like I can reason about this definition intuitively and that it roughly captures my intuitive desiderata. This makes me more optimistic that there is some satisfactory definition of ≻.","markups":[{"type":2,"start":160,"end":164}]},{"name":"5d9a","type":1,"text":"Note that this definition is closely related to the goal in the informed oversight problem, which is roughly to ensure that the overseer “knows everything the AI knows.” In the informed oversight problem we are willing to assume that the overseer is significantly more powerful than the system they are overseeing. That may well be a necessary assumption to actually ensure that the overseer “knows everything the AI knows,” but it probably isn’t needed to define what it would mean for the overseer to “know everything the AI knows.”","markups":[{"type":3,"start":60,"end":90,"href":"https://medium.com/ai-control/the-informed-oversight-problem-1b51b4f66b35","title":"","rel":"","anchorType":0}]},{"name":"daa8","type":3,"text":"Conclusion","markups":[]},{"name":"075d","type":1,"text":"We can try to define the goals of AI control by thinking about how AI systems relate to the underlying machine learning primitives. Such a framework wouldn’t cover all possible approaches to AI control, but where applicable it could be a great way to organize research and a useful analysis tool.","markups":[]},{"name":"e937","type":1,"text":"This post gave a step in that direction, but did not yet succeed. I would love to see other attempts, and I think there is a good chance that it will be possible to find a satisfying problem statement for AI control.","markups":[]}],"sections":[{"name":"c867","startIndex":0}]},"postDisplay":{"coverless":true}},"virtuals":{"statusForCollection":"APPROVED","allowNotes":true,"previewImage":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"wordCount":3855,"imageCount":0,"readingTime":14.547169811320755,"subtitle":"A precise but overambitious goal for AI control research.","publishedInCount":1,"usersBySocialRecommends":[],"noIndex":false,"recommends":4,"socialRecommends":[],"isBookmarked":false,"tags":[],"socialRecommendsCount":0,"responsesCreatedCount":0,"links":{"entries":[{"url":"https://medium.com/ai-control/the-informed-oversight-problem-1b51b4f66b35","alts":[{"type":3,"url":"medium://p/1b51b4f66b35"},{"type":2,"url":"medium://p/1b51b4f66b35"}]},{"url":"https://medium.com/ai-control/the-informed-oversight-problem-1b51b4f66b35#.dl1zzh3jm","alts":[{"type":2,"url":"medium://p/1b51b4f66b35"},{"type":3,"url":"medium://p/1b51b4f66b35"}]},{"url":"https://ordinaryideas.wordpress.com/2012/04/21/indirect-normativity-write-up/","alts":[{"type":1,"url":"https://cdn.ampproject.org/c/s/ordinaryideas.wordpress.com/2012/04/21/indirect-normativity-write-up/amp/"}]},{"url":"https://medium.com/ai-control/counterfactual-human-in-the-loop-a7822e36f399","alts":[{"type":2,"url":"medium://p/a7822e36f399"},{"type":3,"url":"medium://p/a7822e36f399"}]},{"url":"https://medium.com/ai-control/alba-an-explicit-proposal-for-aligned-ai-17a55f60bbcf","alts":[{"type":2,"url":"medium://ai-control/alba-an-explicit-proposal-for-aligned-ai-17a55f60bbcf"},{"type":3,"url":"medium://ai-control/alba-an-explicit-proposal-for-aligned-ai-17a55f60bbcf"}]}],"version":"0.3","generatedAt":1472039437103},"isLockedPreviewOnly":false,"metaDescription":"","totalClapCount":4,"sectionCount":1,"readingList":0,"topics":[]},"coverless":true,"slug":"efficient-and-safely-scalable","translationSourcePostId":"","translationSourceCreatorId":"","isApprovedTranslation":false,"inResponseToPostId":"","inResponseToRemovedAt":0,"isTitleSynthesized":false,"allowResponses":true,"importedUrl":"","importedPublishedAt":0,"visibility":0,"uniqueSlug":"efficient-and-safely-scalable-8218fa8a871f","previewContent":{"bodyModel":{"paragraphs":[{"name":"previewTitle","type":3,"text":"Efficient and safely scalable","alignment":1},{"name":"previewSubtitle","type":13,"text":"A precise but overambitious goal for AI control research.","alignment":1},{"name":"previewSnippet0","type":1,"text":"Precisely defining the goal of AI control research seems quite difficult. This post gives preliminary definitions of safe scalability and efficiency for AI…","alignment":1}],"sections":[{"startIndex":0}]},"isFullContent":false,"subtitle":"A precise but overambitious goal for AI control research."},"license":0,"inResponseToMediaResourceId":"","canonicalUrl":"https://ai-alignment.com/efficient-and-safely-scalable-8218fa8a871f","approvedHomeCollectionId":"624d886c4aa4","approvedHomeCollection":{"id":"624d886c4aa4","name":"AI Alignment","slug":"ai-control","tags":[],"creatorId":"57f1a655a613","description":"Aligning AI systems with human interests.","shortDescription":"Aligning AI systems with human interests.","image":{"imageId":"1*N56Qc5-aHTcfGff0scntKQ.png","filter":"","backgroundSize":"","originalWidth":512,"originalHeight":512,"strategy":"resample","height":0,"width":0},"metadata":{"followerCount":2834,"activeAt":1548040822588},"virtuals":{"permissions":{"canPublish":false,"canPublishAll":false,"canRepublish":false,"canRemove":false,"canManageAll":false,"canSubmit":false,"canEditPosts":false,"canAddWriters":false,"canViewStats":false,"canSendNewsletter":false,"canViewLockedPosts":false,"canViewCloaked":false,"canEditOwnPosts":false,"canBeAssignedAuthor":false,"canEnrollInHightower":false,"canLockPostsForMediumMembers":false,"canLockOwnPostsForMediumMembers":false},"isSubscribed":false,"isNewsletterSubscribed":false,"isEnrolledInHightower":false,"isEligibleForHightower":false},"logo":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"collectionMastheadId":"29f3dcc2e4","domain":"ai-alignment.com","sections":[{"type":2,"collectionHeaderMetadata":{"title":"AI Alignment","backgroundImage":{},"logoImage":{},"alignment":2,"layout":4}},{"type":1,"postListMetadata":{"source":3,"layout":4,"number":3,"postIds":["157debfd1616","b49ad992940b","b959644d79c2"]}},{"type":1,"postListMetadata":{"source":1,"layout":4,"number":24,"postIds":[],"sectionHeader":"Latest"}}],"favicon":{"imageId":"1*cciPf4CUXd_Zyux0Jg0yBQ.png","filter":"","backgroundSize":"","originalWidth":400,"originalHeight":400,"strategy":"resample","height":0,"width":0},"colorPalette":{"defaultBackgroundSpectrum":{"colorPoints":[{"color":"#FF02B875","point":0},{"color":"#FF00AB6B","point":0.1},{"color":"#FF1C9963","point":0.2},{"color":"#FF092E20","point":1}],"backgroundColor":"#FFFFFFFF"},"highlightSpectrum":{"colorPoints":[{"color":"#FFFFFFFF","point":0},{"color":"#FFE9FDF0","point":0.1},{"color":"#FFE2FAEE","point":0.2},{"color":"#FFADFFCF","point":0.6},{"color":"#FF7DFFB3","point":1}],"backgroundColor":"#FFFFFFFF"}},"navItems":[],"colorBehavior":1,"instantArticlesState":0,"acceleratedMobilePagesState":0,"ampLogo":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"header":{"title":"AI Alignment","backgroundImage":{},"logoImage":{},"alignment":2,"layout":4},"paidForDomainAt":1490733089988,"type":"Collection"},"newsletterId":"","webCanonicalUrl":"https://ai-alignment.com/efficient-and-safely-scalable-8218fa8a871f","mediumUrl":"https://ai-alignment.com/efficient-and-safely-scalable-8218fa8a871f","migrationId":"","notifyFollowers":true,"notifyTwitter":false,"notifyFacebook":false,"responseHiddenOnParentPostAt":0,"isSeries":false,"isSubscriptionLocked":false,"seriesLastAppendedAt":0,"audioVersionDurationSec":0,"sequenceId":"","isNsfw":false,"isEligibleForRevenue":false,"isBlockedFromHightower":false,"deletedAt":0,"lockedPostSource":0,"hightowerMinimumGuaranteeStartsAt":0,"hightowerMinimumGuaranteeEndsAt":0,"featureLockRequestAcceptedAt":0,"mongerRequestType":1,"layerCake":0,"socialTitle":"","socialDek":"","editorialPreviewTitle":"","editorialPreviewDek":"","curationEligibleAt":0,"type":"Post"},"mentionedUsers":[],"collaborators":[],"hideMeter":false,"collectionUserRelations":[],"mode":null,"references":{"User":{"57f1a655a613":{"userId":"57f1a655a613","name":"Paul Christiano","username":"paulfchristiano","createdAt":1417286353352,"imageId":"1*BNjZCuQuRfIgcXCBMipuBw.jpeg","backgroundImageId":"","bio":"OpenAI","twitterScreenName":"","socialStats":{"userId":"57f1a655a613","usersFollowedCount":93,"usersFollowedByCount":821,"type":"SocialStats"},"social":{"userId":"lo_1HnsOvgcVrD4","targetUserId":"57f1a655a613","type":"Social"},"facebookAccountId":"1167284919","allowNotes":1,"mediumMemberAt":0,"isNsfw":false,"isWriterProgramEnrolled":true,"isQuarantined":false,"type":"User"}},"Collection":{"624d886c4aa4":{"id":"624d886c4aa4","name":"AI Alignment","slug":"ai-control","tags":[],"creatorId":"57f1a655a613","description":"Aligning AI systems with human interests.","shortDescription":"Aligning AI systems with human interests.","image":{"imageId":"1*N56Qc5-aHTcfGff0scntKQ.png","filter":"","backgroundSize":"","originalWidth":512,"originalHeight":512,"strategy":"resample","height":0,"width":0},"metadata":{"followerCount":2834,"activeAt":1548040822588},"virtuals":{"permissions":{"canPublish":false,"canPublishAll":false,"canRepublish":false,"canRemove":false,"canManageAll":false,"canSubmit":false,"canEditPosts":false,"canAddWriters":false,"canViewStats":false,"canSendNewsletter":false,"canViewLockedPosts":false,"canViewCloaked":false,"canEditOwnPosts":false,"canBeAssignedAuthor":false,"canEnrollInHightower":false,"canLockPostsForMediumMembers":false,"canLockOwnPostsForMediumMembers":false},"isSubscribed":false,"isNewsletterSubscribed":false,"isEnrolledInHightower":false,"isEligibleForHightower":false},"logo":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"collectionMastheadId":"29f3dcc2e4","domain":"ai-alignment.com","sections":[{"type":2,"collectionHeaderMetadata":{"title":"AI Alignment","backgroundImage":{},"logoImage":{},"alignment":2,"layout":4}},{"type":1,"postListMetadata":{"source":3,"layout":4,"number":3,"postIds":["157debfd1616","b49ad992940b","b959644d79c2"]}},{"type":1,"postListMetadata":{"source":1,"layout":4,"number":24,"postIds":[],"sectionHeader":"Latest"}}],"favicon":{"imageId":"1*cciPf4CUXd_Zyux0Jg0yBQ.png","filter":"","backgroundSize":"","originalWidth":400,"originalHeight":400,"strategy":"resample","height":0,"width":0},"colorPalette":{"defaultBackgroundSpectrum":{"colorPoints":[{"color":"#FF02B875","point":0},{"color":"#FF00AB6B","point":0.1},{"color":"#FF1C9963","point":0.2},{"color":"#FF092E20","point":1}],"backgroundColor":"#FFFFFFFF"},"highlightSpectrum":{"colorPoints":[{"color":"#FFFFFFFF","point":0},{"color":"#FFE9FDF0","point":0.1},{"color":"#FFE2FAEE","point":0.2},{"color":"#FFADFFCF","point":0.6},{"color":"#FF7DFFB3","point":1}],"backgroundColor":"#FFFFFFFF"}},"navItems":[],"colorBehavior":1,"instantArticlesState":0,"acceleratedMobilePagesState":0,"ampLogo":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"header":{"title":"AI Alignment","backgroundImage":{},"logoImage":{},"alignment":2,"layout":4},"paidForDomainAt":1490733089988,"type":"Collection"}},"Social":{"57f1a655a613":{"userId":"lo_1HnsOvgcVrD4","targetUserId":"57f1a655a613","type":"Social"}},"SocialStats":{"57f1a655a613":{"userId":"57f1a655a613","usersFollowedCount":93,"usersFollowedByCount":821,"type":"SocialStats"}}}})
// ]]></script><script id="parsely-cfg" src="//d1z2jf7jlzjs58.cloudfront.net/keys/medium.com/p.js"></script><script type="text/javascript">(function(b,r,a,n,c,h,_,s,d,k){if(!b[n]||!b[n]._q){for(;s<_.length;)c(h,_[s++]);d=r.createElement(a);d.async=1;d.src="https://cdn.branch.io/branch-latest.min.js";k=r.getElementsByTagName(a)[0];k.parentNode.insertBefore(d,k);b[n]=h}})(window,document,"script","branch",function(b,r){b[r]=function(){b._q.push([r,arguments])}},{_q:[],_v:1},"addListener applyCode autoAppIndex banner closeBanner closeJourney creditHistory credits data deepview deepviewCta first getCode init link logout redeem referrals removeListener sendSMS setBranchViewData setIdentity track validateCode trackCommerceEvent logEvent".split(" "), 0); branch.init('key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm', {'no_journeys': true, 'disable_exit_animation': true, 'disable_entry_animation': true, 'tracking_disabled':  false }, function(err, data) {});</script><div class="surface-scrollOverlay"></div><script charset="UTF-8" src="https://cdn-static-1.medium.com/_/fp/gen-js/main-common-async.bundle.qFZkgzLZ5TYXIerh_w9awQ.js"></script><script charset="UTF-8" src="https://cdn-static-1.medium.com/_/fp/gen-js/main-notes.bundle.V05mXLtyLz2Mj5DzEML26A.js"></script></body></html>