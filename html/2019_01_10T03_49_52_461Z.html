<!DOCTYPE html><html xmlns:cc="http://creativecommons.org/ns#"><head prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb# medium-com: http://ogp.me/ns/fb/medium-com#"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0, viewport-fit=contain"><title>Towards formalizing universality â€“ AI Alignment</title><link rel="canonical" href="https://ai-alignment.com/towards-formalizing-universality-409ab893a456"><meta name="title" content="Towards formalizing universality â€“ AI Alignment"><meta name="referrer" content="always"><meta name="description" content="An attempt to formalize universality as â€œable to understand anything that any computation can understand.â€"><meta name="theme-color" content="#000000"><meta property="og:title" content="Towards formalizing universality â€“ AI Alignment"><meta property="twitter:title" content="Towards formalizing universality"><meta property="og:url" content="https://ai-alignment.com/towards-formalizing-universality-409ab893a456"><meta property="fb:app_id" content="542599432471018"><meta property="og:description" content="An attempt to formalize universality as â€œable to understand anything that any computation can understand.â€"><meta name="twitter:description" content="An attempt to formalize universality as â€œable to understand anything that any computation can understand.â€"><link rel="author" href="https://ai-alignment.com/@paulfchristiano"><meta name="author" content="Paul Christiano"><meta property="og:type" content="article"><meta name="twitter:card" content="summary"><meta property="article:publisher" content="https://www.facebook.com/medium"><meta property="article:author" content="1167284919"><meta name="robots" content="index, follow"><meta property="article:published_time" content="2019-01-10T03:49:52.461Z"><meta name="twitter:site" content="@Medium"><meta property="og:site_name" content="AI Alignment"><meta name="twitter:label1" value="Reading time"><meta name="twitter:data1" value="22 min read"><meta name="twitter:app:name:iphone" content="Medium"><meta name="twitter:app:id:iphone" content="828256236"><meta name="twitter:app:url:iphone" content="medium://p/409ab893a456"><meta property="al:ios:app_name" content="Medium"><meta property="al:ios:app_store_id" content="828256236"><meta property="al:android:package" content="com.medium.reader"><meta property="al:android:app_name" content="Medium"><meta property="al:ios:url" content="medium://p/409ab893a456"><meta property="al:android:url" content="medium://p/409ab893a456"><meta property="al:web:url" content="https://ai-alignment.com/towards-formalizing-universality-409ab893a456"><link rel="search" type="application/opensearchdescription+xml" title="Medium" href="/osd.xml"><link rel="alternate" href="android-app://com.medium.reader/https/medium.com/p/409ab893a456"><script async="" src="https://cdn.branch.io/branch-latest.min.js"></script><script type="application/ld+json">{"@context":"http://schema.org","@type":"NewsArticle","image":{"@type":"ImageObject","width":545,"height":106,"url":"https://cdn-images-1.medium.com/max/545/1*OMF3fSqH8t4xBJ9-6oZDZw.png"},"url":"https://ai-alignment.com/towards-formalizing-universality-409ab893a456","dateCreated":"2019-01-10T03:49:52.461Z","datePublished":"2019-01-10T03:49:52.461Z","dateModified":"2019-01-11T00:39:52.948Z","headline":"Towards formalizing universality","name":"Towards formalizing universality","articleId":"409ab893a456","thumbnailUrl":"https://cdn-images-1.medium.com/max/545/1*OMF3fSqH8t4xBJ9-6oZDZw.png","keywords":["Tag:Machine Learning","Publication:ai-control","LockedPostSource:0","Elevated:false","LayerCake:0"],"author":{"@type":"Person","name":"Paul Christiano","url":"https://ai-alignment.com/@paulfchristiano"},"creator":["Paul Christiano"],"publisher":{"@type":"Organization","name":"AI Alignment","url":"https://ai-alignment.com","logo":{"@type":"ImageObject","width":308,"height":60,"url":"https://cdn-images-1.medium.com/max/308/1*OMF3fSqH8t4xBJ9-6oZDZw.png"}},"mainEntityOfPage":"https://ai-alignment.com/towards-formalizing-universality-409ab893a456"}</script><meta name="parsely-link" content="https://ai-alignment.com/towards-formalizing-universality-409ab893a456"><link rel="stylesheet" href="https://cdn-static-1.medium.com/_/fp/css/main-branding-base.sMRbh_65n82B91860QdvTg.css"><script>!function(n,e){var t,o,i,c=[],f={passive:!0,capture:!0},r=new Date,a="pointerup",u="pointercancel";function p(n,c){t||(t=c,o=n,i=new Date,w(e),s())}function s(){o>=0&&o<i-r&&(c.forEach(function(n){n(o,t)}),c=[])}function l(t){if(t.cancelable){var o=(t.timeStamp>1e12?new Date:performance.now())-t.timeStamp;"pointerdown"==t.type?function(t,o){function i(){p(t,o),r()}function c(){r()}function r(){e(a,i,f),e(u,c,f)}n(a,i,f),n(u,c,f)}(o,t):p(o,t)}}function w(n){["click","mousedown","keydown","touchstart","pointerdown"].forEach(function(e){n(e,l,f)})}w(n),self.perfMetrics=self.perfMetrics||{},self.perfMetrics.onFirstInputDelay=function(n){c.push(n),s()}}(addEventListener,removeEventListener);</script><script>if (window.top !== window.self) window.top.location = window.self.location.href;var OB_startTime = new Date().getTime(); var OB_loadErrors = []; function _onerror(e) { OB_loadErrors.push(e) }; if (document.addEventListener) document.addEventListener("error", _onerror, true); else if (document.attachEvent) document.attachEvent("onerror", _onerror); function _asyncScript(u) {var d = document, f = d.getElementsByTagName("script")[0], s = d.createElement("script"); s.type = "text/javascript"; s.async = true; s.src = u; f.parentNode.insertBefore(s, f);}function _asyncStyles(u) {var d = document, f = d.getElementsByTagName("script")[0], s = d.createElement("link"); s.rel = "stylesheet"; s.href = u; f.parentNode.insertBefore(s, f); return s}(new Image()).src = "/_/stat?event=pixel.load&origin=" + encodeURIComponent(location.origin);</script><script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date; ga("create", "UA-24232453-2", "auto", {"allowLinker": true, "legacyCookieDomain": window.location.hostname}); ga("send", "pageview");</script><script async="" src="https://www.google-analytics.com/analytics.js"></script><script>(function () {var height = window.innerHeight || document.documentElement.clientHeight || document.body.clientHeight; var width = window.innerWidth || document.documentElement.clientWidth || document.body.clientWidth; document.write("<style>section.section-image--fullBleed.is-backgrounded {padding-top: " + Math.round(1.1 * height) + "px;}section.section-image--fullScreen.is-backgrounded, section.section-image--coverFade.is-backgrounded {min-height: " + height + "px; padding-top: " + Math.round(0.5 * height) + "px;}.u-height100vh {height: " + height + "px !important;}.u-height110vh {height: " + Math.round(1.1 * height) + "px !important;}.u-minHeight100vh {min-height: " + height + "px !important;}.u-maxHeight100vh {max-height: " + height + "px !important;}section.section-image--coverFade {height: " + height + "px;}.section-aspectRatioViewportPlaceholder, .section-aspectRatioViewportCropPlaceholder {max-height: " + height + "px;}.section-aspectRatioViewportBottomSpacer, .section-aspectRatioViewportBottomPlaceholder {max-height: " + Math.round(0.5 * height) + "px;}.zoomable:before {top: " + (-1 * height) + "px; left: " + (-1 * width) + "px; padding: " + height + "px " + width + "px;}</style>");})()</script><style>section.section-image--fullBleed.is-backgrounded {padding-top: 660px;}section.section-image--fullScreen.is-backgrounded, section.section-image--coverFade.is-backgrounded {min-height: 600px; padding-top: 300px;}.u-height100vh {height: 600px !important;}.u-height110vh {height: 660px !important;}.u-minHeight100vh {min-height: 600px !important;}.u-maxHeight100vh {max-height: 600px !important;}section.section-image--coverFade {height: 600px;}.section-aspectRatioViewportPlaceholder, .section-aspectRatioViewportCropPlaceholder {max-height: 600px;}.section-aspectRatioViewportBottomSpacer, .section-aspectRatioViewportBottomPlaceholder {max-height: 300px;}.zoomable:before {top: -600px; left: -800px; padding: 600px 800px;}</style><!--[if lt IE 9]><script charset="UTF-8" src="https://cdn-static-1.medium.com/_/fp/js/shiv.RI2ePTZ5gFmMgLzG5bEVAA.js"></script><![endif]--><link rel="icon" href="https://cdn-images-1.medium.com/fit/c/128/128/1*cciPf4CUXd_Zyux0Jg0yBQ.png" class="js-favicon"><link rel="apple-touch-icon" sizes="152x152" href="https://cdn-images-1.medium.com/fit/c/152/152/1*N56Qc5-aHTcfGff0scntKQ.png"><link rel="apple-touch-icon" sizes="120x120" href="https://cdn-images-1.medium.com/fit/c/120/120/1*N56Qc5-aHTcfGff0scntKQ.png"><link rel="apple-touch-icon" sizes="76x76" href="https://cdn-images-1.medium.com/fit/c/76/76/1*N56Qc5-aHTcfGff0scntKQ.png"><link rel="apple-touch-icon" sizes="60x60" href="https://cdn-images-1.medium.com/fit/c/60/60/1*N56Qc5-aHTcfGff0scntKQ.png"><link rel="mask-icon" href="https://cdn-static-1.medium.com/_/fp/icons/monogram-mask.KPLCSFEZviQN0jQ7veN2RQ.svg" color="#171717"><meta property="og:image" content=""><style type="text/css">.metabar,.u-fixed,footer { display: none}</style></head><body itemscope="" class="postShowScreen browser-chrome os-mac v-glyph v-glyph--m2 is-js is-resizing" data-action-scope="_actionscope_0"><script>document.body.className = document.body.className.replace(/(^|\s)is-noJs(\s|$)/, "$1is-js$2")</script><div class="site-main surface-container" id="container"><div class="butterBar butterBar--error" data-action-scope="_actionscope_1"></div><div class="surface" id="_obv.shell._surface_1557467652343" style="display: block; visibility: visible;"><div class="screenContent surface-content is-supplementalPostContentLoaded" data-used="true" data-action-scope="_actionscope_2"><canvas class="canvas-renderer" width="800" height="600"></canvas><div class="container u-maxWidth740 u-xs-margin0 notesPositionContainer js-notesPositionContainer"><div class="notesMarkers" data-action-scope="_actionscope_4"></div></div><div class="metabar u-clearfix u-boxShadow4px12pxBlackLightest u-fixed u-backgroundTransparentWhiteDarkest u-xs-sizeFullViewportWidth js-metabar"><div class="branch-journeys-top"></div><div class="js-metabarMiddle metabar-inner u-marginAuto u-maxWidth1032 u-flexCenter u-justifyContentSpaceBetween u-height65 u-xs-height56 u-paddingHorizontal20"><div class="metabar-block u-flex1 u-flexCenter"><div class="u-xs-hide js-metabarLogoLeft"><a href="https://medium.com/" data-log-event="home" class="siteNav-logo u-fillTransparentBlackDarker u-flex0 u-flexCenter u-paddingTop0"><span class="svgIcon svgIcon--logoMonogram svgIcon--45px is-flushLeft u-flex0 u-flexCenter u-paddingTop0"><svg class="svgIcon-use" width="45" height="45"><path d="M5 40V5h35v35H5zm8.56-12.627c0 .555-.027.687-.318 1.03l-2.457 2.985v.396h6.974v-.396l-2.456-2.985c-.291-.343-.344-.502-.344-1.03V18.42l6.127 13.364h.714l5.256-13.364v10.644c0 .29 0 .342-.185.528l-1.848 1.796v.396h9.19v-.396l-1.822-1.796c-.184-.186-.21-.238-.21-.528V15.937c0-.291.026-.344.21-.528l1.823-1.797v-.396h-6.471l-4.622 11.542-5.203-11.542h-6.79v.396l2.14 2.64c.239.292.291.37.291.768v10.353z"></path></svg></span><span class="u-textScreenReader">Homepage</span></a></div><div class="u-xs-show js-metabarLogoLeft"><a href="https://medium.com/" data-log-event="home" class="siteNav-logo u-fillTransparentBlackDarker u-flex0 u-flexCenter u-paddingTop0"><span class="svgIcon svgIcon--logoMonogram svgIcon--45px is-flushLeft u-flex0 u-flexCenter u-paddingTop0"><svg class="svgIcon-use" width="45" height="45"><path d="M5 40V5h35v35H5zm8.56-12.627c0 .555-.027.687-.318 1.03l-2.457 2.985v.396h6.974v-.396l-2.456-2.985c-.291-.343-.344-.502-.344-1.03V18.42l6.127 13.364h.714l5.256-13.364v10.644c0 .29 0 .342-.185.528l-1.848 1.796v.396h9.19v-.396l-1.822-1.796c-.184-.186-.21-.238-.21-.528V15.937c0-.291.026-.344.21-.528l1.823-1.797v-.396h-6.471l-4.622 11.542-5.203-11.542h-6.79v.396l2.14 2.64c.239.292.291.37.291.768v10.353z"></path></svg></span><span class="u-textScreenReader">Homepage</span></a></div><div class="u-flexCenter u-height65 u-xs-height56"><span class="u-inlineBlock u-height28 u-xs-height24 u-verticalAlignTop u-marginRight20 u-marginLeft15 u-borderRightLighter"></span></div><div class="u-flexCenter u-height65 u-xs-height56 u-marginRight18"><div class="u-xs-show"><a class="link u-baseColor--link avatar avatar--roundedRectangle" href="https://ai-alignment.com?source=avatar-lo_JuU2p2EvGepj-624d886c4aa4" title="Go to AI Alignment" aria-label="Go to AI Alignment" data-action-source="avatar-lo_JuU2p2EvGepj-624d886c4aa4" data-collection-slug="ai-control"><img src="https://cdn-images-1.medium.com/fit/c/32/32/1*N56Qc5-aHTcfGff0scntKQ.png" class="avatar-image avatar-image--icon" alt="AI Alignment"></a></div><div class="u-xs-hide"><a href="https://ai-alignment.com?source=logo-lo_JuU2p2EvGepj---624d886c4aa4" class="u-flexCenter js-collectionLogoOrName"><span class="u-noWrapWithEllipsis u-maxWidth1032 u-uiTextBold u-fontSize26 u-textColorDarker">AI Alignment</span></a></div></div></div><div class="metabar-block u-flex0 u-flexCenter"><div class="u-flexCenter u-height65 u-xs-height56"><div class="buttonSet buttonSet--wide u-lineHeightInherit"><a class="button button--primary button--chromeless u-accentColor--buttonNormal is-inSiteNavBar u-xs-hide js-signInButton" href="https://medium.com/m/signin?redirect=https%3A%2F%2Fai-alignment.com%2Ftowards-formalizing-universality-409ab893a456&amp;source=--------------------------nav_reg&amp;operation=login" data-action="sign-in-prompt" data-redirect="https://ai-alignment.com/towards-formalizing-universality-409ab893a456" data-action-source="--------------------------nav_reg">Sign in</a><a class="button button--primary button--withChrome u-accentColor--buttonNormal is-inSiteNavBar js-signUpButton" href="https://medium.com/m/signin?redirect=https%3A%2F%2Fai-alignment.com%2Ftowards-formalizing-universality-409ab893a456&amp;source=--------------------------nav_reg&amp;operation=register" data-action="sign-up-prompt" data-redirect="https://ai-alignment.com/towards-formalizing-universality-409ab893a456" data-action-source="--------------------------nav_reg">Get started</a></div></div></div></div></div><div class="metabar metabar--spacer js-metabarSpacer u-height65 u-xs-height56"></div><main role="main"><article class=" u-minHeight100vhOffset65 u-overflowHidden postArticle postArticle--full is-withAccentColors u-marginBottom40" lang="en"><div class="postArticle-content js-postField js-notesSource js-trackPostScrolls" data-post-id="409ab893a456" data-source="post_page" data-collection-id="624d886c4aa4" data-tracking-context="postPage" data-scroll="native"><section name="17dd" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h1 name="866c" id="866c" class="graf graf--h3 graf--leading graf--title">Towards formalizing universality</h1><div class="uiScale uiScale-ui--regular uiScale-caption--regular u-flexCenter u-marginVertical24 u-fontSize15 js-postMetaLockup"><div class="u-flex0"><a class="link u-baseColor--link avatar" href="https://ai-alignment.com/@paulfchristiano?source=post_header_lockup" data-action="show-user-card" data-action-source="post_header_lockup" data-action-value="57f1a655a613" data-action-type="hover" data-user-id="57f1a655a613" data-collection-slug="ai-control" dir="auto"><img src="https://cdn-images-1.medium.com/fit/c/50/50/1*BNjZCuQuRfIgcXCBMipuBw.jpeg" class="avatar-image u-size50x50" alt="Go to the profile of Paul Christiano"></a></div><div class="u-flex1 u-paddingLeft15 u-overflowHidden"><div class="u-paddingBottom3"><a class="ds-link ds-link--styleSubtle ui-captionStrong u-inlineBlock link link--darken link--darker" href="https://ai-alignment.com/@paulfchristiano" data-action="show-user-card" data-action-value="57f1a655a613" data-action-type="hover" data-user-id="57f1a655a613" data-collection-slug="ai-control" dir="auto">Paul Christiano</a><span class="followState js-followState" data-user-id="57f1a655a613"><button class="button button--smallest u-noUserSelect button--withChrome u-baseColor--buttonNormal button--withHover button--unblock js-unblockButton u-marginLeft10 u-xs-hide" data-action="sign-up-prompt" data-sign-in-action="toggle-block-user" data-requires-token="true" data-redirect="https://ai-alignment.com/towards-formalizing-universality-409ab893a456" data-action-source="post_header_lockup"><span class="button-label  button-defaultState">Blocked</span><span class="button-label button-hoverState">Unblock</span></button><button class="button button--primary button--smallest button--dark u-noUserSelect button--withChrome u-accentColor--buttonDark button--follow js-followButton u-marginLeft10 u-xs-hide" data-action="sign-up-prompt" data-sign-in-action="toggle-subscribe-user" data-requires-token="true" data-redirect="https://medium.com/_/subscribe/user/57f1a655a613" data-action-source="post_header_lockup-57f1a655a613-------------------------follow_byline"><span class="button-label  button-defaultState js-buttonLabel">Follow</span><span class="button-label button-activeState">Following</span></button></span></div><div class="ui-caption u-noWrapWithEllipsis js-testPostMetaInlineSupplemental"><time datetime="2019-01-10T03:49:52.461Z">Jan 9</time><span class="middotDivider u-fontSize12"></span><span class="readingTime" title="22 min read"></span></div></div></div><p name="9ced" id="9ced" class="graf graf--p graf-after--h3">The scalability of <a href="https://arxiv.org/pdf/1810.08575.pdf" data-href="https://arxiv.org/pdf/1810.08575.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">iterated amplification</a> or <a href="https://arxiv.org/abs/1805.00899" data-href="https://arxiv.org/abs/1805.00899" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">debate</a> seems to depend on whether large enough teams of humans can carry out arbitrarily complicated reasoning. Are these schemes â€œuniversal,â€ or are there kinds of reasoning that work but which humans fundamentally canâ€™t understand?</p><p name="22c8" id="22c8" class="graf graf--p graf-after--p">This post defines the concept of â€œascription universality,â€ which tries to capture the property that a question-answering system <strong class="markup--strong markup--p-strong">A</strong> is better-informed than any particular simpler computation <strong class="markup--strong markup--p-strong">C</strong>.</p><p name="4a93" id="4a93" class="graf graf--p graf-after--p"><a href="https://ai-alignment.com/informed-oversight-18fcb5d3d1e1" data-href="https://ai-alignment.com/informed-oversight-18fcb5d3d1e1" class="markup--anchor markup--p-anchor" target="_blank">These</a> <a href="https://ai-alignment.com/universality-and-consequentialism-within-hch-c0bee00365bd" data-href="https://ai-alignment.com/universality-and-consequentialism-within-hch-c0bee00365bd" class="markup--anchor markup--p-anchor" target="_blank">parallel</a> <a href="https://ai-alignment.com/universality-and-model-based-rl-b08701394ddd" data-href="https://ai-alignment.com/universality-and-model-based-rl-b08701394ddd" class="markup--anchor markup--p-anchor" target="_blank">posts</a> explain why I believe that the alignment of iterated amplification largely depends on whether HCH is ascription universal. Ultimately I think that the â€œrightâ€ definition will be closely tied to the use we want to make of it, and so we should be refining this definition in parallel with exploring its applications.</p><p name="429b" id="429b" class="graf graf--p graf-after--p">Iâ€™m using the awkward term â€œascription universalityâ€ partly to explicitly flag that this is a preliminary definition, and partly to reserve linguistic space for the better definitions that Iâ€™m optimistic will follow.</p><p name="05ca" id="05ca" class="graf graf--p graf-after--p">(Thanks to Geoffrey Irving for discussions about many of the ideas in this post.)</p><h3 name="13a9" id="13a9" class="graf graf--h3 graf-after--p">I. Definition</h3><p name="3ad6" id="3ad6" class="graf graf--p graf-after--h3">We will try to define what it means for a question-answering system <strong class="markup--strong markup--p-strong">A</strong> to be â€œascription universal.â€</p><h4 name="dc32" id="dc32" class="graf graf--h4 graf-after--p">1. Ascribing beliefs to&nbsp;A</h4><p name="d07e" id="d07e" class="graf graf--p graf-after--h4">Fix a language (e.g. English with arbitrarily big compound <a href="https://ai-alignment.com/approval-directed-algorithm-learning-bf1f8fad42cd" data-href="https://ai-alignment.com/approval-directed-algorithm-learning-bf1f8fad42cd" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">terms</a>) in which we can represent questions and answers.</p><p name="beab" id="beab" class="graf graf--p graf-after--p">To ascribe beliefs to <strong class="markup--strong markup--p-strong">A</strong>, we ask it. If <strong class="markup--strong markup--p-strong">A</strong>(â€œare there infinitely many twin primes?â€) = â€œprobably, though itâ€™s hard to be sureâ€ then we ascribe that belief about twin primes to <strong class="markup--strong markup--p-strong">A</strong>.</p><p name="ef03" id="ef03" class="graf graf--p graf-after--p">This is not a general way of ascribing â€œbelief.â€ This procedure wouldnâ€™t capture the beliefs of a native Spanish speaker, or for someone who wasnâ€™t answering questions honestly. But it can give us a sufficient condition, and is particularly useful for someone who wants to use <strong class="markup--strong markup--p-strong">A</strong> as part of an alignment scheme.</p><p name="88ac" id="88ac" class="graf graf--p graf-after--p">Even in this â€œstraightforwardâ€ procedure there is a lot of subtlety. In some cases there are questions that we canâ€™t articulate in our language, but which (when combined with <strong class="markup--strong markup--p-strong">A</strong>â€™s other beliefs) have consequences that we can articulate. In this case, we can infer something about <strong class="markup--strong markup--p-strong">A</strong>â€™s beliefs from its answers to the questions that we can articulate.</p><h4 name="b8f3" id="b8f3" class="graf graf--h4 graf-after--p">2. Ascribing beliefs to arbitrary computations</h4><p name="91fd" id="91fd" class="graf graf--p graf-after--h4">We are interested in whether <strong class="markup--strong markup--p-strong">A</strong> â€œcan understand everything that could be understood by someone.â€ To clarify this, we need to be more precise about what we mean by â€œcould be understood by someone.â€</p><p name="a5f2" id="a5f2" class="graf graf--p graf-after--p">This will be the most informal step in this post. (Not that any of it is very formal!)</p><p name="e68f" id="e68f" class="graf graf--p graf-after--p">We can imagine various ways of ascribing<em class="markup--em markup--p-em"> </em>beliefs to an arbitrary computation <strong class="markup--strong markup--p-strong">C</strong>. For example:</p><ul class="postList"><li name="b7d3" id="b7d3" class="graf graf--li graf-after--p">We can give <strong class="markup--strong markup--li-strong">C</strong> questions in a particular encoding and assume its answers reflect its beliefs. We can either use those answers directly to infer <strong class="markup--strong markup--li-strong">C</strong>â€™s beliefs (as in the last section), or we can ask what set of beliefs about latent facts would explain <strong class="markup--strong markup--li-strong">C</strong>â€™s answers.</li><li name="f1e9" id="f1e9" class="graf graf--li graf-after--li">We can view <strong class="markup--strong markup--li-strong">C</strong> as optimizing<em class="markup--em markup--li-em"> </em>something and<em class="markup--em markup--li-em"> </em>ask what set of beliefs rationalize that optimization. For example, we can give <strong class="markup--strong markup--li-strong">C</strong> a chess board as input, see what move it produces, assume it is trying to win, and infer what it must believe. We might conclude that <strong class="markup--strong markup--li-strong">C</strong> believes a particular line of play will be won by black, or that <strong class="markup--strong markup--li-strong">C</strong> believes general heuristics like â€œa pawn is worth 3 tempi,â€ or so on.</li><li name="ed07" id="ed07" class="graf graf--li graf-after--li">We can reason about how <strong class="markup--strong markup--li-strong">C</strong>â€™s behavior depends on facts about the world, and ask what state of the world is determined by its current behavior. For example, we can observe that <strong class="markup--strong markup--li-strong">C</strong>(113327) = 1 but that <strong class="markup--strong markup--li-strong">C</strong>(113327) â€œwould have beenâ€ 0 if 113327 had been composite, concluding that <strong class="markup--strong markup--li-strong">C</strong>(11327) â€œknowsâ€ that 113327 is prime. We can extend to probabilistic beliefs, e.g. if <strong class="markup--strong markup--li-strong">C</strong>(113327) â€œprobablyâ€ would have been 0 if 113327 had been composite, then we might that <strong class="markup--strong markup--li-strong">C</strong> knows that 113327 is â€œprobably prime.â€ This certainly isnâ€™t a precise definition, since it involves considering logical counterfactuals, and Iâ€™m not clear whether it can be made precise. (See also ideas along the lines of <a href="https://www.lesswrong.com/posts/b3Bt9Cz4hEtR26ANX/knowledge-is-freedom" data-href="https://www.lesswrong.com/posts/b3Bt9Cz4hEtR26ANX/knowledge-is-freedom" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">â€œknowledge is freedomâ€</a>.)</li><li name="a8d3" id="a8d3" class="graf graf--li graf-after--li">If a computation behaves differently under different conditions, then we could use restrict attention to a particular condition. For example, if a question-answering system appears to be bilingual but answers questions differently in Spanish and English, we could ascribe two different sets of beliefs. Similarly, we could ascribe beliefs to any subcomputation. For example, if a part of <strong class="markup--strong markup--li-strong">C </strong>can be understood as optimizing the way data is laid out in memory, then we can ascribe beliefs to that computation about the way that data will be used.</li></ul><p name="320c" id="320c" class="graf graf--p graf-after--li">Note that these arenâ€™t intended to be efficient procedures that we could actually apply to a given computation <strong class="markup--strong markup--p-strong">C</strong>. They are hypothetical procedures that we will use to define what it means for <strong class="markup--strong markup--p-strong">A</strong> to be universal.</p><p name="168b" id="168b" class="graf graf--p graf-after--p">Iâ€™m not going to try to ascribe a single set of beliefs to a given computation; instead, Iâ€™ll consider all of the reasonable ascription procedures. For example, I think different procedures would ascribe different beliefs to a particular human, and donâ€™t want to claim there is a unique answer to what a human â€œreallyâ€ believes. A universal reasoner needs to have more reasonable beliefs than the beliefs ascribed to that a human using any particular method.</p><p name="5662" id="5662" class="graf graf--p graf-after--p">An ascription-universal reasoner needs to compete with any beliefs that can be ascribed to <strong class="markup--strong markup--p-strong">C</strong>, so I want to be generous with this definition. For example, given a chess-playing algorithm, we might rationalize it as trying to win a game and infer its beliefs about the rules of chess. Or we might rationalize it as trying to look like a human and infer its beliefs about what a human would do. Or something different altogether. Most of these will be kind of crazy ascriptions, but I want to compete with them anyway (competing with crazier beliefs will turn out to just be easier).</p><p name="8cb8" id="8cb8" class="graf graf--p graf-after--p">Itâ€™s not totally clear what counts as a â€œreasonableâ€ ascription procedure, and thatâ€™s the biggest source of informality. Intuitively, the key property is that the ascription itself isnâ€™t doing the â€œhard work.â€ In practice Iâ€™m using an informal extensional definition, guided by examples like those in the bulleted list.</p><h4 name="2960" id="2960" class="graf graf--h4 graf-after--p">3. Comparing beliefs</h4><p name="295c" id="295c" class="graf graf--p graf-after--h4">What does it mean to say that one agent is â€œbetter-informedâ€ than another?</p><p name="8fe9" id="8fe9" class="graf graf--p graf-after--p">Itâ€™s natural to try to express this in terms of empirical information about the world, but we are particularly interested in the different inferences<em class="markup--em markup--p-em"> </em>that agents are able to draw from the same data. Another natural approach is to compare their â€œknowledge,â€ but I have no idea how to define knowledge or justified belief. So Iâ€™m reduced to working directly with sets of beliefs.</p><p name="0358" id="0358" class="graf graf--p graf-after--p">Consider two sets of beliefs, described by the subjective expectations ğ”¼Â¹ and ğ”¼Â². What does it mean to say that ğ”¼Â¹ is better-informed than ğ”¼Â²?</p><p name="d7eb" id="d7eb" class="graf graf--p graf-after--p">This framing makes it tempting to try something simple: â€œfor every quantity, ğ”¼Â¹â€™s belief about that quantity is more accurate.â€ But this is property is totally unachievable. Even if ğ”¼Â¹ is obtained by conditioning ğ”¼Â² on a true fact, it will almost certainly happen to update in the â€œwrongâ€ direction for some claims.</p><p name="f24e" id="f24e" class="graf graf--p graf-after--p">We will instead use a subjective definition, i.e. weâ€™ll define this concept from a particular epistemic position represented by another subjective expectation ğ”¼.</p><p name="33c1" id="33c1" class="graf graf--p graf-after--p">Then we say that ğ”¼Â¹ <strong class="markup--strong markup--p-strong">dominates </strong>ğ”¼Â² (w.r.t. ğ”¼) if, for every bounded quantity X and for every â€œniceâ€ property Î¦:</p><ul class="postList"><li name="021e" id="021e" class="graf graf--li graf-after--p">ğ”¼[X|Î¦(ğ”¼Â¹, ğ”¼Â²)] = ğ”¼[ğ”¼Â¹[X]|Î¦(ğ”¼Â¹, ğ”¼Â²)]</li></ul><p name="bdbe" id="bdbe" class="graf graf--p graf-after--li">(By â€œniceâ€ I mean something like: simple to define and open in the product topology, viewing ğ”¼Â¹ and ğ”¼Â² as infinite tables of numbers.)</p><p name="9d25" id="9d25" class="graf graf--p graf-after--p">Intuitively, this means that ğ”¼ always â€œtrustsâ€ ğ”¼Â¹, even if given arbitrary information about ğ”¼Â¹ and ğ”¼Â². For example, if ğ”¼ was told that ğ”¼Â¹[X] â‰ˆ <em class="markup--em markup--p-em">x</em> and<br>ğ”¼Â²[X] â‰ˆ <em class="markup--em markup--p-em">y</em>, then it would expect X to be around <em class="markup--em markup--p-em">x</em> (rather than <em class="markup--em markup--p-em">y</em>)<em class="markup--em markup--p-em">. </em>Allowing arbitrary predicates Î¦ allows us to make stronger inferences, effectively that ğ”¼ thinks that ğ”¼Â¹ captures <em class="markup--em markup--p-em">everything</em> useful about ğ”¼Â².</p><p name="2809" id="2809" class="graf graf--p graf-after--p">Iâ€™m not sure if this is exactly the right property, and it becomes particularly tricky if the quantity X is itself related to the behavior of ğ”¼Â¹ or ğ”¼Â² (continuity in the product topology is the minimum plausible condition to avoid a self-referential paradox). But I think itâ€™s at least roughly what we want and it may be exactly what we want.</p><p name="5a96" id="5a96" class="graf graf--p graf-after--p">Note that dominance is <em class="markup--em markup--p-em">subjective</em>, i.e. it depends on the epistemic vantage point ğ”¼ used for the outer expectation. This property is a little bit stronger than what we originally asked for, since it also requires ğ”¼ to trust ğ”¼Â¹, but this turns out to be implied anyway by our definition of universality so itâ€™s not a big defect.</p><p name="49ff" id="49ff" class="graf graf--p graf-after--p">Note that dominance is a property of the <em class="markup--em markup--p-em">descriptions</em> of ğ”¼Â¹ and ğ”¼Â². There could be two different computations that in fact compute the same set of expectations, such that ğ”¼ trusts one of them but not the other. Perhaps one computation hard-codes a particular result, while the other does a bunch of work to estimate it. Even if the hard-coded result happened to be correct, such that the two computations had the same outputs, ğ”¼ might trust the hard work but not the wild guess.</p><h4 name="9acc" id="9acc" class="graf graf--h4 graf-after--p">4. Complexity and parameterization</h4><p name="df2a" id="df2a" class="graf graf--p graf-after--h4">There are computations with arbitrarily sophisticated beliefs, so no fixed <strong class="markup--strong markup--p-strong">A</strong> can hope to dominate everything. To remedy this, rather than comparing to a fixed question-answerer <strong class="markup--strong markup--p-strong">A</strong>, weâ€™ll compare to a parameterized family <strong class="markup--strong markup--p-strong">A</strong>[<strong class="markup--strong markup--p-strong">C</strong>].</p><p name="3982" id="3982" class="graf graf--p graf-after--p">Iâ€™ll consider two different kinds of potentially-universal reasoners <strong class="markup--strong markup--p-strong">A</strong>:</p><ul class="postList"><li name="41cc" id="41cc" class="graf graf--li graf-after--p">In the â€œidealizedâ€ case, <strong class="markup--strong markup--li-strong">A</strong>[<strong class="markup--strong markup--li-strong">C</strong>] depends only on the complexity of <strong class="markup--strong markup--li-strong">C</strong>.<br>For example, we might hope that an <em class="markup--em markup--li-em">n</em>-round debate dominates any beliefs that could be ascribed to a fast computation with (<em class="markup--em markup--li-em">n</em>-1) rounds of <a href="https://en.wikipedia.org/wiki/Alternating_Turing_machine" data-href="https://en.wikipedia.org/wiki/Alternating_Turing_machine" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">alternation</a>. In particular, this <strong class="markup--strong markup--li-strong">A</strong>[<strong class="markup--strong markup--li-strong">C</strong>] is the same for any two computations <strong class="markup--strong markup--li-strong">C</strong> of the same complexity.</li><li name="2da1" id="2da1" class="graf graf--li graf-after--li">In the â€œpracticalâ€ case,<strong class="markup--strong markup--li-strong"> A</strong>[<strong class="markup--strong markup--li-strong">C</strong>] depends on the complexity of <strong class="markup--strong markup--li-strong">C</strong> but also uses the computation <strong class="markup--strong markup--li-strong">C</strong> as a hint. For example, if <strong class="markup--strong markup--li-strong">C</strong> is the training process for a neural net, then we might take <strong class="markup--strong markup--li-strong">A</strong>[<strong class="markup--strong markup--li-strong">C</strong>] to be a debate in which the debaters are able to share weights and activations with the neural net throughout the entire training process.</li></ul><p name="e749" id="e749" class="graf graf--p graf-after--li">Iâ€™m generally interested in the case where <strong class="markup--strong markup--p-strong">A</strong>[<strong class="markup--strong markup--p-strong">C</strong>] is only slightly more powerful than <strong class="markup--strong markup--p-strong">C</strong> itself. This mirrors the setting where a universal Turing machine is able to run any other Turing machine with only a modest slowdown.</p><h4 name="4844" id="4844" class="graf graf--h4 graf-after--p">Putting it all&nbsp;together</h4><p name="b5e0" id="b5e0" class="graf graf--p graf-after--h4">We say that a set of beliefs ğ”¼á´¬ <em class="markup--em markup--p-em">epistemically dominates</em> a computation <strong class="markup--strong markup--p-strong">C </strong>(w.r.t. some beliefs ğ”¼ and language L) if the beliefs ascribed to A by the â€œstraightforwardâ€ procedure, using L, dominate (w.r.t. ğ”¼) the beliefs ascribed to <strong class="markup--strong markup--p-strong">C</strong> by any reasonable ascription procedure.</p><p name="9c1b" id="9c1b" class="graf graf--p graf-after--p">We say that a family of question-answering systems <strong class="markup--strong markup--p-strong">A</strong>[<strong class="markup--strong markup--p-strong">Â·</strong>] are <em class="markup--em markup--p-em">ascription universal</em> (w.r.t. ğ”¼ and L) if <strong class="markup--strong markup--p-strong">A</strong>[<strong class="markup--strong markup--p-strong">C</strong>] epistemically dominates <strong class="markup--strong markup--p-strong">C</strong> for every computation <strong class="markup--strong markup--p-strong">C</strong>.</p><h3 name="3398" id="3398" class="graf graf--h3 graf-after--p">II. Discussion</h3><h4 name="a216" id="a216" class="graf graf--h4 graf-after--h3">Why is (subjective) dominance sufficient?</h4><p name="5e85" id="5e85" class="graf graf--p graf-after--h4">This universality condition requires that <em class="markup--em markup--p-em">we</em> believe that <strong class="markup--strong markup--p-strong">A</strong>[<strong class="markup--strong markup--p-strong">C</strong>] is better-informed than <strong class="markup--strong markup--p-strong">C</strong>. Naively we might have wanted it to <em class="markup--em markup--p-em">actually</em> be the case that <strong class="markup--strong markup--p-strong">A</strong>[<strong class="markup--strong markup--p-strong">C</strong>] is better-informed than <strong class="markup--strong markup--p-strong">C</strong>; the stronger condition is clearly unachievable, but why should we be satisfied with the weaker condition?</p><p name="d8e8" id="d8e8" class="graf graf--p graf-after--p">In <a href="https://ai-alignment.com/informed-oversight-18fcb5d3d1e1" data-href="https://ai-alignment.com/informed-oversight-18fcb5d3d1e1" class="markup--anchor markup--p-anchor" target="_blank">applications</a> <a href="https://ai-alignment.com/universality-and-consequentialism-within-hch-c0bee00365bd" data-href="https://ai-alignment.com/universality-and-consequentialism-within-hch-c0bee00365bd" class="markup--anchor markup--p-anchor" target="_blank">of this</a> <a href="https://ai-alignment.com/universality-and-model-based-rl-b08701394ddd" data-href="https://ai-alignment.com/universality-and-model-based-rl-b08701394ddd" class="markup--anchor markup--p-anchor" target="_blank">property</a>, the subjective condition is what we need in order for <em class="markup--em markup--p-em">us </em>to believe that <strong class="markup--strong markup--p-strong">A</strong>[<strong class="markup--strong markup--p-strong">C</strong>] will cope with the challenges posed by <strong class="markup--strong markup--p-strong">C</strong>. For example, suppose that <strong class="markup--strong markup--p-strong">C</strong> formulates a plan to â€œtrickâ€ <strong class="markup--strong markup--p-strong">A</strong>[<strong class="markup--strong markup--p-strong">C</strong>]. Then the subjective universality condition implies that we donâ€™t expect <strong class="markup--strong markup--p-strong">C</strong> to succeed.</p><p name="71dd" id="71dd" class="graf graf--p graf-after--p">This isnâ€™t as good as actually knowing that <strong class="markup--strong markup--p-strong">C</strong> wonâ€™t succeed. But I think it should be good enough for usâ€Šâ€”â€Šthe reason we are thinking about AI safety is because we are concerned that something bad will happen. If we find a technique that defuses this argument, then weâ€™ve addressed the motivating problem. It may still be the case that bad things happen (and we should still search for additional reasons that bad things might happen), but we donâ€™t particularly expect them to.</p><p name="eb3b" id="eb3b" class="graf graf--p graf-after--p">Of course if you select over a large number of computations, then you may find one that will succeed in tricking <strong class="markup--strong markup--p-strong">A</strong>. But if we are concerned about that, then we can instead apply ascription universality to the entire process including the selection.</p><h4 name="1361" id="1361" class="graf graf--h4 graf-after--p">Why trust opaque computation?</h4><p name="9535" id="9535" class="graf graf--p graf-after--h4">If <strong class="markup--strong markup--p-strong">C</strong> uses some clever heuristics that I donâ€™t understand, then <strong class="markup--strong markup--p-strong">C</strong>â€™s â€œbeliefsâ€ might be excellent, but I might not <em class="markup--em markup--p-em">expect</em> them to be excellent. In this sense understanding may seem almost vacuous. If there is some heuristic that I trust, wouldnâ€™t <strong class="markup--strong markup--p-strong">A </strong>just use it?</p><p name="80aa" id="80aa" class="graf graf--p graf-after--p">To see why the definition is demanding, consider the special case where <strong class="markup--strong markup--p-strong">C</strong> performs an extensive search to find a computation that works well empirically. For example, <strong class="markup--strong markup--p-strong">C</strong> might be the following computation:</p><ul class="postList"><li name="0fa2" id="0fa2" class="graf graf--li graf-after--p">Start with a training set of (image, label) pairs.</li><li name="790d" id="790d" class="graf graf--li graf-after--li">Search over simple programs to find one that makes good predictions.</li><li name="e464" id="e464" class="graf graf--li graf-after--li">Run that simple program on a new image to predict its label.</li></ul><p name="1b49" id="1b49" class="graf graf--p graf-after--li">In this case, we can ascribe beliefs to <strong class="markup--strong markup--p-strong">C</strong> about the contents of the new image. And because those beliefs are coming from a simple program that works empirically, I expect them to be accurate (in some respects).</p><p name="a800" id="a800" class="graf graf--p graf-after--p">For example, a simple classifier <strong class="markup--strong markup--p-strong">C</strong> may â€œbelieveâ€ that the new image contains a particular curve that typically appears in images labeled â€œdog;â€ or a really sophisticated classifier may perform complex deductions about the contents of the scene, starting from premises that were empirically validated on the training set.</p><p name="440a" id="440a" class="graf graf--p graf-after--p">So itâ€™s not OK for <strong class="markup--strong markup--p-strong">A</strong> to simply ignore whatever heuristics <strong class="markup--strong markup--p-strong">C</strong> is usingâ€Šâ€”â€Šif those heuristics have the kind of empirical support that makes us think they actually work, then A needs to be able to understand everything that those heuristics imply about the domain.</p><h4 name="d06b" id="d06b" class="graf graf--h4 graf-after--p">Why be so&nbsp;general?</h4><p name="877a" id="877a" class="graf graf--p graf-after--h4">Iâ€™ve formulated universality as competing with arbitrary computations <strong class="markup--strong markup--p-strong">C</strong>. It seems totally possible that the form of <strong class="markup--strong markup--p-strong">C </strong>discussed in the last sectionâ€Šâ€”â€Šsearching for a program that works well in practice and then using it in a new situationâ€Šâ€”â€Šis so central that the definition of universality should focus entirely on it.</p><p name="ec32" id="ec32" class="graf graf--p graf-after--p">One reason to use the broader definition is because sometimes this â€œselectionâ€ process can be embedded in a non-trivial way in a larger computation. For example, if I have a sufficiently large group of humans, I might expect memetic selection to occur and produce systems that could be said to have â€œbeliefs,â€ and Iâ€™d like universal systems to dominate those beliefs as well.</p><p name="6699" id="6699" class="graf graf--p graf-after--p">The other reason to use this very general definition is because I donâ€™t see an easy way to simplify the definition by using the additional structural assumption about <strong class="markup--strong markup--p-strong">C</strong>. I do think itâ€™s likely thereâ€™s a nicer statement out there that someone else can find.</p><h4 name="35b2" id="35b2" class="graf graf--h4 graf-after--p">Universal from whose perspective?</h4><p name="3d18" id="3d18" class="graf graf--p graf-after--h4">Unfortunately, achieving universality depends a lot on the epistemic perspective ğ”¼ from which it is being evaluated. For example, if ğ”¼ knows any facts, than a universal agent must know all of those facts as well. Thus â€œa debate judged by Paulâ€ may be universal from Paulâ€™s perspective, but â€œa debate arbitrated by Aliceâ€ cannot be universal from my perspective unless I believe that Alice knows everything I know.</p><p name="2187" id="2187" class="graf graf--p graf-after--p">This isnâ€™t necessarily a big problem. It will limit us to conclusions like: Google engineers believe that the AI theyâ€™ve built serves the userâ€™s interests reasonably well. The user might not agree with that assessment, if they have different beliefs from Google engineers. This is what youâ€™d expect in any case where Google engineers build a product, however good their intentions.</p><p name="3299" id="3299" class="graf graf--p graf-after--p">(Of course Google engineersâ€™ notion of â€œserving the userâ€™s interestsâ€ can involve deferring to the userâ€™s beliefs in cases where they disagree with Google engineers, just as they could defer to the userâ€™s beliefs with other products. That gives us reason to be less concerned about such divergences, but eventually these evaluations do need to bottom out somewhere.)</p><p name="1a5f" id="1a5f" class="graf graf--p graf-after--p">This property becomes more problematic when we ask questions like: is there a way to <a href="https://ai-alignment.com/universality-and-security-amplification-551b314a3bab" data-href="https://ai-alignment.com/universality-and-security-amplification-551b314a3bab" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">seriously limit the inputs and outputs to a human while preserving universality of HCH</a>? This causes trouble because even if limiting the human intuitively preserves universality, it will effectively eliminate some of the humanâ€™s knowledge and know-how that can <a href="https://medium.com/@weidai/to-put-it-another-way-a-human-translator-has-learned-a-lot-of-valuable-information-much-of-it-48457f95b9bf" data-href="https://medium.com/@weidai/to-put-it-another-way-a-human-translator-has-learned-a-lot-of-valuable-information-much-of-it-48457f95b9bf" class="markup--anchor markup--p-anchor" target="_blank">only be accessed on large inputs</a>, and hence violate universality.</p><p name="aeac" id="aeac" class="graf graf--p graf-after--p">So when investigating schemes based on this kind of impoverished human, we would need to evaluate universality from some impoverished epistemic perspective. Weâ€™d like to say that the impoverished perspective is still â€œgood enoughâ€ for us to feel safe, despite not being good enough to capture literally everything we know. But now we risk begging the question: how do we evaluate whether the impoverished perspective is good enough? I think this is probably OK, but itâ€™s definitely subtle.</p><p name="691f" id="691f" class="graf graf--p graf-after--p">I think that defining universality w.r.t. ğ”¼ is an artifact of this definition strategy, and Iâ€™m optimistic that a better definition wouldnâ€™t have this dependence, probably by directly attacking the notion of â€œjustifiedâ€ belief (which would likely also be useful for actually establishing universality, and may even be necessary). But thatâ€™s a hard problem. Philosophers have thought about very similar problems extensively without making the kind of progress that seems adequate for our purposes, and I donâ€™t see an immediate angle of attack.</p><h3 name="1f08" id="1f08" class="graf graf--h3 graf-after--p">III. Which A might be universal?</h3><h4 name="f5a5" id="f5a5" class="graf graf--h4 graf-after--h3">Two regimes</h4><p name="8e1b" id="8e1b" class="graf graf--p graf-after--h4">Iâ€™m interested in universality in two distinct regimes:</p><ul class="postList"><li name="93c1" id="93c1" class="graf graf--li graf-after--p">Universality of idealized procedures defined in terms of perfect optimization, such as <a href="https://arxiv.org/abs/1805.00899" data-href="https://arxiv.org/abs/1805.00899" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">debate</a> under optimal play or <a href="https://ai-alignment.com/humans-consulting-hch-f893f6051455" data-href="https://ai-alignment.com/humans-consulting-hch-f893f6051455" class="markup--anchor markup--li-anchor" target="_blank">max-HCH</a>, where <strong class="markup--strong markup--li-strong">A</strong>[<strong class="markup--strong markup--li-strong">C</strong>] depends only the computational complexity of <strong class="markup--strong markup--li-strong">C</strong>.</li><li name="93ce" id="93ce" class="graf graf--li graf-after--li">Universality of practical variants, in which the perfect optimization is replaced by a bounded optimization, which might use the computation <strong class="markup--strong markup--li-strong">C</strong> as a â€œhint.â€ For example, we might consider amplification or debate where the agents are allowed to inspect a transcript of <strong class="markup--strong markup--li-strong">C</strong>; or if <strong class="markup--strong markup--li-strong">C</strong> is a neural net training process, we might train them jointly with <strong class="markup--strong markup--li-strong">C</strong>.</li></ul><h4 name="c90e" id="c90e" class="graf graf--h4 graf-after--li">Idealized models</h4><p name="a0bd" id="a0bd" class="graf graf--p graf-after--h4"><strong class="markup--strong markup--p-strong">A</strong>[<strong class="markup--strong markup--p-strong">C</strong>] could be:</p><ul class="postList"><li name="55ee" id="55ee" class="graf graf--li graf-after--p">A<strong class="markup--strong markup--li-strong"> </strong>debate, judged by a human, with perfect debaters, whose length depends on the complexity of <strong class="markup--strong markup--li-strong">C</strong>. (Hopefully in such a way that <strong class="markup--strong markup--li-strong">A</strong>[<strong class="markup--strong markup--li-strong">A</strong>[<strong class="markup--strong markup--li-strong">C</strong>]] is only a slightly longer debate than <strong class="markup--strong markup--li-strong">A</strong>[<strong class="markup--strong markup--li-strong">C</strong>].)</li><li name="2eb2" id="2eb2" class="graf graf--li graf-after--li">max-HCH, using perfect optimization rather than a bounded optimizer. Again, we can impose a budget limit that depends on the complexity of <strong class="markup--strong markup--li-strong">C</strong>, ideally such that <strong class="markup--strong markup--li-strong">A</strong>[<strong class="markup--strong markup--li-strong">A</strong>[<strong class="markup--strong markup--li-strong">C</strong>]] does not involve a much larger computation than <strong class="markup--strong markup--li-strong">A</strong>[<strong class="markup--strong markup--li-strong">C</strong>].</li><li name="95fd" id="95fd" class="graf graf--li graf-after--li">Use HCH to implement two functions, Info and Answer. Define <strong class="markup--strong markup--li-strong">A</strong>[<strong class="markup--strong markup--li-strong">C</strong>](Q) to be Answer(Q, <em class="markup--em markup--li-em">a*</em>)<em class="markup--em markup--li-em">, </em>where<em class="markup--em markup--li-em"> a* = </em>argmax Info(<em class="markup--em markup--li-em">a</em>, Q<em class="markup--em markup--li-em">), </em>and the optimization is over advice strings <em class="markup--em markup--li-em">a</em> of length comparable to the description of <strong class="markup--strong markup--li-strong">C</strong>.</li></ul><p name="10ec" id="10ec" class="graf graf--p graf-after--li">I donâ€™t think any of these would be universal if instantiated with an open-ended human policyâ€Šâ€”â€Šproviding arbitrary complex inputs to a human is likely to lead to bad behavior. Instead, I imagine restricting the human to some small enough space of inputs that they can behave â€œreasonablyâ€ over the whole space, while still being rich enough to be universal. Even as such, I think <a href="https://ai-alignment.com/universality-and-consequentialism-within-hch-c0bee00365bd" data-href="https://ai-alignment.com/universality-and-consequentialism-within-hch-c0bee00365bd" class="markup--anchor markup--p-anchor" target="_blank">additional measures</a> might be needed to achieve security (since a team of humans may behave badly in some exotic corners of the input space, even if individual humans are sheltered from exotic inputs).</p><h4 name="c396" id="c396" class="graf graf--h4 graf-after--p">Practical models</h4><p name="9ebe" id="9ebe" class="graf graf--p graf-after--h4">In practice Iâ€™m particularly interested in the case where <strong class="markup--strong markup--p-strong">C</strong> has the special form described in the â€œTrusting opaque computationsâ€ section: it trains a neural net on some dataset, and then runs that neural network on new inputs.</p><p name="aec8" id="aec8" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">A</strong>[<strong class="markup--strong markup--p-strong">C</strong>] could be:</p><ul class="postList"><li name="b309" id="b309" class="graf graf--li graf-after--p">Debates arbitrated by a human where the debaters are trained jointly with <strong class="markup--strong markup--li-strong">C</strong>, and can make references to individual examples from the dataset.</li><li name="2394" id="2394" class="graf graf--li graf-after--li">max-HCH where the agent is trained jointly with <strong class="markup--strong markup--li-strong">C</strong>, and the overseer can consult individual entries from the dataset.</li><li name="a03c" id="a03c" class="graf graf--li graf-after--li">HCH, combined with interpretability techniques that allow it to get information about what <strong class="markup--strong markup--li-strong">C</strong> is thinking and to relate its behavior to the dataset on which it was trained.</li></ul><p name="446c" id="446c" class="graf graf--p graf-after--li">For each of these practical variants, I donâ€™t expect it to be universal unless the corresponding idealized model is. So we can think of dividing the universality question into two pieces:</p><ul class="postList"><li name="2a27" id="2a27" class="graf graf--li graf-after--p">Would our procedure be universal <em class="markup--em markup--li-em">if the optimization was performed perfectly</em>?</li><li name="08f2" id="08f2" class="graf graf--li graf-after--li">Is weight sharing / interpretability strong enough to solve the optimization problems needed for universality?</li></ul><p name="11e5" id="11e5" class="graf graf--p graf-after--li">I think none of the suggestions in this section are plausibly universal without some additional mechanisms, and so even if HCH or debate is universal, additional ideas are needed in order to translate those to practical universal systems. Iâ€™m still inclined to focus on the idealized case, because I want to better understand what is needed for universality without thinking about algorithmic difficulties.</p><h3 name="237f" id="237f" class="graf graf--h3 graf-after--p">IV. Which C are hard to epistemically dominate?</h3><p name="ef7b" id="ef7b" class="graf graf--p graf-after--h3">The definition in this post was somewhat informal, but the discussion in this appendix will be extremely informal. The purpose is to communicate some of my early thinking about universality and my sense for what a universal reasoner might look like.</p><p name="43f4" id="43f4" class="graf graf--p graf-after--p">The first three examples will consider programs <strong class="markup--strong markup--p-strong">C</strong> with the following form:</p><ul class="postList"><li name="9984" id="9984" class="graf graf--li graf-after--p">Iterate over programs to find an <em class="markup--em markup--li-em">f</em> that successfully predicts the answers to some dataset of historical questions for which we know the answer.</li><li name="50cb" id="50cb" class="graf graf--li graf-after--li">Use that program to predict the answers to future questions.</li></ul><p name="778b" id="778b" class="graf graf--p graf-after--li">The different examples explore features of <em class="markup--em markup--p-em">f</em> that might make it difficult to dominate the overall computation.</p><h4 name="f14f" id="f14f" class="graf graf--h4 graf-after--p">Deduction</h4><p name="a24a" id="a24a" class="graf graf--p graf-after--h4">Suppose <em class="markup--em markup--p-em">f</em> takes the following form:</p><ul class="postList"><li name="63e7" id="63e7" class="graf graf--li graf-after--p">Given a question and context, it performs a short series of deductions from facts in the context, together with some axioms (e.g. â€œBobâ€™s eyes are more likely to twitch when he is lying,â€ the rules of poker, <em class="markup--em markup--li-em">etc.</em>)</li><li name="19b9" id="19b9" class="graf graf--li graf-after--li">When it arrives at answers that constrain the possible answers to the given question, it outputs an answer consistent with those constraints.</li></ul><p name="cf96" id="cf96" class="graf graf--p graf-after--li">We could attribute the statements produced by this process to <strong class="markup--strong markup--p-strong">C </strong>as beliefs.<strong class="markup--strong markup--p-strong"> </strong>We expect this set of beliefs to have useful information because it was selected for making correct predictions.</p><p name="11bf" id="11bf" class="graf graf--p graf-after--p">Fortunately, if HCH is able to see the training dataset then it may be able to arrive at similarly accurate views:</p><ul class="postList"><li name="18b2" id="18b2" class="graf graf--li graf-after--p">It can compute that P(observations | axioms) is high by following along with the deductive process used by <em class="markup--em markup--li-em">f&nbsp;.</em></li><li name="ae06" id="ae06" class="graf graf--li graf-after--li">Bayes implies that the axioms are likely to be true (fully capturing the argument which leads us, from our current perspective, to expect them to be true).</li><li name="d11d" id="d11d" class="graf graf--li graf-after--li">Once HCH knows the axioms are likely to be true, it can follow along with the deductive process to reproduce all of <em class="markup--em markup--li-em">f</em>â€™s beliefs.</li></ul><h4 name="d033" id="d033" class="graf graf--h4 graf-after--li">Modeling</h4><p name="39df" id="39df" class="graf graf--p graf-after--h4">Suppose <em class="markup--em markup--p-em">f</em> takes the following form:</p><ul class="postList"><li name="ff93" id="ff93" class="graf graf--li graf-after--p">Given a question Q and context, set up a relevant physical situation.</li><li name="dea7" id="dea7" class="graf graf--li graf-after--li">Use a dynamics model M to predict what will happen at some future time.</li><li name="6650" id="6650" class="graf graf--li graf-after--li">Use the resulting state to answer Q.</li></ul><p name="86af" id="86af" class="graf graf--p graf-after--li">We could say that <strong class="markup--strong markup--p-strong">C</strong> believes everything implied by the outcome of this physical simulation. We expect these beliefs to be accurate because <em class="markup--em markup--p-em">f</em> has predicted well in the past.</p><p name="04e1" id="04e1" class="graf graf--p graf-after--p">As in the deduction case, hopefully HCH can compute that P(observations|M is accurate) is high, since it can also compute the consequences of M. Then Bayes implies the dynamics model is accurate, and HCH can use that model to compute physical states.</p><p name="7f69" id="7f69" class="graf graf--p graf-after--p">Inferring all the beliefs from a dynamics model is not trivial though. As an extreme example, if <em class="markup--em markup--p-em">f </em>is performing an atom-by-atom simulation of a room, and that room contains Alice and Bob, then we could ascribe extensive beliefs about Alice and Bob to the computation <strong class="markup--strong markup--p-strong">C</strong>.</p><p name="2cd4" id="2cd4" class="graf graf--p graf-after--p">(Here we run head on into the fuzziness about what counts as a â€œreasonableâ€ ascription procedure, but for the moment Iâ€™ll assume that some reasonable procedure ascribes beliefs about Alice and Bob to the computation.)</p><p name="8002" id="8002" class="graf graf--p graf-after--p">To compete with these ascriptions, HCH needs to infer those high-level beliefs about Alice and Bob from the low-level computation involving atoms. One way to do this is to search over possible â€œbridgingâ€ hypotheses that relate low-level physical facts to high-level facts about the environment. If such a hypothesis can explain additional high-level facts, then a Bayesian can learn that it is true. Similarly, if the bridging hypothesis relates facts about the model to constraints we know from the high-level interpretation, then the Bayesian can potentially use that as evidence. (This kind of reasoning will be discussed in a bit more detail in the next section.)</p><p name="887f" id="887f" class="graf graf--p graf-after--p">We could further hope that searching for a bridging hypothesis isnâ€™t much harder than performing the original search over low-level physics, given that the low-level physics needed to explain a bunch of high-level facts and so already must encode some part of that correspondence.</p><p name="fda6" id="fda6" class="graf graf--p graf-after--p">(Note that the â€œdeductionâ€ example in the previous case could also involve alien concepts or models, in which case the same kind of work would be needed.)</p><h4 name="3adb" id="3adb" class="graf graf--h4 graf-after--p">Alien reasoning</h4><p name="d82c" id="d82c" class="graf graf--p graf-after--h4">In the previous section we described two styles of reasoning we already understand. But there are probably many kinds of reasoning that work well in practice but that would be more alien, and those might be more challenging. This section will explore one example in some detail to try to help anchor our reasoning about the general phenomenon. It will also elaborate on some of the reasoning about â€œbridgingâ€ hypotheses mentioned in the last section.</p><p name="70ed" id="70ed" class="graf graf--p graf-after--p">Suppose that our predictions are always of the same form (e.g. what is the probability the stock market will go up today), and <em class="markup--em markup--p-em">f</em> works as follows (the details are long but not very important):</p><ul class="postList"><li name="4823" id="4823" class="graf graf--li graf-after--p">Find the PSD matrix A with maximum log determinant subject to the constraints in the next bullet points, then output the (0, 0) entry.</li><li name="47cf" id="47cf" class="graf graf--li graf-after--li">There is an implicit correspondence between the rows/columns of A, and some uncertain properties X(0), X(1), X(2),&nbsp;â€¦. (which weâ€™ll view as 0â€“1 variables), where X(0) is the property we want to forecast.</li><li name="3e06" id="3e06" class="graf graf--li graf-after--li">If the (<em class="markup--em markup--li-em">i</em>,<em class="markup--em markup--li-em"> j</em>) entry of A represented the expectation E[X(<em class="markup--em markup--li-em">i</em>)X(<em class="markup--em markup--li-em">j</em>)], then the matrix would necessarily satisfy a bunch of constraints, which we impose A. For example:</li><li name="1c22" id="1c22" class="graf graf--li graf-after--li">If the context implies that X(<em class="markup--em markup--li-em">i</em>) = 1, then E[X(<em class="markup--em markup--li-em">i</em>)X(<em class="markup--em markup--li-em">j</em>)] = E[X(<em class="markup--em markup--li-em">j</em>)] = E[X(<em class="markup--em markup--li-em">j</em>)Â²], so A(<em class="markup--em markup--li-em">i</em>,<em class="markup--em markup--li-em"> j</em>) = A(<em class="markup--em markup--li-em">j</em>, <em class="markup--em markup--li-em">j</em>).</li><li name="9b02" id="9b02" class="graf graf--li graf-after--li">If X(<em class="markup--em markup--li-em">i</em>) and X(<em class="markup--em markup--li-em">j</em>) together imply X(<em class="markup--em markup--li-em">k</em>), then we must have E[X(<em class="markup--em markup--li-em">i</em>)X(<em class="markup--em markup--li-em">j</em>)] â‰¤ E[X(<em class="markup--em markup--li-em">i</em>)X(<em class="markup--em markup--li-em">k</em>)] and hence A(<em class="markup--em markup--li-em">i</em>, <em class="markup--em markup--li-em">j</em>) â‰¤ A(<em class="markup--em markup--li-em">i</em>, <em class="markup--em markup--li-em">k</em>).</li><li name="5f2e" id="5f2e" class="graf graf--li graf-after--li">For any constants <em class="markup--em markup--li-em">a</em>, <em class="markup--em markup--li-em">b</em>,&nbsp;â€¦, E[(<em class="markup--em markup--li-em">a</em> X(1) + <em class="markup--em markup--li-em">b</em> X(2) +&nbsp;â€¦ )Â²] â‰¥ 0â€Šâ€”â€Ši.e., the matrix A must be PSD.</li></ul><p name="3e8c" id="3e8c" class="graf graf--p graf-after--li">The chosen matrix A(opt) corresponds to a set of beliefs about the propositions X(<em class="markup--em markup--p-em">i</em>), and we can ascribe these beliefs to <strong class="markup--strong markup--p-strong">C</strong>. Because <em class="markup--em markup--p-em">f</em> predicts well, we again expect these beliefs to say something important about the world.</p><p name="38f1" id="38f1" class="graf graf--p graf-after--p">I chose this procedure <em class="markup--em markup--p-em">f </em>in part because we can give a kind of argument for why the matrix A(opt) should tend to encode accurate beliefs. But I donâ€™t think that a universal reasoner can make use of that argument:</p><ul class="postList"><li name="768e" id="768e" class="graf graf--li graf-after--p">Finding the argument that <em class="markup--em markup--li-em">f</em> works is an additional problem, beyond finding <em class="markup--em markup--li-em">f</em> itself, which might be much harder.</li><li name="9240" id="9240" class="graf graf--li graf-after--li">A comprehensible version of that argument may be much larger than the strategy itself, so even in the idealized cases like debate with perfect optimization, we may need to increase the scale.</li><li name="a083" id="a083" class="graf graf--li graf-after--li">I donâ€™t expect that all â€œgoodâ€ reasoning strategies have clean understandable arguments in their favor (and even in this case, if it the scheme worked well it would be largely an empirical fact rather than a consequence of the simple theorems we could prove). I think this kind of example is useful because we can easily imagine a human debate judge not having the argument while still being apparently universal. This makes it a useful analogy for cases where the argument really doesnâ€™t exist.</li></ul><p name="386b" id="386b" class="graf graf--p graf-after--li">Instead, I think a universal reasoner needs to be able to infer the efficacy of this reasoning procedure from its empirical success. Itâ€™s relatively easy for a Bayesian to learn the regularity â€œ<em class="markup--em markup--p-em">f</em> makes good predictions.â€ Recovering the rest of the matrix A, and learning how to interpret and whether to trust them, is the hard part.</p><p name="20cb" id="20cb" class="graf graf--p graf-after--p">This is going to require the same kind of bridging/identification we discussed in the last section. Letâ€™s write X(A) for the set of beliefs about the world implied by the â€œintendedâ€ identification. Searching over possible identifications to find X (or something like it) is the only way we can ever relate the rows of A to the quantities X(<em class="markup--em markup--p-em">i</em>). Again, we can hope that it isnâ€™t much harder than finding the original reasoning procedure.</p><p name="a9e4" id="a9e4" class="graf graf--p graf-after--p">I think that a sufficiently sophisticated Bayesian would probably be able to learn to trust X(A):</p><ul class="postList"><li name="d861" id="d861" class="graf graf--li graf-after--p">If <em class="markup--em markup--li-em">f</em> is performing well enough that we think itâ€™s more likely to be right in the future, then the Bayesian is going to end believing some claim like â€œthe predictions of <em class="markup--em markup--li-em">f</em> are goodâ€ (since it explains the data so well).</li><li name="d43a" id="d43a" class="graf graf--li graf-after--li">This is a complicated statement, and without some kind of explanation this claim has a low prior probability (roughly decaying with the complexity of <em class="markup--em markup--li-em">f)</em>. The Bayesian is motivated to find an explanation with higher prior probability.</li><li name="f4a1" id="f4a1" class="graf graf--li graf-after--li">The correspondence X can explain the constraints on the matrix A, in terms of facts that we already know about the world. This explanation may end up being simpler (or at least higher prior) than a direct enumeration of the constraints on Aâ€Šâ€”â€ŠI hope (and think itâ€™s plausible) that this happens iff weâ€™d actually believe on reflection that X(A) captures reality.<br>(To the extent that we are uncertain and think Aâ€™s beliefs have a non-negligible chance of capturing reality, then hopefully we can capture that by the same mechanism by ending up with a non-degenerate posterior.)</li><li name="1aa8" id="1aa8" class="graf graf--li graf-after--li">Now the Bayesian is faced with at least two kinds of explanations:<br>(a) â€œIf you use the constraints implied by correspondence X(A) + positive semidefiniteness, and then optimize log det, you get a matrix A for which X(A) makes good predictions,â€<br>(b) â€œThe actual situation in the real world is described by positive semi-definite matrices with higher log determinant (under the correspondence X).â€</li><li name="bc6a" id="bc6a" class="graf graf--li graf-after--li">Explanation (b) is explaining two things at once: both why the optimization done by <em class="markup--em markup--li-em">f</em> respects the constraints on our beliefs, and why that optimization leads to good predictions. Hopefully this is simpler than making two separate bridging claims, one which explains <em class="markup--em markup--li-em">f</em> as respecting the constraints implied by X, and one which claims that <em class="markup--em markup--li-em">f</em> makes good predictions. Ideally, this 2-for-1 that favors (b) exactly mirrors the underlying reasoning that leads us to actually believe that X(A) is correct, rather than resembling what we know about reality and making good predictions â€œby coincidence.â€</li></ul><p name="86e4" id="86e4" class="graf graf--p graf-after--li">This is a pretty speculative discussionâ€Šâ€”â€Šitâ€™s not very careful, and itâ€™s hard to make it careful in part because I donâ€™t have a formalization of Bayesian reasoning that can even really be applied to this setting. But it seems to match my intuitions about what reasonable Bayesian reasoning â€œshouldâ€ do, which gives me a lot more optimism that a careful Bayesian would be able to epistemically dominate <strong class="markup--strong markup--p-strong">C</strong>.</p><h4 name="cfe4" id="cfe4" class="graf graf--h4 graf-after--p">Deliberation and self-improvement</h4><p name="90c5" id="90c5" class="graf graf--p graf-after--h4">Often we expect the computation <strong class="markup--strong markup--p-strong">C</strong> to have accurate beliefs because it uses a strategy that appears to work in practiceâ€Šâ€”â€Šthe last 3 examples have discussed that case. But there are other reasons to trust a computation.</p><p name="6d74" id="6d74" class="graf graf--p graf-after--p">For example, humans often write code and trust it (to some extent) even without extensive empirical testingâ€Šâ€”â€Šinstead, we have a reason to think it will work, and need only modest testing to make sure that we havenâ€™t made an error in our implementation or reasoning. If I write an automated mathematician that works by finding proofs that pass a proof checker, I donâ€™t expect it to be correct because of the empirical record (Empirical data backs up some key assumptions, but isnâ€™t being used to directly establishing the correctness of the method.)</p><p name="cf51" id="cf51" class="graf graf--p graf-after--p">Likewise, if we train a powerful agent, that agent might initially use strategies that work well in training, but over time it might use learned reasoning to identify other promising strategies and use those. Reasoning might allow it to totally skip empirical testing, or to adopt the method after much less testing than would have been necessary without the reasoning.</p><p name="32b2" id="32b2" class="graf graf--p graf-after--p">To dominate the beliefs produced by such reasoning, we canâ€™t directly appeal to the kind of statistical inference made in the previous section. But in these cases I think we have access to an even more direct strategy.</p><p name="6a02" id="6a02" class="graf graf--p graf-after--p">Concretely, consider the situation where <strong class="markup--strong markup--p-strong">C</strong> contains a process <em class="markup--em markup--p-em">f </em>that designs a new reasoning process <em class="markup--em markup--p-em">g</em>. Then:</p><ul class="postList"><li name="3ec5" id="3ec5" class="graf graf--li graf-after--p">From the outside, we trust <em class="markup--em markup--li-em">g</em> because we trust <em class="markup--em markup--li-em">f </em>and it trusts <em class="markup--em markup--li-em">g.</em></li><li name="1b8f" id="1b8f" class="graf graf--li graf-after--li">An otherwise-universal reasoner <strong class="markup--strong markup--li-strong">A</strong> will dominate <em class="markup--em markup--li-em">f</em>â€™s beliefs, and in particular if <em class="markup--em markup--li-em">f</em> is justified in thinking that <em class="markup--em markup--li-em">g </em>will work then <strong class="markup--strong markup--li-strong">A</strong> will believe that and understand why.</li><li name="abb7" id="abb7" class="graf graf--li graf-after--li">Once we understand <em class="markup--em markup--li-em">fâ€™</em>s beliefs, dominating <em class="markup--em markup--li-em">g </em>is essentially another instance of the original ascription universality problem, but now from a slightly stronger epistemic state that involves both what ğ”¼ knows and what <em class="markup--em markup--li-em">f</em> knows. So unless our original approach to universality was tightly wedded to details of ğ”¼, we can probably dominate <em class="markup--em markup--li-em">g</em>.</li></ul><p name="6019" id="6019" class="graf graf--p graf-after--li graf--trailing">At the end of the day weâ€™d like to put all of this together into a tight argument for universality, which will need to incorporate both statistical arguments and this kind of dynamic. But Iâ€™m tentatively optimistic about achieving universality in light of the prospect of agents designing new agents, and am much more worried about the kind of opaque computations that â€œjust workâ€ described in the last few sections.</p></div></div></section></div><footer class="u-paddingTop10"><div class="container u-maxWidth740"><div class="row"><div class="col u-size12of12"></div></div><div class="row"><div class="col u-size12of12 js-postTags"><div class="u-paddingBottom10"><ul class="tags tags--postTags tags--borderless"><li><a class="link u-baseColor--link" href="https://ai-alignment.com/tagged/machine-learning?source=post" data-action-source="post" data-collection-slug="ai-control">Machine Learning</a></li></ul></div></div></div><div class="postActions js-postActionsFooter "><div class="u-flexCenter"><div class="u-flex1"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="409ab893a456" data-is-icon-29px="true" data-is-circle="true" data-has-recommend-list="true" data-source="post_actions_footer-----409ab893a456---------------------clap_footer" data-clap-string-singular="clap" data-clap-string-plural="claps"><div class="u-relative u-foreground"><button class="button button--large button--circle button--withChrome u-baseColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--darker clapButton--largePill u-relative u-foreground u-xs-paddingLeft13 u-width60 u-height60 u-accentColor--textNormal u-accentColor--buttonNormal clap-onboardingcollection" data-action="sign-up-prompt" data-sign-in-action="multivote" data-requires-token="true" data-redirect="https://medium.com/_/vote/p/409ab893a456" data-action-source="post_actions_footer-----409ab893a456---------------------clap_footer" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--33px u-relative u-topNegative2 u-xs-top0"><svg class="svgIcon-use" width="33" height="33"><path d="M28.86 17.342l-3.64-6.402c-.292-.433-.712-.729-1.163-.8a1.124 1.124 0 0 0-.889.213c-.63.488-.742 1.181-.33 2.061l1.222 2.587 1.4 2.46c2.234 4.085 1.511 8.007-2.145 11.663-.26.26-.526.49-.797.707 1.42-.084 2.881-.683 4.292-2.094 3.822-3.823 3.565-7.876 2.05-10.395zm-6.252 11.075c3.352-3.35 3.998-6.775 1.978-10.469l-3.378-5.945c-.292-.432-.712-.728-1.163-.8a1.122 1.122 0 0 0-.89.213c-.63.49-.742 1.182-.33 2.061l1.72 3.638a.502.502 0 0 1-.806.568l-8.91-8.91a1.335 1.335 0 0 0-1.887 1.886l5.292 5.292a.5.5 0 0 1-.707.707l-5.292-5.292-1.492-1.492c-.503-.503-1.382-.505-1.887 0a1.337 1.337 0 0 0 0 1.886l1.493 1.492 5.292 5.292a.499.499 0 0 1-.353.854.5.5 0 0 1-.354-.147L5.642 13.96a1.338 1.338 0 0 0-1.887 0 1.338 1.338 0 0 0 0 1.887l2.23 2.228 3.322 3.324a.499.499 0 0 1-.353.853.502.502 0 0 1-.354-.146l-3.323-3.324a1.333 1.333 0 0 0-1.886 0 1.325 1.325 0 0 0-.39.943c0 .356.138.691.39.943l6.396 6.397c3.528 3.53 8.86 5.313 12.821 1.353zM12.73 9.26l5.68 5.68-.49-1.037c-.518-1.107-.426-2.13.224-2.89l-3.303-3.304a1.337 1.337 0 0 0-1.886 0 1.326 1.326 0 0 0-.39.944c0 .217.067.42.165.607zm14.787 19.184c-1.599 1.6-3.417 2.392-5.353 2.392-.349 0-.7-.03-1.058-.082a7.922 7.922 0 0 1-3.667.887c-3.049 0-6.115-1.626-8.359-3.87l-6.396-6.397A2.315 2.315 0 0 1 2 19.724a2.327 2.327 0 0 1 1.923-2.296l-.875-.875a2.339 2.339 0 0 1 0-3.3 2.33 2.33 0 0 1 1.24-.647l-.139-.139c-.91-.91-.91-2.39 0-3.3.884-.884 2.421-.882 3.301 0l.138.14a2.335 2.335 0 0 1 3.948-1.24l.093.092c.091-.423.291-.828.62-1.157a2.336 2.336 0 0 1 3.3 0l3.384 3.386a2.167 2.167 0 0 1 1.271-.173c.534.086 1.03.354 1.441.765.11-.549.415-1.034.911-1.418a2.12 2.12 0 0 1 1.661-.41c.727.117 1.385.565 1.853 1.262l3.652 6.423c1.704 2.832 2.025 7.377-2.205 11.607zM13.217.484l-1.917.882 2.37 2.837-.454-3.719zm8.487.877l-1.928-.86-.44 3.697 2.368-2.837zM16.5 3.293L15.478-.005h2.044L16.5 3.293z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--33px u-relative u-topNegative2 u-xs-top0"><svg class="svgIcon-use" width="33" height="33"><g fill-rule="evenodd"><path d="M29.58 17.1l-3.854-6.78c-.365-.543-.876-.899-1.431-.989a1.491 1.491 0 0 0-1.16.281c-.42.327-.65.736-.7 1.207v.001l3.623 6.367c2.46 4.498 1.67 8.802-2.333 12.807-.265.265-.536.505-.81.728 1.973-.222 3.474-1.286 4.45-2.263 4.166-4.165 3.875-8.6 2.215-11.36zm-4.831.82l-3.581-6.3c-.296-.439-.725-.742-1.183-.815a1.105 1.105 0 0 0-.89.213c-.647.502-.755 1.188-.33 2.098l1.825 3.858a.601.601 0 0 1-.197.747.596.596 0 0 1-.77-.067L10.178 8.21c-.508-.506-1.393-.506-1.901 0a1.335 1.335 0 0 0-.393.95c0 .36.139.698.393.95v.001l5.61 5.61a.599.599 0 1 1-.848.847l-5.606-5.606c-.001 0-.002 0-.003-.002L5.848 9.375a1.349 1.349 0 0 0-1.902 0 1.348 1.348 0 0 0 0 1.901l1.582 1.582 5.61 5.61a.6.6 0 0 1-.848.848l-5.61-5.61c-.51-.508-1.393-.508-1.9 0a1.332 1.332 0 0 0-.394.95c0 .36.139.697.393.952l2.363 2.362c.002.001.002.002.002.003l3.52 3.52a.6.6 0 0 1-.848.847l-3.522-3.523h-.001a1.336 1.336 0 0 0-.95-.393 1.345 1.345 0 0 0-.949 2.295l6.779 6.78c3.715 3.713 9.327 5.598 13.49 1.434 3.527-3.528 4.21-7.13 2.086-11.015zM11.817 7.727c.06-.328.213-.64.466-.893.64-.64 1.755-.64 2.396 0l3.232 3.232c-.82.783-1.09 1.833-.764 2.992l-5.33-5.33z"></path><path d="M13.285.48l-1.916.881 2.37 2.837z"></path><path d="M21.719 1.361L19.79.501l-.44 3.697z"></path><path d="M16.502 3.298L15.481 0h2.043z"></path></g></svg></span></span></button><div class="clapUndo u-width60 u-round u-height32 u-absolute u-borderBox u-paddingRight5 u-transition--transform200Springu-backgroundGrayLighter js-clapUndo" style="top: 14px; padding: 2px;"><button class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon u-floatRight" data-action="multivote-undo" data-action-value="409ab893a456"><span class="svgIcon svgIcon--removeThin svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M20.13 8.11l-5.61 5.61-5.609-5.61-.801.801 5.61 5.61-5.61 5.61.801.8 5.61-5.609 5.61 5.61.8-.801-5.609-5.61 5.61-5.61" fill-rule="evenodd"></path></svg></span></button></div></div><span class="u-relative u-background js-actionMultirecommendCount u-marginLeft16"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-textColorDarker" data-action="show-recommends" data-action-value="409ab893a456">60 claps</button><span class="u-xs-hide"></span></span></div></div><div class="buttonSet u-flex0"><a class="button button--dark button--chromeless u-baseColor--buttonDark button--withIcon button--withSvgIcon button--dark button--chromeless u-xs-hide u-marginRight12" href="https://medium.com/p/409ab893a456/share/twitter" title="Share on Twitter" aria-label="Share on Twitter" target="_blank" data-action-source="post_actions_footer"><span class="button-defaultState"><span class="svgIcon svgIcon--twitterFilled svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M22.053 7.54a4.474 4.474 0 0 0-3.31-1.455 4.526 4.526 0 0 0-4.526 4.524c0 .35.04.7.082 1.05a12.9 12.9 0 0 1-9.3-4.77c-.39.69-.61 1.46-.65 2.26.03 1.6.83 2.99 2.02 3.79-.72-.02-1.41-.22-2.02-.57-.01.02-.01.04 0 .08-.01 2.17 1.55 4 3.63 4.44-.39.08-.79.13-1.21.16-.28-.03-.57-.05-.81-.08.54 1.77 2.21 3.08 4.2 3.15a9.564 9.564 0 0 1-5.66 1.94c-.34-.03-.7-.06-1.05-.08 2 1.27 4.38 2.02 6.94 2.02 8.31 0 12.86-6.9 12.84-12.85.02-.24.01-.43 0-.65.89-.62 1.65-1.42 2.26-2.34-.82.38-1.69.62-2.59.72a4.37 4.37 0 0 0 1.94-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></span></span></a><a class="button button--dark button--chromeless u-baseColor--buttonDark button--withIcon button--withSvgIcon button--dark button--chromeless u-xs-hide u-marginRight12" href="https://medium.com/p/409ab893a456/share/facebook" title="Share on Facebook" aria-label="Share on Facebook" target="_blank" data-action-source="post_actions_footer"><span class="button-defaultState"><span class="svgIcon svgIcon--facebookSquare svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M23.209 5H5.792A.792.792 0 0 0 5 5.791V23.21c0 .437.354.791.792.791h9.303v-7.125H12.72v-2.968h2.375v-2.375c0-2.455 1.553-3.662 3.741-3.662 1.049 0 1.95.078 2.213.112v2.565h-1.517c-1.192 0-1.469.567-1.469 1.397v1.963h2.969l-.594 2.968h-2.375L18.11 24h5.099a.791.791 0 0 0 .791-.791V5.79a.791.791 0 0 0-.791-.79"></path></svg></span></span></a><button class="button button--large button--dark button--chromeless u-baseColor--buttonDark button--withIcon button--withSvgIcon u-xs-show u-marginRight10" title="Share this story on Twitter or Facebook" aria-label="Share this story on Twitter or Facebook" data-action="show-share-popover" data-action-source="post_actions_footer"><span class="svgIcon svgIcon--share svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M20.385 8H19a.5.5 0 1 0 .011 1h1.39c.43 0 .84.168 1.14.473.31.305.48.71.48 1.142v10.77c0 .43-.17.837-.47 1.142-.3.305-.71.473-1.14.473H8.62c-.43 0-.84-.168-1.144-.473a1.603 1.603 0 0 1-.473-1.142v-10.77c0-.43.17-.837.48-1.142A1.599 1.599 0 0 1 8.62 9H10a.502.502 0 0 0 0-1H8.615c-.67 0-1.338.255-1.85.766-.51.51-.765 1.18-.765 1.85v10.77c0 .668.255 1.337.766 1.848.51.51 1.18.766 1.85.766h11.77c.668 0 1.337-.255 1.848-.766.51-.51.766-1.18.766-1.85v-10.77c0-.668-.255-1.337-.766-1.848A2.61 2.61 0 0 0 20.384 8zm-8.67-2.508L14 3.207v8.362c0 .27.224.5.5.5s.5-.23.5-.5V3.2l2.285 2.285a.49.49 0 0 0 .704-.001.511.511 0 0 0 0-.708l-3.14-3.14a.504.504 0 0 0-.71 0L11 4.776a.501.501 0 0 0 .71.706" fill-rule="evenodd"></path></svg></span></button><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon" data-action="scroll-to-responses" data-action-source="post_actions_footer"><span class="svgIcon svgIcon--response svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M21.27 20.058c1.89-1.826 2.754-4.17 2.754-6.674C24.024 8.21 19.67 4 14.1 4 8.53 4 4 8.21 4 13.384c0 5.175 4.53 9.385 10.1 9.385 1.007 0 2-.14 2.95-.41.285.25.592.49.918.7 1.306.87 2.716 1.31 4.19 1.31.276-.01.494-.14.6-.36a.625.625 0 0 0-.052-.65c-.61-.84-1.042-1.71-1.282-2.58a5.417 5.417 0 0 1-.154-.75zm-3.85 1.324l-.083-.28-.388.12a9.72 9.72 0 0 1-2.85.424c-4.96 0-8.99-3.706-8.99-8.262 0-4.556 4.03-8.263 8.99-8.263 4.95 0 8.77 3.71 8.77 8.27 0 2.25-.75 4.35-2.5 5.92l-.24.21v.32c0 .07 0 .19.02.37.03.29.1.6.19.92.19.7.49 1.4.89 2.08-.93-.14-1.83-.49-2.67-1.06-.34-.22-.88-.48-1.16-.74z"></path></svg></span></button><button class="button button--large button--dark button--chromeless is-touchIconFadeInPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="sign-up-prompt" data-sign-in-action="add-to-bookmarks" data-requires-token="true" data-redirect="https://medium.com/_/bookmark/p/409ab893a456" data-action-source="post_actions_footer"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--29px u-marginRight4"><svg class="svgIcon-use" width="29" height="29"><path d="M19.385 4h-9.77A2.623 2.623 0 0 0 7 6.615V23.01a1.022 1.022 0 0 0 1.595.847l5.905-4.004 5.905 4.004A1.022 1.022 0 0 0 22 23.011V6.62A2.625 2.625 0 0 0 19.385 4zM21 23l-5.91-3.955-.148-.107a.751.751 0 0 0-.884 0l-.147.107L8 23V6.615C8 5.725 8.725 5 9.615 5h9.77C20.275 5 21 5.725 21 6.615V23z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--29px u-marginRight4"><svg class="svgIcon-use" width="29" height="29"><path d="M19.385 4h-9.77A2.623 2.623 0 0 0 7 6.615V23.01a1.022 1.022 0 0 0 1.595.847l5.905-4.004 5.905 4.004A1.022 1.022 0 0 0 22 23.011V6.62A2.625 2.625 0 0 0 19.385 4z" fill-rule="evenodd"></path></svg></span></span></button><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon js-moreActionsButton" title="More actions" aria-label="More actions" data-action="more-actions"><span class="svgIcon svgIcon--more svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="-480.5 272.5 21 21"><path d="M-463 284.6c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5z"></path></svg></span></button></div></div></div></div><div class="u-maxWidth740 u-paddingTop20 u-marginTop20 u-borderTopLightest container u-paddingBottom20 u-xs-paddingBottom10 js-postAttributionFooterContainer"><div class="row js-postFooterInfo"><div class="col u-size6of12 u-xs-size12of12"><li class="uiScale uiScale-ui--small uiScale-caption--regular u-block u-paddingBottom18 js-cardUser"><div class="u-marginLeft20 u-floatRight"><span class="followState js-followState" data-user-id="57f1a655a613"><button class="button button--small u-noUserSelect button--withChrome u-baseColor--buttonNormal button--withHover button--unblock js-unblockButton" data-action="sign-up-prompt" data-sign-in-action="toggle-block-user" data-requires-token="true" data-redirect="https://ai-alignment.com/towards-formalizing-universality-409ab893a456" data-action-source="footer_card"><span class="button-label  button-defaultState">Blocked</span><span class="button-label button-hoverState">Unblock</span></button><button class="button button--primary button--small u-noUserSelect button--withChrome u-accentColor--buttonNormal button--follow js-followButton" data-action="sign-up-prompt" data-sign-in-action="toggle-subscribe-user" data-requires-token="true" data-redirect="https://medium.com/_/subscribe/user/57f1a655a613" data-action-source="footer_card-57f1a655a613-------------------------follow_footer"><span class="button-label  button-defaultState js-buttonLabel">Follow</span><span class="button-label button-activeState">Following</span></button></span></div><div class="u-tableCell"><a class="link u-baseColor--link avatar" href="https://ai-alignment.com/@paulfchristiano?source=footer_card" title="Go to the profile of Paul Christiano" aria-label="Go to the profile of Paul Christiano" data-action-source="footer_card" data-user-id="57f1a655a613" data-collection-slug="ai-control" dir="auto"><img src="https://cdn-images-1.medium.com/fit/c/60/60/1*BNjZCuQuRfIgcXCBMipuBw.jpeg" class="avatar-image avatar-image--small" alt="Go to the profile of Paul Christiano"></a></div><div class="u-tableCell u-verticalAlignMiddle u-breakWord u-paddingLeft15"><h3 class="ui-h3 u-fontSize18 u-lineHeightTighter u-marginBottom4"><a class="link link--primary u-accentColor--hoverTextNormal" href="https://ai-alignment.com/@paulfchristiano" property="cc:attributionName" title="Go to the profile of Paul Christiano" aria-label="Go to the profile of Paul Christiano" rel="author cc:attributionUrl" data-user-id="57f1a655a613" data-collection-slug="ai-control" dir="auto">Paul Christiano</a></h3><p class="ui-body u-fontSize14 u-lineHeightBaseSans u-textColorDark u-marginBottom4">OpenAI</p></div></li></div><div class="col u-size6of12 u-xs-size12of12 u-xs-marginTop30"><li class="uiScale uiScale-ui--small uiScale-caption--regular u-block u-paddingBottom18 js-cardCollection"><div class="u-marginLeft20 u-floatRight"><button class="button button--primary button--small u-noUserSelect button--withChrome u-accentColor--buttonNormal js-relationshipButton" data-action="sign-up-prompt" data-sign-in-action="toggle-follow-collection" data-requires-token="true" data-redirect="https://medium.com/_/subscribe/collection/ai-control" data-action-source="footer_card----624d886c4aa4----------------------follow_footer" data-collection-id="624d886c4aa4"><span class="button-label  js-buttonLabel">Follow</span></button></div><div class="u-tableCell "><a class="link u-baseColor--link avatar avatar--roundedRectangle" href="https://ai-alignment.com?source=footer_card" title="Go to AI Alignment" aria-label="Go to AI Alignment" data-action-source="footer_card" data-collection-slug="ai-control"><img src="https://cdn-images-1.medium.com/fit/c/60/60/1*N56Qc5-aHTcfGff0scntKQ.png" class="avatar-image u-size60x60" alt="AI Alignment"></a></div><div class="u-tableCell u-verticalAlignMiddle u-breakWord u-paddingLeft15"><h3 class="ui-h3 u-fontSize18 u-lineHeightTighter u-marginBottom4"><a class="link link--primary u-accentColor--hoverTextNormal" href="https://ai-alignment.com?source=footer_card" rel="collection" data-action-source="footer_card" data-collection-slug="ai-control">AI Alignment</a></h3><p class="ui-body u-fontSize14 u-lineHeightBaseSans u-textColorDark u-marginBottom4">Aligning AI systems with human interests.</p><div class="buttonSet"></div></div></li></div></div></div><div class="js-postFooterPlacements" data-post-id="409ab893a456" data-collection-id="624d886c4aa4" data-scroll="native"><div class="streamItem streamItem--placementCardGrid js-streamItem"><div class="u-clearfix u-backgroundGrayLightest"><div class="row u-marginAuto u-maxWidth1032 u-paddingTop30 u-paddingBottom40"><div class="col u-padding8 u-xs-size12of12 u-size4of12"><div class="uiScale uiScale-ui--small uiScale-caption--regular u-height280 u-width100pct u-backgroundWhite u-borderCardBorder u-boxShadow u-borderBox u-borderRadius4 js-trackPostPresentation" data-post-id="c0bee00365bd" data-source="placement_card_footer_grid---------0-41" data-tracking-context="placement"><div class="u-padding15 u-borderBox u-flexColumn u-sizeFull"><a class="link link--noUnderline u-baseColor--link u-flex1 u-flexColumn" href="https://ai-alignment.com/universality-and-consequentialism-within-hch-c0bee00365bd?source=placement_card_footer_grid---------0-41" data-action-source="placement_card_footer_grid---------0-41"><div class="uiScale uiScale-ui--regular uiScale-caption--small u-textColorNormal u-marginBottom7">More from AI Alignment</div><div class="ui-h3 ui-clamp2 u-textColorDarkest u-contentSansBold u-fontSize24 u-maxHeight2LineHeightTighter u-lineClamp2 u-textOverflowEllipsis u-letterSpacingTight u-paddingBottom2">Universality and consequentialism within HCH</div><div class="ui-body ui-clamp2 u-lineClamp2 u-textOverflowEllipsis u-maxHeight2LineHeightTighter">One exotic reason HCH can fail to be universal is the emergence of malicious patterns of behavior; universality may help address this risk.</div></a><div class="u-paddingBottom10 u-flex0 u-flexCenter"><div class="u-flex1 u-minWidth0 u-marginRight10"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://ai-alignment.com/@paulfchristiano" data-action="show-user-card" data-action-value="57f1a655a613" data-action-type="hover" data-user-id="57f1a655a613" data-collection-slug="ai-control" dir="auto"><img src="https://cdn-images-1.medium.com/fit/c/36/36/1*BNjZCuQuRfIgcXCBMipuBw.jpeg" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Paul Christiano"></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--darker" href="https://ai-alignment.com/@paulfchristiano?source=placement_card_footer_grid---------0-41" data-action="show-user-card" data-action-source="placement_card_footer_grid---------0-41" data-action-value="57f1a655a613" data-action-type="hover" data-user-id="57f1a655a613" data-collection-slug="ai-control" dir="auto">Paul Christiano</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><a class="link link--darken" href="https://ai-alignment.com/universality-and-consequentialism-within-hch-c0bee00365bd?source=placement_card_footer_grid---------0-41" data-action="open-post" data-action-value="https://ai-alignment.com/universality-and-consequentialism-within-hch-c0bee00365bd?source=placement_card_footer_grid---------0-41" data-action-source="preview-listing"><time datetime="2019-01-10T03:50:05.776Z">Jan 9</time></a><span class="middotDivider u-fontSize12"></span><span class="readingTime" title="9 min read"></span></div></div></div></div><div class="u-flex0 u-flexCenter"><div class="buttonSet"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="c0bee00365bd" data-is-label-padded="true" data-source="placement_card_footer_grid-----c0bee00365bd----0-41----------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--darker" data-action="sign-up-prompt" data-sign-in-action="multivote" data-requires-token="true" data-redirect="https://medium.com/_/vote/p/c0bee00365bd" data-action-source="placement_card_footer_grid-----c0bee00365bd----0-41----------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-relative u-background js-actionMultirecommendCount u-marginLeft5"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents u-marginLeft4" data-action="show-recommends" data-action-value="c0bee00365bd">18</button></span></div></div><div class="u-height20 u-borderRightLighter u-inlineBlock u-relative u-marginRight10 u-marginLeft12"></div><div class="buttonSet"><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="sign-up-prompt" data-sign-in-action="add-to-bookmarks" data-requires-token="true" data-redirect="https://medium.com/_/bookmark/p/c0bee00365bd" data-action-source="placement_card_footer_grid-----c0bee00365bd----0-41----------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button></div></div></div></div></div></div><div class="col u-padding8 u-xs-size12of12 u-size4of12"><div class="uiScale uiScale-ui--small uiScale-caption--regular u-height280 u-width100pct u-backgroundWhite u-borderCardBorder u-boxShadow u-borderBox u-borderRadius4 js-trackPostPresentation" data-post-id="6b949ef33a4f" data-source="placement_card_footer_grid---------1-60" data-tracking-context="placement"><a class="link link--noUnderline u-baseColor--link" href="https://medium.com/@ThingMaker/focusing-for-skeptics-6b949ef33a4f?source=placement_card_footer_grid---------1-60" data-action-source="placement_card_footer_grid---------1-60"><div class="u-backgroundCover u-backgroundColorGrayLight u-height100 u-width100pct u-borderBottomLight u-borderRadiusTop4" style="background-image: url(&quot;https://cdn-images-1.medium.com/fit/c/400/120/1*ueBd2qH1b_0rsR0-4UOiwQ.png&quot;); background-position: 50% 50% !important;"></div></a><div class="u-padding15 u-borderBox u-flexColumn u-height180"><a class="link link--noUnderline u-baseColor--link u-flex1" href="https://medium.com/@ThingMaker/focusing-for-skeptics-6b949ef33a4f?source=placement_card_footer_grid---------1-60" data-action-source="placement_card_footer_grid---------1-60"><div class="uiScale uiScale-ui--regular uiScale-caption--small u-textColorNormal u-marginBottom7">Related reads</div><div class="ui-h3 ui-clamp2 u-textColorDarkest u-contentSansBold u-fontSize24 u-maxHeight2LineHeightTighter u-lineClamp2 u-textOverflowEllipsis u-letterSpacingTight u-paddingBottom2">â€œFocusingâ€ for skeptics</div></a><div class="u-paddingBottom10 u-flex0 u-flexCenter"><div class="u-flex1 u-minWidth0 u-marginRight10"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://medium.com/@ThingMaker" data-action="show-user-card" data-action-value="afd4fe84e81b" data-action-type="hover" data-user-id="afd4fe84e81b" dir="auto"><img src="https://cdn-images-1.medium.com/fit/c/36/36/1*kTrnixDRh5FyyFArtFyyUg.png" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Duncan A Sabien"></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--darker" href="https://medium.com/@ThingMaker?source=placement_card_footer_grid---------1-60" data-action="show-user-card" data-action-source="placement_card_footer_grid---------1-60" data-action-value="afd4fe84e81b" data-action-type="hover" data-user-id="afd4fe84e81b" dir="auto">Duncan A Sabien</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><a class="link link--darken" href="https://medium.com/@ThingMaker/focusing-for-skeptics-6b949ef33a4f?source=placement_card_footer_grid---------1-60" data-action="open-post" data-action-value="https://medium.com/@ThingMaker/focusing-for-skeptics-6b949ef33a4f?source=placement_card_footer_grid---------1-60" data-action-source="preview-listing"><time datetime="2018-05-04T23:41:53.167Z">May 4, 2018</time></a><span class="middotDivider u-fontSize12"></span><span class="readingTime" title="9 min read"></span></div></div></div></div><div class="u-flex0 u-flexCenter"><div class="buttonSet"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="6b949ef33a4f" data-is-label-padded="true" data-source="placement_card_footer_grid-----6b949ef33a4f----1-60----------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--darker" data-action="sign-up-prompt" data-sign-in-action="multivote" data-requires-token="true" data-redirect="https://medium.com/_/vote/p/6b949ef33a4f" data-action-source="placement_card_footer_grid-----6b949ef33a4f----1-60----------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-relative u-background js-actionMultirecommendCount u-marginLeft5"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents u-marginLeft4" data-action="show-recommends" data-action-value="6b949ef33a4f">368</button></span></div></div><div class="u-height20 u-borderRightLighter u-inlineBlock u-relative u-marginRight10 u-marginLeft12"></div><div class="buttonSet"><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="sign-up-prompt" data-sign-in-action="add-to-bookmarks" data-requires-token="true" data-redirect="https://medium.com/_/bookmark/p/6b949ef33a4f" data-action-source="placement_card_footer_grid-----6b949ef33a4f----1-60----------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button></div></div></div></div></div></div><div class="col u-padding8 u-xs-size12of12 u-size4of12"><div class="uiScale uiScale-ui--small uiScale-caption--regular u-height280 u-width100pct u-backgroundWhite u-borderCardBorder u-boxShadow u-borderBox u-borderRadius4 js-trackPostPresentation" data-post-id="e09048b9856d" data-source="placement_card_footer_grid---------2-60" data-tracking-context="placement"><a class="link link--noUnderline u-baseColor--link" href="https://medium.com/mit-technology-review/why-humans-learn-faster-than-ai-for-now-e09048b9856d?source=placement_card_footer_grid---------2-60" data-action-source="placement_card_footer_grid---------2-60"><div class="u-backgroundCover u-backgroundColorGrayLight u-height100 u-width100pct u-borderBottomLight u-borderRadiusTop4" style="background-image: url(&quot;https://cdn-images-1.medium.com/fit/c/400/120/1*HNoYNrb2O5UNy8NclofQKw.jpeg&quot;); background-position: 50% 50% !important;"></div></a><div class="u-padding15 u-borderBox u-flexColumn u-height180"><a class="link link--noUnderline u-baseColor--link u-flex1" href="https://medium.com/mit-technology-review/why-humans-learn-faster-than-ai-for-now-e09048b9856d?source=placement_card_footer_grid---------2-60" data-action-source="placement_card_footer_grid---------2-60"><div class="uiScale uiScale-ui--regular uiScale-caption--small u-textColorNormal u-marginBottom7"><div class="u-floatRight u-textColorNormal"><span class="svgIcon svgIcon--star svgIcon--15px"><svg class="svgIcon-use" width="15" height="15"><path d="M7.438 2.324c.034-.099.09-.099.123 0l1.2 3.53a.29.29 0 0 0 .26.19h3.884c.11 0 .127.049.038.111L9.8 8.327a.271.271 0 0 0-.099.291l1.2 3.53c.034.1-.011.131-.098.069l-3.142-2.18a.303.303 0 0 0-.32 0l-3.145 2.182c-.087.06-.132.03-.099-.068l1.2-3.53a.271.271 0 0 0-.098-.292L2.056 6.146c-.087-.06-.071-.112.038-.112h3.884a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></div><div class="u-noWrapWithEllipsis u-marginRight40">Related reads</div></div><div class="ui-h3 ui-clamp2 u-textColorDarkest u-contentSansBold u-fontSize24 u-maxHeight2LineHeightTighter u-lineClamp2 u-textOverflowEllipsis u-letterSpacingTight u-paddingBottom2">Why Humans Learn Faster Than AI â€” for Now</div></a><div class="u-paddingBottom10 u-flex0 u-flexCenter"><div class="u-flex1 u-minWidth0 u-marginRight10"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://medium.com/@MITTechReview" data-action="show-user-card" data-action-value="defe73a9b0ba" data-action-type="hover" data-user-id="defe73a9b0ba" dir="auto"><img src="https://cdn-images-1.medium.com/fit/c/36/36/1*fRTAqrjmS6BLG1L49aq-dg.jpeg" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of MIT Technology Review"></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--darker" href="https://medium.com/@MITTechReview?source=placement_card_footer_grid---------2-60" data-action="show-user-card" data-action-source="placement_card_footer_grid---------2-60" data-action-value="defe73a9b0ba" data-action-type="hover" data-user-id="defe73a9b0ba" dir="auto">MIT Technology Review</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><a class="link link--darken" href="https://medium.com/mit-technology-review/why-humans-learn-faster-than-ai-for-now-e09048b9856d?source=placement_card_footer_grid---------2-60" data-action="open-post" data-action-value="https://medium.com/mit-technology-review/why-humans-learn-faster-than-ai-for-now-e09048b9856d?source=placement_card_footer_grid---------2-60" data-action-source="preview-listing"><time datetime="2018-03-08T19:01:33.407Z">Mar 8, 2018</time></a><span class="middotDivider u-fontSize12"></span><span class="readingTime" title="5 min read"></span><span class="u-paddingLeft4"><span class="svgIcon svgIcon--star svgIcon--15px"><svg class="svgIcon-use" width="15" height="15"><path d="M7.438 2.324c.034-.099.09-.099.123 0l1.2 3.53a.29.29 0 0 0 .26.19h3.884c.11 0 .127.049.038.111L9.8 8.327a.271.271 0 0 0-.099.291l1.2 3.53c.034.1-.011.131-.098.069l-3.142-2.18a.303.303 0 0 0-.32 0l-3.145 2.182c-.087.06-.132.03-.099-.068l1.2-3.53a.271.271 0 0 0-.098-.292L2.056 6.146c-.087-.06-.071-.112.038-.112h3.884a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></span></div></div></div></div><div class="u-flex0 u-flexCenter"><div class="buttonSet"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="e09048b9856d" data-is-label-padded="true" data-source="placement_card_footer_grid-----e09048b9856d----2-60----------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--darker" data-action="sign-up-prompt" data-sign-in-action="multivote" data-requires-token="true" data-redirect="https://medium.com/_/vote/p/e09048b9856d" data-action-source="placement_card_footer_grid-----e09048b9856d----2-60----------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-relative u-background js-actionMultirecommendCount u-marginLeft5"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents u-marginLeft4" data-action="show-recommends" data-action-value="e09048b9856d">1K</button></span></div></div><div class="u-height20 u-borderRightLighter u-inlineBlock u-relative u-marginRight10 u-marginLeft12"></div><div class="buttonSet"><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="sign-up-prompt" data-sign-in-action="add-to-bookmarks" data-requires-token="true" data-redirect="https://medium.com/_/bookmark/p/e09048b9856d" data-action-source="placement_card_footer_grid-----e09048b9856d----2-60----------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button></div></div></div></div></div></div></div></div></div></div><div class="u-padding0 u-clearfix u-backgroundGrayLightest u-print-hide supplementalPostContent js-responsesWrapper" data-action-scope="_actionscope_5"><div class="container u-maxWidth740"><div class="responsesStreamWrapper u-maxWidth640 u-hide js-responsesStreamWrapper"><div class="container responsesStream-title u-paddingTop15"><div class="row"><header class="heading"><div class="u-clearfix"><div class="heading-content u-floatLeft"><span class="heading-title heading-title--semibold">Responses</span></div></div></header></div></div><div class="responsesStream js-responsesStream"></div><div class="container u-hide js-showOtherResponses"><div class="row"><button class="button button--primary button--withChrome u-accentColor--buttonNormal responsesStream-showOtherResponses cardChromeless u-width100pct u-marginVertical20 u-heightAuto" data-action="show-other-responses">Show all responses</button></div></div><div class="responsesStream js-responsesStreamOther"></div></div></div></div><div class="supplementalPostContent js-heroPromo"></div></footer></article></main><aside class="u-marginAuto u-maxWidth1032 js-postLeftSidebar"><div class="u-foreground u-top0 u-transition--fadeOut300 u-fixed u-sm-hide js-postShareWidget" data-scroll="fixed" style="transform: translateY(150px);"><div class="u-breakWord u-md-hide u-width131"><div class="u-width131 collection-title u-fontWeightBold u-fontSize18 u-lineHeightTight"><a href="https://ai-alignment.com?source=logo-lo_JuU2p2EvGepj">AI Alignment</a></div><div class="u-width131 u-multiline-clamp u-textColorNormal u-fontSize14 u-lineHeightTight u-paddingTop3">Aligning AI systems with human interests.</div><div class="u-paddingTop15 u-paddingBottom30 u-borderBottomLight u-marginBottom30"><button class="button button--primary button--small u-noUserSelect button--withChrome u-accentColor--buttonNormal js-relationshipButton" data-action="sign-up-prompt" data-sign-in-action="toggle-follow-collection" data-requires-token="true" data-redirect="https://medium.com/_/subscribe/collection/ai-control" data-action-source="post_sidebar----624d886c4aa4----------------------post_sidebar" data-collection-id="624d886c4aa4"><span class="button-label  js-buttonLabel">Follow</span></button></div></div><ul><li class="u-marginVertical10"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="409ab893a456" data-is-icon-29px="true" data-has-recommend-list="true" data-source="post_share_widget-----409ab893a456---------------------clap_sidebar"><div class="u-relative u-foreground"><button class="button button--primary button--large button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--darker" data-action="sign-up-prompt" data-sign-in-action="multivote" data-requires-token="true" data-redirect="https://medium.com/_/vote/p/409ab893a456" data-action-source="post_share_widget-----409ab893a456---------------------clap_sidebar" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><g fill-rule="evenodd"><path d="M13.739 1l.761 2.966L15.261 1z"></path><path d="M16.815 4.776l1.84-2.551-1.43-.471z"></path><path d="M10.378 2.224l1.84 2.551-.408-3.022z"></path><path d="M22.382 22.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L6.11 15.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L8.43 9.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L20.628 15c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM12.99 6.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><g fill-rule="evenodd"><path d="M13.738 1l.762 2.966L15.262 1z"></path><path d="M18.634 2.224l-1.432-.47-.408 3.022z"></path><path d="M11.79 1.754l-1.431.47 1.84 2.552z"></path><path d="M24.472 14.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M14.58 10.887c-.156-.83.096-1.569.692-2.142L12.78 6.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M17.812 10.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L9.2 7.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L7.046 9.54 5.802 8.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394l1.241 1.241 4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L4.89 11.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C21.74 20.8 22.271 18 20.62 14.982l-2.809-4.942z"></path></g></svg></span></span></button></div><span class="u-relative u-background js-actionMultirecommendCount u-marginLeft5"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton" data-action="show-recommends" data-action-value="409ab893a456">60</button></span></div></li><li class="u-marginVertical10 u-marginLeft3"><button class="button button--large button--dark button--chromeless is-touchIconFadeInPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="sign-up-prompt" data-sign-in-action="add-to-bookmarks" data-requires-token="true" data-redirect="https://medium.com/_/bookmark/p/409ab893a456" data-action-source="post_share_widget-----409ab893a456---------------------bookmark_sidebar"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M19.385 4h-9.77A2.623 2.623 0 0 0 7 6.615V23.01a1.022 1.022 0 0 0 1.595.847l5.905-4.004 5.905 4.004A1.022 1.022 0 0 0 22 23.011V6.62A2.625 2.625 0 0 0 19.385 4zM21 23l-5.91-3.955-.148-.107a.751.751 0 0 0-.884 0l-.147.107L8 23V6.615C8 5.725 8.725 5 9.615 5h9.77C20.275 5 21 5.725 21 6.615V23z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M19.385 4h-9.77A2.623 2.623 0 0 0 7 6.615V23.01a1.022 1.022 0 0 0 1.595.847l5.905-4.004 5.905 4.004A1.022 1.022 0 0 0 22 23.011V6.62A2.625 2.625 0 0 0 19.385 4z" fill-rule="evenodd"></path></svg></span></span></button></li><li class="u-marginVertical10 u-marginLeft3"><a class="button button--dark button--chromeless u-baseColor--buttonDark button--withIcon button--withSvgIcon button--dark button--chromeless" href="https://medium.com/p/409ab893a456/share/twitter" title="Share on Twitter" aria-label="Share on Twitter" target="_blank" data-action-source="post_share_widget"><span class="button-defaultState"><span class="svgIcon svgIcon--twitterFilled svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M22.053 7.54a4.474 4.474 0 0 0-3.31-1.455 4.526 4.526 0 0 0-4.526 4.524c0 .35.04.7.082 1.05a12.9 12.9 0 0 1-9.3-4.77c-.39.69-.61 1.46-.65 2.26.03 1.6.83 2.99 2.02 3.79-.72-.02-1.41-.22-2.02-.57-.01.02-.01.04 0 .08-.01 2.17 1.55 4 3.63 4.44-.39.08-.79.13-1.21.16-.28-.03-.57-.05-.81-.08.54 1.77 2.21 3.08 4.2 3.15a9.564 9.564 0 0 1-5.66 1.94c-.34-.03-.7-.06-1.05-.08 2 1.27 4.38 2.02 6.94 2.02 8.31 0 12.86-6.9 12.84-12.85.02-.24.01-.43 0-.65.89-.62 1.65-1.42 2.26-2.34-.82.38-1.69.62-2.59.72a4.37 4.37 0 0 0 1.94-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></span></span></a></li><li class="u-marginVertical10 u-marginLeft3"><a class="button button--dark button--chromeless u-baseColor--buttonDark button--withIcon button--withSvgIcon button--dark button--chromeless" href="https://medium.com/p/409ab893a456/share/facebook" title="Share on Facebook" aria-label="Share on Facebook" target="_blank" data-action-source="post_share_widget"><span class="button-defaultState"><span class="svgIcon svgIcon--facebookSquare svgIcon--29px"><svg class="svgIcon-use" width="29" height="29"><path d="M23.209 5H5.792A.792.792 0 0 0 5 5.791V23.21c0 .437.354.791.792.791h9.303v-7.125H12.72v-2.968h2.375v-2.375c0-2.455 1.553-3.662 3.741-3.662 1.049 0 1.95.078 2.213.112v2.565h-1.517c-1.192 0-1.469.567-1.469 1.397v1.963h2.969l-.594 2.968h-2.375L18.11 24h5.099a.791.791 0 0 0 .791-.791V5.79a.791.791 0 0 0-.791-.79"></path></svg></span></span></a></li></ul></div></aside><div class="u-fixed u-bottom0 u-width100pct u-backgroundWhite u-boxShadowTop u-borderBox u-paddingTop10 u-paddingBottom10 u-zIndexMetabar u-xs-hide js-stickyFooter"><div class="u-maxWidth700 u-marginAuto u-flexCenter"><div class="u-fontSize16 u-flex1 u-flexCenter"><div class="u-flex0 u-inlineBlock u-paddingRight20"><a class="link u-baseColor--link avatar avatar--roundedRectangle" href="https://ai-alignment.com" title="Go to AI Alignment" aria-label="Go to AI Alignment" data-collection-slug="ai-control"><img src="https://cdn-images-1.medium.com/fit/c/40/40/1*N56Qc5-aHTcfGff0scntKQ.png" class="avatar-image avatar-image--smaller" alt="AI Alignment"></a></div><div class="u-flex1 u-inlineBlock">Never miss a story from<strong> AI Alignment</strong>, when you sign up for Medium. <a class="link u-baseColor--link link--accent u-accentColor--textNormal u-accentColor--textDarken" href="https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg" data-action-source="sticky_footer">Learn more</a></div></div><div class="u-marginLeft50"><button class="button button--primary button--dark is-active u-noUserSelect button--withChrome u-accentColor--buttonDark u-uiTextSemibold u-textUppercase u-fontSize12 button--followCollection js-followCollectionButton" data-action="sign-up-prompt" data-sign-in-action="toggle-subscribe-collection" data-requires-token="true" data-redirect="https://medium.com/_/subscribe/collection/ai-control" data-action-source="sticky_footer----624d886c4aa4----------------------follow_metabar"><span class="button-label  button-defaultState js-buttonLabel">Get updates</span><span class="button-label button-activeState">Get updates</span></button></div></div></div><style class="js-collectionStyle">
.u-accentColor--borderLight {border-color: #02B875 !important;}
.u-accentColor--borderNormal {border-color: #02B875 !important;}
.u-accentColor--borderDark {border-color: #1C9963 !important;}
.u-accentColor--iconLight .svgIcon,.u-accentColor--iconLight.svgIcon {fill: #02B875 !important;}
.u-accentColor--iconNormal .svgIcon,.u-accentColor--iconNormal.svgIcon {fill: #02B875 !important;}
.u-accentColor--iconDark .svgIcon,.u-accentColor--iconDark.svgIcon {fill: #1C9963 !important;}
.u-accentColor--textNormal {color: #1C9963 !important;}
.u-accentColor--hoverTextNormal:hover {color: #1C9963 !important;}
.u-accentColor--textNormal.u-accentColor--textDarken:hover {color: #1C9963 !important;}
.u-accentColor--textDark {color: #1C9963 !important;}
.u-accentColor--backgroundLight {background-color: #02B875 !important;}
.u-accentColor--backgroundNormal {background-color: #02B875 !important;}
.u-accentColor--backgroundDark {background-color: #1C9963 !important;}
.u-accentColor--buttonDark {border-color: #1C9963 !important; color: #1C9963 !important;}
.u-accentColor--buttonDark:hover {border-color: #1C9963 !important;}
.u-accentColor--buttonDark .icon:before,.u-accentColor--buttonDark .svgIcon{color: #1C9963 !important; fill: #1C9963 !important;}
.u-accentColor--buttonNormal:not(.clapButton--largePill) {border-color: #02B875 !important; color: #1C9963 !important;}
.u-accentColor--buttonNormal:hover {border-color: #1C9963 !important;}
.u-accentColor--buttonNormal .icon:before,.u-accentColor--buttonNormal .svgIcon{color: #02B875 !important; fill: #02B875 !important;}
.u-accentColor--buttonNormal.button--filled .icon:before,.u-accentColor--buttonNormal.button--filled .svgIcon{color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-accentColor--buttonDark.button--filled,.u-accentColor--buttonDark.button--withChrome.is-active,.u-accentColor--fillWhenActive.is-active {background-color: #1C9963 !important; border-color: #1C9963 !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-accentColor--buttonNormal.button--filled:not(.clapButton--largePill),.u-accentColor--buttonNormal.button--withChrome.is-active:not(.clapButton--largePill) {background-color: #02B875 !important; border-color: #02B875 !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.postArticle.is-withAccentColors .markup--user,.postArticle.is-withAccentColors .markup--query {color: #1C9963 !important;}
.u-accentColor--highlightFaint {background-color: rgba(233, 253, 240, 1) !important;}
.u-accentColor--highlightStrong.is-active .svgIcon {fill: rgba(125, 255, 179, 1) !important;}
.postArticle.is-withAccentColors .markup--quote.is-other {background-color: rgba(233, 253, 240, 1) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .markup--quote.is-other {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(233, 253, 240, 1), rgba(233, 253, 240, 1));}
.postArticle.is-withAccentColors .markup--quote.is-me {background-color: rgba(173, 255, 207, 1) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .markup--quote.is-me {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(173, 255, 207, 1), rgba(173, 255, 207, 1));}
.postArticle.is-withAccentColors .markup--quote.is-targeted {background-color: rgba(125, 255, 179, 1) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .markup--quote.is-targeted {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(125, 255, 179, 1), rgba(125, 255, 179, 1));}
.postArticle.is-withAccentColors .markup--quote.is-selected {background-color: rgba(125, 255, 179, 1) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .markup--quote.is-selected {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(125, 255, 179, 1), rgba(125, 255, 179, 1));}
.postArticle.is-withAccentColors .markup--highlight {background-color: rgba(125, 255, 179, 1) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .markup--highlight {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(125, 255, 179, 1), rgba(125, 255, 179, 1));}.u-baseColor--iconNormal.avatar-halo {fill: rgba(0, 0, 0, 0.4980392156862745) !important;}</style><style class="js-collectionStyleConstant">.u-imageBgColor {background-color: rgba(0, 0, 0, 0.24705882352941178);}
.u-imageSpectrum .u-baseColor--borderLight {border-color: rgba(255, 255, 255, 0.6980392156862745) !important;}
.u-imageSpectrum .u-baseColor--borderNormal {border-color: rgba(255, 255, 255, 0.8980392156862745) !important;}
.u-imageSpectrum .u-baseColor--borderDark {border-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--iconLight .svgIcon,.u-imageSpectrum .u-baseColor--iconLight.svgIcon {fill: rgba(255, 255, 255, 0.8) !important;}
.u-imageSpectrum .u-baseColor--iconNormal .svgIcon,.u-imageSpectrum .u-baseColor--iconNormal.svgIcon {fill: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--iconDark .svgIcon,.u-imageSpectrum .u-baseColor--iconDark.svgIcon {fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--textNormal {color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--textNormal.u-baseColor--textDarken:hover {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--textDark {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--textDarker {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--backgroundLight {background-color: rgba(255, 255, 255, 0.8980392156862745) !important;}
.u-imageSpectrum .u-baseColor--backgroundNormal {background-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--backgroundDark {background-color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--buttonLight {border-color: rgba(255, 255, 255, 0.6980392156862745) !important; color: rgba(255, 255, 255, 0.8) !important;}
.u-imageSpectrum .u-baseColor--buttonLight:hover {border-color: rgba(255, 255, 255, 0.6980392156862745) !important;}
.u-imageSpectrum .u-baseColor--buttonLight .icon:before,.u-imageSpectrum .u-baseColor--buttonLight .svgIcon {color: rgba(255, 255, 255, 0.8) !important; fill: rgba(255, 255, 255, 0.8) !important;}
.u-imageSpectrum .u-baseColor--buttonDark {border-color: rgba(255, 255, 255, 0.9490196078431372) !important; color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--buttonDark:hover {border-color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--buttonDark .icon:before,.u-imageSpectrum .u-baseColor--buttonDark .svgIcon {color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--buttonNormal {border-color: rgba(255, 255, 255, 0.8980392156862745) !important; color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--buttonNormal:hover {border-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--buttonNormal .icon:before,.u-imageSpectrum .u-baseColor--buttonNormal .svgIcon {color: rgba(255, 255, 255, 0.9490196078431372) !important; fill: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--buttonDark.button--filled,.u-imageSpectrum .u-baseColor--buttonDark.button--withChrome.is-active {background-color: rgba(255, 255, 255, 1) !important; border-color: rgba(255, 255, 255, 1) !important; color: rgba(0, 0, 0, 0.24705882352941178) !important; fill: rgba(0, 0, 0, 0.24705882352941178) !important;}
.u-imageSpectrum .u-baseColor--buttonNormal.button--filled,.u-imageSpectrum .u-baseColor--buttonNormal.button--withChrome.is-active {background-color: rgba(255, 255, 255, 0.9490196078431372) !important; border-color: rgba(255, 255, 255, 0.9490196078431372) !important; color: rgba(0, 0, 0, 0.24705882352941178) !important; fill: rgba(0, 0, 0, 0.24705882352941178) !important;}
.u-imageSpectrum .u-baseColor--link {color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--link.link--darkenOnHover:hover {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--link.link--darken:hover,.u-imageSpectrum .u-baseColor--link.link--darken:focus,.u-imageSpectrum .u-baseColor--link.link--darken:active {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--link.link--dark {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--link.link--dark.link--darken:hover,.u-imageSpectrum .u-baseColor--link.link--dark.link--darken:focus,.u-imageSpectrum .u-baseColor--link.link--dark.link--darken:active {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--link.link--darker {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--placeholderNormal ::-webkit-input-placeholder {color: rgba(255, 255, 255, 0.8);}
.u-imageSpectrum .u-baseColor--placeholderNormal ::-moz-placeholder {color: rgba(255, 255, 255, 0.8);}
.u-imageSpectrum .u-baseColor--placeholderNormal :-ms-input-placeholder {color: rgba(255, 255, 255, 0.8);}
.u-imageSpectrum .svgIcon--logoWordmark {stroke: none !important; fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .svgIcon--logoMonogram {stroke: none !important; fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum  .ui-h1,.u-imageSpectrum  .ui-h2,.u-imageSpectrum  .ui-h3,.u-imageSpectrum  .ui-h4,.u-imageSpectrum  .ui-brand1,.u-imageSpectrum  .ui-brand2,.u-imageSpectrum  .ui-captionStrong {color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum  .ui-body,.u-imageSpectrum  .ui-caps {color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum  .ui-summary,.u-imageSpectrum  .ui-caption {color: rgba(255, 255, 255, 0.8) !important; fill: rgba(255, 255, 255, 0.8) !important;}
.u-imageSpectrum .u-accentColor--borderLight {border-color: rgba(255, 255, 255, 0.6980392156862745) !important;}
.u-imageSpectrum .u-accentColor--borderNormal {border-color: rgba(255, 255, 255, 0.8980392156862745) !important;}
.u-imageSpectrum .u-accentColor--borderDark {border-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--iconLight .svgIcon,.u-imageSpectrum .u-accentColor--iconLight.svgIcon {fill: rgba(255, 255, 255, 0.8) !important;}
.u-imageSpectrum .u-accentColor--iconNormal .svgIcon,.u-imageSpectrum .u-accentColor--iconNormal.svgIcon {fill: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--iconDark .svgIcon,.u-imageSpectrum .u-accentColor--iconDark.svgIcon {fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--textNormal {color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--hoverTextNormal:hover {color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--textNormal.u-accentColor--textDarken:hover {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--textDark {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--backgroundLight {background-color: rgba(255, 255, 255, 0.8980392156862745) !important;}
.u-imageSpectrum .u-accentColor--backgroundNormal {background-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--backgroundDark {background-color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--buttonDark {border-color: rgba(255, 255, 255, 0.9490196078431372) !important; color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--buttonDark:hover {border-color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--buttonDark .icon:before,.u-imageSpectrum .u-accentColor--buttonDark .svgIcon{color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--buttonNormal:not(.clapButton--largePill) {border-color: rgba(255, 255, 255, 0.8980392156862745) !important; color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--buttonNormal:hover {border-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--buttonNormal .icon:before,.u-imageSpectrum .u-accentColor--buttonNormal .svgIcon{color: rgba(255, 255, 255, 0.9490196078431372) !important; fill: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--buttonNormal.button--filled .icon:before,.u-imageSpectrum .u-accentColor--buttonNormal.button--filled .svgIcon{color: rgba(0, 0, 0, 0.24705882352941178) !important; fill: rgba(0, 0, 0, 0.24705882352941178) !important;}
.u-imageSpectrum .u-accentColor--buttonDark.button--filled,.u-imageSpectrum .u-accentColor--buttonDark.button--withChrome.is-active,.u-imageSpectrum .u-accentColor--fillWhenActive.is-active {background-color: rgba(255, 255, 255, 1) !important; border-color: rgba(255, 255, 255, 1) !important; color: rgba(0, 0, 0, 0.24705882352941178) !important; fill: rgba(0, 0, 0, 0.24705882352941178) !important;}
.u-imageSpectrum .u-accentColor--buttonNormal.button--filled:not(.clapButton--largePill),.u-imageSpectrum .u-accentColor--buttonNormal.button--withChrome.is-active:not(.clapButton--largePill) {background-color: rgba(255, 255, 255, 0.9490196078431372) !important; border-color: rgba(255, 255, 255, 0.9490196078431372) !important; color: rgba(0, 0, 0, 0.24705882352941178) !important; fill: rgba(0, 0, 0, 0.24705882352941178) !important;}
.u-imageSpectrum .postArticle.is-withAccentColors .markup--user,.u-imageSpectrum .postArticle.is-withAccentColors .markup--query {color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--highlightFaint {background-color: rgba(255, 255, 255, 0.2) !important;}
.u-imageSpectrum .u-accentColor--highlightStrong.is-active .svgIcon {fill: rgba(255, 255, 255, 0.6) !important;}
.postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-other {background-color: rgba(255, 255, 255, 0.2) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-other {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 0.2));}
.postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-me {background-color: rgba(255, 255, 255, 0.4) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-me {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(255, 255, 255, 0.4), rgba(255, 255, 255, 0.4));}
.postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-targeted {background-color: rgba(255, 255, 255, 0.6) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-targeted {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(255, 255, 255, 0.6), rgba(255, 255, 255, 0.6));}
.postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-selected {background-color: rgba(255, 255, 255, 0.6) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-selected {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(255, 255, 255, 0.6), rgba(255, 255, 255, 0.6));}
.postArticle.is-withAccentColors .u-imageSpectrum .markup--highlight {background-color: rgba(255, 255, 255, 0.6) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .u-imageSpectrum .markup--highlight {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(255, 255, 255, 0.6), rgba(255, 255, 255, 0.6));}.u-resetSpectrum .u-tintBgColor {background-color: rgba(255, 255, 255, 1) !important;}.u-resetSpectrum .u-tintBgColor .u-fadeLeft:before {background-image: linear-gradient(to right, rgba(255, 255, 255, 1) 0%, rgba(255, 255, 255, 0) 100%) !important;}.u-resetSpectrum .u-tintBgColor .u-fadeRight:after {background-image: linear-gradient(to right, rgba(255, 255, 255, 0) 0%, rgba(255, 255, 255, 1) 100%) !important;}
.u-resetSpectrum .u-baseColor--borderLight {border-color: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-baseColor--borderNormal {border-color: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--borderDark {border-color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--iconLight .svgIcon,.u-resetSpectrum .u-baseColor--iconLight.svgIcon {fill: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-baseColor--iconNormal .svgIcon,.u-resetSpectrum .u-baseColor--iconNormal.svgIcon {fill: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--iconDark .svgIcon,.u-resetSpectrum .u-baseColor--iconDark.svgIcon {fill: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--textNormal {color: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--textNormal.u-baseColor--textDarken:hover {color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--textDark {color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--textDarker {color: rgba(0, 0, 0, 0.8) !important;}
.u-resetSpectrum .u-baseColor--backgroundLight {background-color: rgba(0, 0, 0, 0.09803921568627451) !important;}
.u-resetSpectrum .u-baseColor--backgroundNormal {background-color: rgba(0, 0, 0, 0.2) !important;}
.u-resetSpectrum .u-baseColor--backgroundDark {background-color: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-baseColor--buttonLight {border-color: rgba(0, 0, 0, 0.2980392156862745) !important; color: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-baseColor--buttonLight:hover {border-color: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-baseColor--buttonLight .icon:before,.u-resetSpectrum .u-baseColor--buttonLight .svgIcon {color: rgba(0, 0, 0, 0.2980392156862745) !important; fill: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-baseColor--buttonDark {border-color: rgba(0, 0, 0, 0.6) !important; color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--buttonDark:hover {border-color: rgba(0, 0, 0, 0.8) !important;}
.u-resetSpectrum .u-baseColor--buttonDark .icon:before,.u-resetSpectrum .u-baseColor--buttonDark .svgIcon {color: rgba(0, 0, 0, 0.6) !important; fill: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--buttonNormal {border-color: rgba(0, 0, 0, 0.4980392156862745) !important; color: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--buttonNormal:hover {border-color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--buttonNormal .icon:before,.u-resetSpectrum .u-baseColor--buttonNormal .svgIcon {color: rgba(0, 0, 0, 0.4980392156862745) !important; fill: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--buttonDark.button--filled,.u-resetSpectrum .u-baseColor--buttonDark.button--withChrome.is-active {background-color: rgba(0, 0, 0, 0.2980392156862745) !important; border-color: rgba(0, 0, 0, 0.2980392156862745) !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-resetSpectrum .u-baseColor--buttonNormal.button--filled,.u-resetSpectrum .u-baseColor--buttonNormal.button--withChrome.is-active {background-color: rgba(0, 0, 0, 0.2) !important; border-color: rgba(0, 0, 0, 0.2) !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-resetSpectrum .u-baseColor--link {color: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--link.link--darkenOnHover:hover {color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--link.link--darken:hover,.u-resetSpectrum .u-baseColor--link.link--darken:focus,.u-resetSpectrum .u-baseColor--link.link--darken:active {color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--link.link--dark {color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--link.link--dark.link--darken:hover,.u-resetSpectrum .u-baseColor--link.link--dark.link--darken:focus,.u-resetSpectrum .u-baseColor--link.link--dark.link--darken:active {color: rgba(0, 0, 0, 0.8) !important;}
.u-resetSpectrum .u-baseColor--link.link--darker {color: rgba(0, 0, 0, 0.8) !important;}
.u-resetSpectrum .u-baseColor--placeholderNormal ::-webkit-input-placeholder {color: rgba(0, 0, 0, 0.2980392156862745);}
.u-resetSpectrum .u-baseColor--placeholderNormal ::-moz-placeholder {color: rgba(0, 0, 0, 0.2980392156862745);}
.u-resetSpectrum .u-baseColor--placeholderNormal :-ms-input-placeholder {color: rgba(0, 0, 0, 0.2980392156862745);}
.u-resetSpectrum .svgIcon--logoWordmark {stroke: none !important; fill: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .svgIcon--logoMonogram {stroke: none !important; fill: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum  .ui-h1,.u-resetSpectrum  .ui-h2,.u-resetSpectrum  .ui-h3,.u-resetSpectrum  .ui-h4,.u-resetSpectrum  .ui-brand1,.u-resetSpectrum  .ui-brand2,.u-resetSpectrum  .ui-captionStrong {color: rgba(0, 0, 0, 0.8) !important; fill: rgba(0, 0, 0, 0.8) !important;}
.u-resetSpectrum  .ui-body,.u-resetSpectrum  .ui-caps {color: rgba(0, 0, 0, 0.6) !important; fill: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum  .ui-summary,.u-resetSpectrum  .ui-caption {color: rgba(0, 0, 0, 0.2980392156862745) !important; fill: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-accentColor--borderLight {border-color: rgba(2, 184, 117, 1) !important;}
.u-resetSpectrum .u-accentColor--borderNormal {border-color: rgba(2, 184, 117, 1) !important;}
.u-resetSpectrum .u-accentColor--borderDark {border-color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--iconLight .svgIcon,.u-resetSpectrum .u-accentColor--iconLight.svgIcon {fill: rgba(2, 184, 117, 1) !important;}
.u-resetSpectrum .u-accentColor--iconNormal .svgIcon,.u-resetSpectrum .u-accentColor--iconNormal.svgIcon {fill: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--iconDark .svgIcon,.u-resetSpectrum .u-accentColor--iconDark.svgIcon {fill: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--textNormal {color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--hoverTextNormal:hover {color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--textNormal.u-accentColor--textDarken:hover {color: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--textDark {color: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--backgroundLight {background-color: rgba(2, 184, 117, 1) !important;}
.u-resetSpectrum .u-accentColor--backgroundNormal {background-color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--backgroundDark {background-color: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonDark {border-color: rgba(0, 171, 107, 1) !important; color: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonDark:hover {border-color: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonDark .icon:before,.u-resetSpectrum .u-accentColor--buttonDark .svgIcon{color: rgba(28, 153, 99, 1) !important; fill: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonNormal:not(.clapButton--largePill) {border-color: rgba(2, 184, 117, 1) !important; color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonNormal:hover {border-color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonNormal .icon:before,.u-resetSpectrum .u-accentColor--buttonNormal .svgIcon{color: rgba(0, 171, 107, 1) !important; fill: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonNormal.button--filled .icon:before,.u-resetSpectrum .u-accentColor--buttonNormal.button--filled .svgIcon{color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonDark.button--filled,.u-resetSpectrum .u-accentColor--buttonDark.button--withChrome.is-active,.u-resetSpectrum .u-accentColor--fillWhenActive.is-active {background-color: rgba(28, 153, 99, 1) !important; border-color: rgba(28, 153, 99, 1) !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonNormal.button--filled:not(.clapButton--largePill),.u-resetSpectrum .u-accentColor--buttonNormal.button--withChrome.is-active:not(.clapButton--largePill) {background-color: rgba(0, 171, 107, 1) !important; border-color: rgba(0, 171, 107, 1) !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-resetSpectrum .postArticle.is-withAccentColors .markup--user,.u-resetSpectrum .postArticle.is-withAccentColors .markup--query {color: rgba(0, 171, 107, 1) !important;}</style><div class="highlightMenu" data-action-scope="_actionscope_3"><div class="highlightMenu-inner"><div class="buttonSet"><a class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--chromeless button--highlightMenu js-highlightMenuTwitterShare" href="https://medium.com/p/409ab893a456/share/twitter" title="Share on Twitter" aria-label="Share on Twitter" target="_blank" data-action="twitter"><span class="button-defaultState"><span class="svgIcon svgIcon--twitterFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><path d="M21.725 5.338c-.744.47-1.605.804-2.513 1.006a3.978 3.978 0 0 0-2.942-1.293c-2.22 0-4.02 1.81-4.02 4.02 0 .32.034.63.07.94-3.31-.18-6.27-1.78-8.255-4.23a4.544 4.544 0 0 0-.574 2.01c.04 1.43.74 2.66 1.8 3.38-.63-.01-1.25-.19-1.79-.5v.08c0 1.93 1.38 3.56 3.23 3.95-.34.07-.7.12-1.07.14-.25-.02-.5-.04-.72-.07.49 1.58 1.97 2.74 3.74 2.8a8.49 8.49 0 0 1-5.02 1.72c-.3-.03-.62-.04-.93-.07A11.447 11.447 0 0 0 8.88 21c7.386 0 11.43-6.13 11.414-11.414.015-.21.01-.38 0-.578a7.604 7.604 0 0 0 2.01-2.08 7.27 7.27 0 0 1-2.297.645 3.856 3.856 0 0 0 1.72-2.23"></path></svg></span></span></a><div class="buttonSet-separator"></div><button class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--highlightMenu" data-action="sign-up-prompt" data-sign-in-action="highlight" data-redirect="https://ai-alignment.com/towards-formalizing-universality-409ab893a456" data-skip-onboarding="true" data-action-source="quote_menu--------------------------privatenote_text"><span class="svgIcon svgIcon--privatenoteFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25"><g fill-rule="evenodd"><path d="M17.662 4.552H7.346A4.36 4.36 0 0 0 3 8.898v5.685c0 2.168 1.614 3.962 3.697 4.28v2.77c0 .303.35.476.59.29l3.904-2.994h6.48c2.39 0 4.35-1.96 4.35-4.35V8.9c0-2.39-1.95-4.346-4.34-4.346zM16 14.31a.99.99 0 0 1-1.003.99h-4.994C9.45 15.3 9 14.85 9 14.31v-3.02a.99.99 0 0 1 1-.99v-.782a2.5 2.5 0 0 1 2.5-2.51c1.38 0 2.5 1.13 2.5 2.51v.782c.552.002 1 .452 1 .99v3.02z"></path><path d="M14 9.81c0-.832-.674-1.68-1.5-1.68-.833 0-1.5.84-1.5 1.68v.49h3v-.49z"></path></g></svg></span></button></div></div><div class="highlightMenu-arrowClip"><span class="highlightMenu-arrow"></span></div></div></div></div></div><div class="loadingBar"></div><script>// <![CDATA[
window["obvInit"] = function (opt_embedded) {window["obvInit"]["embedded"] = opt_embedded; window["obvInit"]["ready"] = true;}
// ]]></script><script>// <![CDATA[
var GLOBALS = {"audioUrl":"https://d1fcbxp97j4nb2.cloudfront.net","baseUrl":"https://ai-alignment.com","buildLabel":"37496-7f20643","currentUser":{"userId":"lo_JuU2p2EvGepj","isVerified":false,"subscriberEmail":"","hasPastMemberships":false,"isEnrolledInHightower":false,"isEligibleForHightower":false,"hightowerLastLockedAt":0,"isWriterProgramEnrolled":true,"isWriterProgramInvited":false,"isWriterProgramOptedOut":false,"writerProgramVersion":0,"writerProgramEnrolledAt":0,"friendLinkOnboarding":0,"hasAdditionalUnlocks":false,"hasApiAccess":false,"isQuarantined":false,"writerProgramDistributionSettingOptedIn":false},"currentUserHasUnverifiedEmail":false,"isAuthenticated":false,"isCurrentUserVerified":false,"miroUrl":"https://cdn-images-1.medium.com","moduleUrls":{"base":"https://cdn-static-1.medium.com/_/fp/gen-js/main-base.bundle.h-S66qYGELCwGMjo-I5sGg.js","common-async":"https://cdn-static-1.medium.com/_/fp/gen-js/main-common-async.bundle.qFZkgzLZ5TYXIerh_w9awQ.js","hightower":"https://cdn-static-1.medium.com/_/fp/gen-js/main-hightower.bundle.YYDLiHqz4VuEAfIKbpgHlA.js","home-screens":"https://cdn-static-1.medium.com/_/fp/gen-js/main-home-screens.bundle.KOsn4BMHvTHwO7kChpWnwQ.js","misc-screens":"https://cdn-static-1.medium.com/_/fp/gen-js/main-misc-screens.bundle.9HQt5flvjhqNbC82Kz7w3A.js","notes":"https://cdn-static-1.medium.com/_/fp/gen-js/main-notes.bundle.V05mXLtyLz2Mj5DzEML26A.js","payments":"https://cdn-static-1.medium.com/_/fp/gen-js/main-payments.bundle.4s7BX_pcnrqTR9pXdg44lw.js","posters":"https://cdn-static-1.medium.com/_/fp/gen-js/main-posters.bundle.oAF5QtbeXBsEy2D-ZeKzOA.js","power-readers":"https://cdn-static-1.medium.com/_/fp/gen-js/main-power-readers.bundle.242jbfrxhns9kmBC8hYbGA.js","pubs":"https://cdn-static-1.medium.com/_/fp/gen-js/main-pubs.bundle.OVIBk1ifJgWYBiuHy2mNlA.js","stats":"https://cdn-static-1.medium.com/_/fp/gen-js/main-stats.bundle.FRAzB3YCXktr7d19iN9skg.js"},"previewConfig":{"weightThreshold":1,"weightImageParagraph":0.51,"weightIframeParagraph":0.8,"weightTextParagraph":0.08,"weightEmptyParagraph":0,"weightP":0.003,"weightH":0.005,"weightBq":0.003,"minPTextLength":60,"truncateBoundaryChars":20,"detectTitle":true,"detectTitleLevThreshold":0.15},"productName":"Medium","supportsEdit":false,"termsUrl":"//medium.com/policy/9db0094a1e0f","textshotHost":"textshot.medium.com","transactionId":"1557467651291:6c8937d25292","useragent":{"browser":"headlesschrome","family":"chrome","os":"mac","version":0,"supportsDesktopEdit":false,"supportsInteract":false,"supportsView":true,"isMobile":false,"isTablet":false,"isNative":false,"supportsFileAPI":false,"isTier1":false,"clientVersion":"","unknownParagraphsBad":false,"clientChannel":"","supportsRealScrollEvents":false,"supportsVhUnits":false,"ruinsViewportSections":false,"supportsHtml5Video":false,"supportsMagicUnderlines":false,"isWebView":false,"isFacebookWebView":false,"supportsProgressiveMedia":false,"supportsPromotedPosts":true,"isBot":false,"isNativeIphone":false,"supportsCssVariables":false,"supportsVideoSections":true,"emojiSupportLevel":5,"isSearchBot":false,"isSyndicationBot":false,"isNativeAndroid":false,"isNativeIos":false,"supportsScrollableMetabar":false},"variants":{"allow_access":true,"allow_signup":true,"allow_test_auth":"disallow","signin_services":"twitter,facebook,google,email,google-fastidv,google-one-tap","signup_services":"twitter,facebook,google,email,google-fastidv,google-one-tap","google_sign_in_android":true,"reengagement_notification_duration":3,"browsable_stream_config_bucket":"curated-topics","enable_dedicated_series_tab_api_ios":true,"enable_post_import":true,"available_monthly_plan":"60e220181034","available_annual_plan":"2c754bcc2995","disable_ios_resume_reading_toast":true,"is_not_medium_subscriber":true,"glyph_font_set":"m2","enable_branding":true,"enable_branding_fonts":true,"max_premium_content_per_user_under_metering":3,"enable_automated_mission_control_triggers":true,"enable_lite_profile":true,"enable_marketing_emails":true,"enable_topic_lifecycle_email":true,"enable_parsely":true,"enable_branch_io":true,"enable_ios_post_stats":true,"enable_lite_topics":true,"enable_lite_stories":true,"redis_read_write_splitting":true,"enable_tipalti_onboarding":true,"enable_annual_renewal_reminder_email":true,"enable_janky_spam_rules":"users,posts","enable_new_collaborative_filtering_data":true,"android_rating_prompt_stories_read_threshold":2,"stripe_v3":true,"enable_google_one_tap":true,"enable_email_sign_in_captcha":true,"enable_rito_with_viewer_query":true,"enable_rito_with_flag_query":true,"enable_rito_post_handler":true,"enable_rito_sequence_post_recirc_query":true,"enable_rito_post_recirc_query":true,"enable_rito_topic_handler":true,"enable_rito_stats_post_handler":true,"enable_rito_stats_post_chart":true,"enable_rito_lifetime_earnings_tooltip":true,"enable_rito_stats_post_referrers_container":true,"enable_rito_post_feature_mutation":true,"enable_rito_post_unfeature_mutation":true,"enable_rito_quote_delete_mutation":true,"enable_rito_user_block_mutation":true,"enable_rito_user_unblock_mutation":true,"enable_rito_report_user_link":true,"enable_rito_bookmark_post_default":true,"enable_rito_unbookmark_post_default":true,"enable_rito_archive_post_default":true,"enable_rito_unarchive_post_default":true,"enable_rito_clap":true,"enable_rito_subscribe_series":true,"enable_rito_unsubscribe_series":true,"enable_rito_follow_topic":true,"enable_rito_unfollow_topic":true,"enable_rito_follow_user":true,"enable_rito_unfollow_user":true,"enable_rito_your_story_delete_mutation":true,"enable_rito_update_last_read_section":true,"editorial_push_notifications":true,"enable_primary_topic_for_mobile":true,"enable_rito_sequence_post_handler":true,"enable_todays_highlights_ios":true,"enable_logged_out_homepage_signup":true,"use_new_admin_topic_backend":true,"enable_quarantine_rules":true,"enable_lite_privacy_banner":true,"enable_patronus_on_kubernetes":true,"pub_sidebar":true,"disable_mobile_featured_chunk":true,"enable_rito_user_profile_overview_handler":true,"enable_rito_user_stream_overview":true,"enable_rito_user_profile_latest_handler":true,"enable_rito_user_stream_latest":true,"enable_rito_user_profile_actions":true,"enable_rito_post_actions":true,"enable_rito_user_profile_highlights_handler":true,"enable_rito_user_stream_highlights":true,"enable_rito_user_profile_series_handler":true,"enable_rito_user_stream_series":true,"enable_rito_user_profile_claps_handler":true,"enable_rito_user_stream_claps":true,"enable_rito_user_profile_responses_handler":true,"enable_rito_user_stream_responses":true,"enable_rito_billing_history_handler":true,"enable_rito_your_stories_handler":true,"enable_rito_sequence_library_handler":true,"enable_rito_series_handler":true,"enable_rito_amppost_handler":true,"enable_rito_follow_collection_mutation":true,"enable_rito_unfollow_collection_mutation":true,"enable_pub_newsletters":true,"enable_may_meter_email_test":true,"enable_rex_app_highlights":true,"enable_new_user_avatar_dropdown_menu":true,"enable_mobile_pubcrawl_home_feed":true,"enable_draft_in_post_cotent":true},"xsrfToken":"","iosAppId":"828256236","supportEmail":"yourfriends@medium.com","fp":{"/icons/monogram-mask.svg":"https://cdn-static-1.medium.com/_/fp/icons/monogram-mask.KPLCSFEZviQN0jQ7veN2RQ.svg","/icons/favicon-dev-editor.ico":"https://cdn-static-1.medium.com/_/fp/icons/favicon-dev-editor.YKKRxBO8EMvIqhyCwIiJeQ.ico","/icons/favicon-hatch-editor.ico":"https://cdn-static-1.medium.com/_/fp/icons/favicon-hatch-editor.BuEyHIqlyh2s_XEk4Rl32Q.ico","/icons/favicon-medium-editor.ico":"https://cdn-static-1.medium.com/_/fp/icons/favicon-medium-editor.PiakrZWB7Yb80quUVQWM6g.ico"},"authBaseUrl":"https://medium.com","imageUploadSizeMb":25,"isAuthDomainRequest":false,"domainCollectionSlug":"ai-control","algoliaApiEndpoint":"https://MQ57UUUQZ2-dsn.algolia.net","algoliaAppId":"MQ57UUUQZ2","algoliaSearchOnlyApiKey":"394474ced050e3911ae2249ecc774921","iosAppStoreUrl":"https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8","iosAppLinkBaseUrl":"medium:","algoliaIndexPrefix":"medium_","androidPlayStoreUrl":"https://play.google.com/store/apps/details?id=com.medium.reader","googleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","androidPackage":"com.medium.reader","androidPlayStoreMarketScheme":"market://details?id=com.medium.reader","googleAuthUri":"https://accounts.google.com/o/oauth2/auth","androidScheme":"medium","layoutData":{"useDynamicScripts":false,"googleAnalyticsTrackingCode":"UA-24232453-2","jsShivUrl":"https://cdn-static-1.medium.com/_/fp/js/shiv.RI2ePTZ5gFmMgLzG5bEVAA.js","useDynamicCss":false,"faviconUrl":"https://cdn-static-1.medium.com/_/fp/icons/favicon-rebrand-medium.3Y6xpZ-0FSdWDnPM3hSBIA.ico","faviconImageId":"1*8I-HPL0bfoIzGied-dzOvA.png","fontSets":[{"id":8,"url":"https://glyph.medium.com/css/e/sr/latin/e/ssr/latin/e/ssb/latin/m2.css"},{"id":11,"url":"https://glyph.medium.com/css/m2.css"},{"id":9,"url":"https://glyph.medium.com/css/mkt.css"}],"editorFaviconUrl":"https://cdn-static-1.medium.com/_/fp/icons/favicon-rebrand-medium-editor.3Y6xpZ-0FSdWDnPM3hSBIA.ico","glyphUrl":"https://glyph.medium.com"},"authBaseUrlRev":"moc.muidem//:sptth","isDnt":false,"stripePublishableKey":"pk_live_7FReX44VnNIInZwrIIx6ghjl","archiveUploadSizeMb":100,"paymentData":{"currencies":{"1":{"label":"US Dollar","external":"usd"}},"countries":{"1":{"label":"United States of America","external":"US"}},"accountTypes":{"1":{"label":"Individual","external":"individual"},"2":{"label":"Company","external":"company"}}},"previewConfig2":{"weightThreshold":1,"weightImageParagraph":0.05,"raiseImage":true,"enforceHeaderHierarchy":true,"isImageInsetRight":true},"isAmp":false,"iosScheme":"medium","isSwBoot":false,"lightstep":{"accessToken":"ce5be895bef60919541332990ac9fef2","carrier":"{\"ot-tracer-spanid\":\"55c81fa8283afa79\",\"ot-tracer-traceid\":\"1ccaebca3df5c434\",\"ot-tracer-sampled\":\"true\"}","host":"collector-medium.lightstep.com"},"facebook":{"key":"542599432471018","namespace":"medium-com","scope":{"default":["public_profile","email"],"connect":["public_profile","email"],"login":["public_profile","email"],"share":["public_profile","email"]}},"editorsPicksTopicId":"3985d2a191c5","popularOnMediumTopicId":"9d34e48ecf94","memberContentTopicId":"13d7efd82fb2","audioContentTopicId":"3792abbd134","brandedSequenceId":"7d337ddf1941","isDoNotAuth":false,"buggle":{"url":"https://buggle.medium.com","videoUrl":"https://cdn-videos-1.medium.com","audioUrl":"https://cdn-audio-1.medium.com"},"referrerType":5,"isMeteredOut":false,"meterConfig":{"maxUnlockCount":3,"windowLength":"MONTHLY"},"partnerProgramEmail":"partnerprogram@medium.com","userResearchPrompts":[{"promptId":"lo_post_page_4","type":0,"url":"www.calendly.com"},{"promptId":"lo_home_page","type":1,"url":"www.calendly.com"},{"promptId":"lo_profile_page","type":2,"url":"www.calendly.com"}],"recaptchaKey":"6LdAokEUAAAAAC7seICd4vtC8chDb3jIXDQulyUJ","signinWallCustomDomainCollectionIds":["3a8144eabfe3","336d898217ee","61061eb0c96b","138adf9c44c","819cc2aaeee0"],"countryCode":"US","bypassMeter":false,"branchKey":"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm","paypal":{"clientMode":"production","oneYearGift":{"name":"Medium Membership (1 Year, Digital Gift Code)","description":"Unlimited access to the best and brightest stories on Medium. Gift codes can be redeemed at medium.com/redeem.","price":"50.00","currency":"USD","sku":"membership-gift-1-yr"}},"collectionConfig":{"mediumOwnedAndOperatedCollectionIds":["544c7006046e // Human Parts","bcc38c8f6edf // Matter","444d13b52878 // OneZero","8d6b8a439e32 // Elemental","92d2092dc598 // Gay Mag","1285ba81cada // Heated"]}}
// ]]></script><script charset="UTF-8" src="https://cdn-static-1.medium.com/_/fp/gen-js/main-base.bundle.h-S66qYGELCwGMjo-I5sGg.js" async=""></script><script>// <![CDATA[
window["obvInit"]({"value":{"id":"409ab893a456","versionId":"bc6ca472e384","creatorId":"57f1a655a613","creator":{"userId":"57f1a655a613","name":"Paul Christiano","username":"paulfchristiano","createdAt":1417286353352,"imageId":"1*BNjZCuQuRfIgcXCBMipuBw.jpeg","backgroundImageId":"","bio":"OpenAI","twitterScreenName":"","socialStats":{"userId":"57f1a655a613","usersFollowedCount":93,"usersFollowedByCount":821,"type":"SocialStats"},"social":{"userId":"lo_JuU2p2EvGepj","targetUserId":"57f1a655a613","type":"Social"},"facebookAccountId":"1167284919","allowNotes":1,"mediumMemberAt":0,"isNsfw":false,"isWriterProgramEnrolled":true,"isQuarantined":false,"type":"User"},"homeCollection":{"id":"624d886c4aa4","name":"AI Alignment","slug":"ai-control","tags":[],"creatorId":"57f1a655a613","description":"Aligning AI systems with human interests.","shortDescription":"Aligning AI systems with human interests.","image":{"imageId":"1*N56Qc5-aHTcfGff0scntKQ.png","filter":"","backgroundSize":"","originalWidth":512,"originalHeight":512,"strategy":"resample","height":0,"width":0},"metadata":{"followerCount":2834,"activeAt":1548040822588},"virtuals":{"permissions":{"canPublish":false,"canPublishAll":false,"canRepublish":false,"canRemove":false,"canManageAll":false,"canSubmit":false,"canEditPosts":false,"canAddWriters":false,"canViewStats":false,"canSendNewsletter":false,"canViewLockedPosts":false,"canViewCloaked":false,"canEditOwnPosts":false,"canBeAssignedAuthor":false,"canEnrollInHightower":false,"canLockPostsForMediumMembers":false,"canLockOwnPostsForMediumMembers":false},"isSubscribed":false,"isNewsletterSubscribed":false,"isEnrolledInHightower":false,"isEligibleForHightower":false},"logo":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"collectionMastheadId":"29f3dcc2e4","domain":"ai-alignment.com","sections":[{"type":2,"collectionHeaderMetadata":{"title":"AI Alignment","backgroundImage":{},"logoImage":{},"alignment":2,"layout":4}},{"type":1,"postListMetadata":{"source":3,"layout":4,"number":3,"postIds":["157debfd1616","b49ad992940b","b959644d79c2"]}},{"type":1,"postListMetadata":{"source":1,"layout":4,"number":24,"postIds":[],"sectionHeader":"Latest"}}],"favicon":{"imageId":"1*cciPf4CUXd_Zyux0Jg0yBQ.png","filter":"","backgroundSize":"","originalWidth":400,"originalHeight":400,"strategy":"resample","height":0,"width":0},"colorPalette":{"defaultBackgroundSpectrum":{"colorPoints":[{"color":"#FF02B875","point":0},{"color":"#FF00AB6B","point":0.1},{"color":"#FF1C9963","point":0.2},{"color":"#FF092E20","point":1}],"backgroundColor":"#FFFFFFFF"},"highlightSpectrum":{"colorPoints":[{"color":"#FFFFFFFF","point":0},{"color":"#FFE9FDF0","point":0.1},{"color":"#FFE2FAEE","point":0.2},{"color":"#FFADFFCF","point":0.6},{"color":"#FF7DFFB3","point":1}],"backgroundColor":"#FFFFFFFF"}},"navItems":[],"colorBehavior":1,"instantArticlesState":0,"acceleratedMobilePagesState":0,"ampLogo":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"header":{"title":"AI Alignment","backgroundImage":{},"logoImage":{},"alignment":2,"layout":4},"paidForDomainAt":1490733089988,"type":"Collection"},"homeCollectionId":"624d886c4aa4","title":"Towards formalizing universality","detectedLanguage":"en","latestVersion":"bc6ca472e384","latestPublishedVersion":"bc6ca472e384","hasUnpublishedEdits":false,"latestRev":6590,"createdAt":1543902604114,"updatedAt":1547167192948,"acceptedAt":0,"firstPublishedAt":1547092192461,"latestPublishedAt":1547167192948,"vote":false,"experimentalCss":"","displayAuthor":"","content":{"subtitle":"An attempt to formalize universality as â€œable to understand anything that any computation can understand.â€","bodyModel":{"paragraphs":[{"name":"866c","type":3,"text":"Towards formalizing universality","markups":[]},{"name":"9ced","type":1,"text":"The scalability of iterated amplification or debate seems to depend on whether large enough teams of humans can carry out arbitrarily complicated reasoning. Are these schemes â€œuniversal,â€ or are there kinds of reasoning that work but which humans fundamentally canâ€™t understand?","markups":[{"type":3,"start":19,"end":41,"href":"https://arxiv.org/pdf/1810.08575.pdf","title":"","rel":"","anchorType":0},{"type":3,"start":45,"end":51,"href":"https://arxiv.org/abs/1805.00899","title":"","rel":"","anchorType":0}]},{"name":"22c8","type":1,"text":"This post defines the concept of â€œascription universality,â€ which tries to capture the property that a question-answering system A is better-informed than any particular simpler computation C.","markups":[{"type":1,"start":129,"end":130},{"type":1,"start":190,"end":191}]},{"name":"4a93","type":1,"text":"These parallel posts explain why I believe that the alignment of iterated amplification largely depends on whether HCH is ascription universal. Ultimately I think that the â€œrightâ€ definition will be closely tied to the use we want to make of it, and so we should be refining this definition in parallel with exploring its applications.","markups":[{"type":3,"start":0,"end":5,"href":"https://ai-alignment.com/informed-oversight-18fcb5d3d1e1","title":"","rel":"","anchorType":0},{"type":3,"start":6,"end":14,"href":"https://ai-alignment.com/universality-and-consequentialism-within-hch-c0bee00365bd","title":"","rel":"","anchorType":0},{"type":3,"start":15,"end":20,"href":"https://ai-alignment.com/universality-and-model-based-rl-b08701394ddd","title":"","rel":"","anchorType":0}]},{"name":"429b","type":1,"text":"Iâ€™m using the awkward term â€œascription universalityâ€ partly to explicitly flag that this is a preliminary definition, and partly to reserve linguistic space for the better definitions that Iâ€™m optimistic will follow.","markups":[]},{"name":"05ca","type":1,"text":"(Thanks to Geoffrey Irving for discussions about many of the ideas in this post.)","markups":[]},{"name":"13a9","type":3,"text":"I. Definition","markups":[]},{"name":"3ad6","type":1,"text":"We will try to define what it means for a question-answering system A to be â€œascription universal.â€","markups":[{"type":1,"start":68,"end":69}]},{"name":"dc32","type":13,"text":"1. Ascribing beliefs to A","markups":[]},{"name":"d07e","type":1,"text":"Fix a language (e.g. English with arbitrarily big compound terms) in which we can represent questions and answers.","markups":[{"type":3,"start":59,"end":64,"href":"https://ai-alignment.com/approval-directed-algorithm-learning-bf1f8fad42cd","title":"","rel":"noopener","anchorType":0}]},{"name":"beab","type":1,"text":"To ascribe beliefs to A, we ask it. If A(â€œare there infinitely many twin primes?â€) = â€œprobably, though itâ€™s hard to be sureâ€ then we ascribe that belief about twin primes to A.","markups":[{"type":1,"start":22,"end":23},{"type":1,"start":39,"end":40},{"type":1,"start":174,"end":175}]},{"name":"ef03","type":1,"text":"This is not a general way of ascribing â€œbelief.â€ This procedure wouldnâ€™t capture the beliefs of a native Spanish speaker, or for someone who wasnâ€™t answering questions honestly. But it can give us a sufficient condition, and is particularly useful for someone who wants to use A as part of an alignment scheme.","markups":[{"type":1,"start":277,"end":278}]},{"name":"88ac","type":1,"text":"Even in this â€œstraightforwardâ€ procedure there is a lot of subtlety. In some cases there are questions that we canâ€™t articulate in our language, but which (when combined with Aâ€™s other beliefs) have consequences that we can articulate. In this case, we can infer something about Aâ€™s beliefs from its answers to the questions that we can articulate.","markups":[{"type":1,"start":175,"end":176},{"type":1,"start":279,"end":280}]},{"name":"b8f3","type":13,"text":"2. Ascribing beliefs to arbitrary computations","markups":[]},{"name":"91fd","type":1,"text":"We are interested in whether A â€œcan understand everything that could be understood by someone.â€ To clarify this, we need to be more precise about what we mean by â€œcould be understood by someone.â€","markups":[{"type":1,"start":29,"end":30}]},{"name":"a5f2","type":1,"text":"This will be the most informal step in this post. (Not that any of it is very formal!)","markups":[]},{"name":"e68f","type":1,"text":"We can imagine various ways of ascribing beliefs to an arbitrary computation C. For example:","markups":[{"type":1,"start":77,"end":78},{"type":2,"start":40,"end":41}]},{"name":"b7d3","type":9,"text":"We can give C questions in a particular encoding and assume its answers reflect its beliefs. We can either use those answers directly to infer Câ€™s beliefs (as in the last section), or we can ask what set of beliefs about latent facts would explain Câ€™s answers.","markups":[{"type":1,"start":12,"end":13},{"type":1,"start":143,"end":144},{"type":1,"start":248,"end":249}]},{"name":"f1e9","type":9,"text":"We can view C as optimizing something and ask what set of beliefs rationalize that optimization. For example, we can give C a chess board as input, see what move it produces, assume it is trying to win, and infer what it must believe. We might conclude that C believes a particular line of play will be won by black, or that C believes general heuristics like â€œa pawn is worth 3 tempi,â€ or so on.","markups":[{"type":1,"start":12,"end":13},{"type":1,"start":122,"end":123},{"type":1,"start":258,"end":259},{"type":1,"start":325,"end":326},{"type":2,"start":27,"end":28},{"type":2,"start":41,"end":42}]},{"name":"ed07","type":9,"text":"We can reason about how Câ€™s behavior depends on facts about the world, and ask what state of the world is determined by its current behavior. For example, we can observe that C(113327) = 1 but that C(113327) â€œwould have beenâ€ 0 if 113327 had been composite, concluding that C(11327) â€œknowsâ€ that 113327 is prime. We can extend to probabilistic beliefs, e.g. if C(113327) â€œprobablyâ€ would have been 0 if 113327 had been composite, then we might that C knows that 113327 is â€œprobably prime.â€ This certainly isnâ€™t a precise definition, since it involves considering logical counterfactuals, and Iâ€™m not clear whether it can be made precise. (See also ideas along the lines of â€œknowledge is freedomâ€.)","markups":[{"type":3,"start":673,"end":695,"href":"https://www.lesswrong.com/posts/b3Bt9Cz4hEtR26ANX/knowledge-is-freedom","title":"","rel":"","anchorType":0},{"type":1,"start":24,"end":25},{"type":1,"start":175,"end":176},{"type":1,"start":198,"end":199},{"type":1,"start":274,"end":275},{"type":1,"start":361,"end":362},{"type":1,"start":449,"end":450}]},{"name":"a8d3","type":9,"text":"If a computation behaves differently under different conditions, then we could use restrict attention to a particular condition. For example, if a question-answering system appears to be bilingual but answers questions differently in Spanish and English, we could ascribe two different sets of beliefs. Similarly, we could ascribe beliefs to any subcomputation. For example, if a part of C can be understood as optimizing the way data is laid out in memory, then we can ascribe beliefs to that computation about the way that data will be used.","markups":[{"type":1,"start":388,"end":390}]},{"name":"320c","type":1,"text":"Note that these arenâ€™t intended to be efficient procedures that we could actually apply to a given computation C. They are hypothetical procedures that we will use to define what it means for A to be universal.","markups":[{"type":1,"start":111,"end":112},{"type":1,"start":192,"end":193}]},{"name":"168b","type":1,"text":"Iâ€™m not going to try to ascribe a single set of beliefs to a given computation; instead, Iâ€™ll consider all of the reasonable ascription procedures. For example, I think different procedures would ascribe different beliefs to a particular human, and donâ€™t want to claim there is a unique answer to what a human â€œreallyâ€ believes. A universal reasoner needs to have more reasonable beliefs than the beliefs ascribed to that a human using any particular method.","markups":[]},{"name":"5662","type":1,"text":"An ascription-universal reasoner needs to compete with any beliefs that can be ascribed to C, so I want to be generous with this definition. For example, given a chess-playing algorithm, we might rationalize it as trying to win a game and infer its beliefs about the rules of chess. Or we might rationalize it as trying to look like a human and infer its beliefs about what a human would do. Or something different altogether. Most of these will be kind of crazy ascriptions, but I want to compete with them anyway (competing with crazier beliefs will turn out to just be easier).","markups":[{"type":1,"start":91,"end":92}]},{"name":"8cb8","type":1,"text":"Itâ€™s not totally clear what counts as a â€œreasonableâ€ ascription procedure, and thatâ€™s the biggest source of informality. Intuitively, the key property is that the ascription itself isnâ€™t doing the â€œhard work.â€ In practice Iâ€™m using an informal extensional definition, guided by examples like those in the bulleted list.","markups":[]},{"name":"2960","type":13,"text":"3. Comparing beliefs","markups":[]},{"name":"295c","type":1,"text":"What does it mean to say that one agent is â€œbetter-informedâ€ than another?","markups":[]},{"name":"8fe9","type":1,"text":"Itâ€™s natural to try to express this in terms of empirical information about the world, but we are particularly interested in the different inferences that agents are able to draw from the same data. Another natural approach is to compare their â€œknowledge,â€ but I have no idea how to define knowledge or justified belief. So Iâ€™m reduced to working directly with sets of beliefs.","markups":[{"type":2,"start":149,"end":150}]},{"name":"0358","type":1,"text":"Consider two sets of beliefs, described by the subjective expectations ğ”¼Â¹ and ğ”¼Â². What does it mean to say that ğ”¼Â¹ is better-informed than ğ”¼Â²?","markups":[]},{"name":"d7eb","type":1,"text":"This framing makes it tempting to try something simple: â€œfor every quantity, ğ”¼Â¹â€™s belief about that quantity is more accurate.â€ But this is property is totally unachievable. Even if ğ”¼Â¹ is obtained by conditioning ğ”¼Â² on a true fact, it will almost certainly happen to update in the â€œwrongâ€ direction for some claims.","markups":[]},{"name":"f24e","type":1,"text":"We will instead use a subjective definition, i.e. weâ€™ll define this concept from a particular epistemic position represented by another subjective expectation ğ”¼.","markups":[]},{"name":"33c1","type":1,"text":"Then we say that ğ”¼Â¹ dominates ğ”¼Â² (w.r.t. ğ”¼) if, for every bounded quantity X and for every â€œniceâ€ property Î¦:","markups":[{"type":1,"start":21,"end":31}]},{"name":"021e","type":9,"text":"ğ”¼[X|Î¦(ğ”¼Â¹, ğ”¼Â²)] = ğ”¼[ğ”¼Â¹[X]|Î¦(ğ”¼Â¹, ğ”¼Â²)]","markups":[]},{"name":"bdbe","type":1,"text":"(By â€œniceâ€ I mean something like: simple to define and open in the product topology, viewing ğ”¼Â¹ and ğ”¼Â² as infinite tables of numbers.)","markups":[]},{"name":"9d25","type":1,"text":"Intuitively, this means that ğ”¼ always â€œtrustsâ€ ğ”¼Â¹, even if given arbitrary information about ğ”¼Â¹ and ğ”¼Â². For example, if ğ”¼ was told that ğ”¼Â¹[X] â‰ˆ x and\nğ”¼Â²[X] â‰ˆ y, then it would expect X to be around x (rather than y). Allowing arbitrary predicates Î¦ allows us to make stronger inferences, effectively that ğ”¼ thinks that ğ”¼Â¹ captures everything useful about ğ”¼Â².","markups":[{"type":2,"start":150,"end":151},{"type":2,"start":165,"end":166},{"type":2,"start":204,"end":205},{"type":2,"start":219,"end":220},{"type":2,"start":221,"end":223},{"type":2,"start":339,"end":349}]},{"name":"2809","type":1,"text":"Iâ€™m not sure if this is exactly the right property, and it becomes particularly tricky if the quantity X is itself related to the behavior of ğ”¼Â¹ or ğ”¼Â² (continuity in the product topology is the minimum plausible condition to avoid a self-referential paradox). But I think itâ€™s at least roughly what we want and it may be exactly what we want.","markups":[]},{"name":"5a96","type":1,"text":"Note that dominance is subjective, i.e. it depends on the epistemic vantage point ğ”¼ used for the outer expectation. This property is a little bit stronger than what we originally asked for, since it also requires ğ”¼ to trust ğ”¼Â¹, but this turns out to be implied anyway by our definition of universality so itâ€™s not a big defect.","markups":[{"type":2,"start":23,"end":33}]},{"name":"49ff","type":1,"text":"Note that dominance is a property of the descriptions of ğ”¼Â¹ and ğ”¼Â². There could be two different computations that in fact compute the same set of expectations, such that ğ”¼ trusts one of them but not the other. Perhaps one computation hard-codes a particular result, while the other does a bunch of work to estimate it. Even if the hard-coded result happened to be correct, such that the two computations had the same outputs, ğ”¼ might trust the hard work but not the wild guess.","markups":[{"type":2,"start":41,"end":53}]},{"name":"9acc","type":13,"text":"4. Complexity and parameterization","markups":[]},{"name":"df2a","type":1,"text":"There are computations with arbitrarily sophisticated beliefs, so no fixed A can hope to dominate everything. To remedy this, rather than comparing to a fixed question-answerer A, weâ€™ll compare to a parameterized family A[C].","markups":[{"type":1,"start":75,"end":76},{"type":1,"start":177,"end":178},{"type":1,"start":220,"end":221},{"type":1,"start":222,"end":223}]},{"name":"3982","type":1,"text":"Iâ€™ll consider two different kinds of potentially-universal reasoners A:","markups":[{"type":1,"start":69,"end":70}]},{"name":"41cc","type":9,"text":"In the â€œidealizedâ€ case, A[C] depends only on the complexity of C.\nFor example, we might hope that an n-round debate dominates any beliefs that could be ascribed to a fast computation with (n-1) rounds of alternation. In particular, this A[C] is the same for any two computations C of the same complexity.","markups":[{"type":3,"start":205,"end":216,"href":"https://en.wikipedia.org/wiki/Alternating_Turing_machine","title":"","rel":"noopener","anchorType":0},{"type":1,"start":25,"end":26},{"type":1,"start":27,"end":28},{"type":1,"start":64,"end":65},{"type":1,"start":238,"end":239},{"type":1,"start":240,"end":241},{"type":1,"start":280,"end":281},{"type":2,"start":102,"end":103},{"type":2,"start":190,"end":191}]},{"name":"2da1","type":9,"text":"In the â€œpracticalâ€ case, A[C] depends on the complexity of C but also uses the computation C as a hint. For example, if C is the training process for a neural net, then we might take A[C] to be a debate in which the debaters are able to share weights and activations with the neural net throughout the entire training process.","markups":[{"type":1,"start":24,"end":26},{"type":1,"start":27,"end":28},{"type":1,"start":59,"end":60},{"type":1,"start":91,"end":92},{"type":1,"start":120,"end":121},{"type":1,"start":183,"end":184},{"type":1,"start":185,"end":186}]},{"name":"e749","type":1,"text":"Iâ€™m generally interested in the case where A[C] is only slightly more powerful than C itself. This mirrors the setting where a universal Turing machine is able to run any other Turing machine with only a modest slowdown.","markups":[{"type":1,"start":43,"end":44},{"type":1,"start":45,"end":46},{"type":1,"start":84,"end":85}]},{"name":"4844","type":13,"text":"Putting it all together","markups":[]},{"name":"b5e0","type":1,"text":"We say that a set of beliefs ğ”¼á´¬ epistemically dominates a computation C (w.r.t. some beliefs ğ”¼ and language L) if the beliefs ascribed to A by the â€œstraightforwardâ€ procedure, using L, dominate (w.r.t. ğ”¼) the beliefs ascribed to C by any reasonable ascription procedure.","markups":[{"type":1,"start":71,"end":73},{"type":1,"start":232,"end":233},{"type":2,"start":33,"end":56}]},{"name":"9c1b","type":1,"text":"We say that a family of question-answering systems A[Â·] are ascription universal (w.r.t. ğ”¼ and L) if A[C] epistemically dominates C for every computation C.","markups":[{"type":1,"start":51,"end":52},{"type":1,"start":53,"end":54},{"type":1,"start":102,"end":103},{"type":1,"start":104,"end":105},{"type":1,"start":131,"end":132},{"type":1,"start":155,"end":156},{"type":2,"start":60,"end":80}]},{"name":"3398","type":3,"text":"II. Discussion","markups":[]},{"name":"a216","type":13,"text":"Why is (subjective) dominance sufficient?","markups":[]},{"name":"5e85","type":1,"text":"This universality condition requires that we believe that A[C] is better-informed than C. Naively we might have wanted it to actually be the case that A[C] is better-informed than C; the stronger condition is clearly unachievable, but why should we be satisfied with the weaker condition?","markups":[{"type":1,"start":58,"end":59},{"type":1,"start":60,"end":61},{"type":1,"start":87,"end":88},{"type":1,"start":151,"end":152},{"type":1,"start":153,"end":154},{"type":1,"start":180,"end":181},{"type":2,"start":42,"end":44},{"type":2,"start":125,"end":133}]},{"name":"d8e8","type":1,"text":"In applications of this property, the subjective condition is what we need in order for us to believe that A[C] will cope with the challenges posed by C. For example, suppose that C formulates a plan to â€œtrickâ€ A[C]. Then the subjective universality condition implies that we donâ€™t expect C to succeed.","markups":[{"type":3,"start":3,"end":15,"href":"https://ai-alignment.com/informed-oversight-18fcb5d3d1e1","title":"","rel":"","anchorType":0},{"type":3,"start":16,"end":23,"href":"https://ai-alignment.com/universality-and-consequentialism-within-hch-c0bee00365bd","title":"","rel":"","anchorType":0},{"type":3,"start":24,"end":32,"href":"https://ai-alignment.com/universality-and-model-based-rl-b08701394ddd","title":"","rel":"","anchorType":0},{"type":1,"start":107,"end":108},{"type":1,"start":109,"end":110},{"type":1,"start":151,"end":152},{"type":1,"start":180,"end":181},{"type":1,"start":211,"end":212},{"type":1,"start":213,"end":214},{"type":1,"start":289,"end":290},{"type":2,"start":88,"end":91}]},{"name":"71dd","type":1,"text":"This isnâ€™t as good as actually knowing that C wonâ€™t succeed. But I think it should be good enough for us â€” the reason we are thinking about AI safety is because we are concerned that something bad will happen. If we find a technique that defuses this argument, then weâ€™ve addressed the motivating problem. It may still be the case that bad things happen (and we should still search for additional reasons that bad things might happen), but we donâ€™t particularly expect them to.","markups":[{"type":1,"start":44,"end":45}]},{"name":"eb3b","type":1,"text":"Of course if you select over a large number of computations, then you may find one that will succeed in tricking A. But if we are concerned about that, then we can instead apply ascription universality to the entire process including the selection.","markups":[{"type":1,"start":113,"end":114}]},{"name":"1361","type":13,"text":"Why trust opaque computation?","markups":[]},{"name":"9535","type":1,"text":"If C uses some clever heuristics that I donâ€™t understand, then Câ€™s â€œbeliefsâ€ might be excellent, but I might not expect them to be excellent. In this sense understanding may seem almost vacuous. If there is some heuristic that I trust, wouldnâ€™t A just use it?","markups":[{"type":1,"start":3,"end":4},{"type":1,"start":63,"end":64},{"type":1,"start":245,"end":247},{"type":2,"start":113,"end":119}]},{"name":"80aa","type":1,"text":"To see why the definition is demanding, consider the special case where C performs an extensive search to find a computation that works well empirically. For example, C might be the following computation:","markups":[{"type":1,"start":72,"end":73},{"type":1,"start":167,"end":168}]},{"name":"0fa2","type":9,"text":"Start with a training set of (image, label) pairs.","markups":[]},{"name":"790d","type":9,"text":"Search over simple programs to find one that makes good predictions.","markups":[]},{"name":"e464","type":9,"text":"Run that simple program on a new image to predict its label.","markups":[]},{"name":"1b49","type":1,"text":"In this case, we can ascribe beliefs to C about the contents of the new image. And because those beliefs are coming from a simple program that works empirically, I expect them to be accurate (in some respects).","markups":[{"type":1,"start":40,"end":41}]},{"name":"a800","type":1,"text":"For example, a simple classifier C may â€œbelieveâ€ that the new image contains a particular curve that typically appears in images labeled â€œdog;â€ or a really sophisticated classifier may perform complex deductions about the contents of the scene, starting from premises that were empirically validated on the training set.","markups":[{"type":1,"start":33,"end":34}]},{"name":"440a","type":1,"text":"So itâ€™s not OK for A to simply ignore whatever heuristics C is using â€” if those heuristics have the kind of empirical support that makes us think they actually work, then A needs to be able to understand everything that those heuristics imply about the domain.","markups":[{"type":1,"start":19,"end":20},{"type":1,"start":58,"end":59}]},{"name":"d06b","type":13,"text":"Why be so general?","markups":[]},{"name":"877a","type":1,"text":"Iâ€™ve formulated universality as competing with arbitrary computations C. It seems totally possible that the form of C discussed in the last section â€” searching for a program that works well in practice and then using it in a new situation â€” is so central that the definition of universality should focus entirely on it.","markups":[{"type":1,"start":70,"end":71},{"type":1,"start":116,"end":118}]},{"name":"ec32","type":1,"text":"One reason to use the broader definition is because sometimes this â€œselectionâ€ process can be embedded in a non-trivial way in a larger computation. For example, if I have a sufficiently large group of humans, I might expect memetic selection to occur and produce systems that could be said to have â€œbeliefs,â€ and Iâ€™d like universal systems to dominate those beliefs as well.","markups":[]},{"name":"6699","type":1,"text":"The other reason to use this very general definition is because I donâ€™t see an easy way to simplify the definition by using the additional structural assumption about C. I do think itâ€™s likely thereâ€™s a nicer statement out there that someone else can find.","markups":[{"type":1,"start":167,"end":168}]},{"name":"35b2","type":13,"text":"Universal from whose perspective?","markups":[]},{"name":"3d18","type":1,"text":"Unfortunately, achieving universality depends a lot on the epistemic perspective ğ”¼ from which it is being evaluated. For example, if ğ”¼ knows any facts, than a universal agent must know all of those facts as well. Thus â€œa debate judged by Paulâ€ may be universal from Paulâ€™s perspective, but â€œa debate arbitrated by Aliceâ€ cannot be universal from my perspective unless I believe that Alice knows everything I know.","markups":[]},{"name":"2187","type":1,"text":"This isnâ€™t necessarily a big problem. It will limit us to conclusions like: Google engineers believe that the AI theyâ€™ve built serves the userâ€™s interests reasonably well. The user might not agree with that assessment, if they have different beliefs from Google engineers. This is what youâ€™d expect in any case where Google engineers build a product, however good their intentions.","markups":[]},{"name":"3299","type":1,"text":"(Of course Google engineersâ€™ notion of â€œserving the userâ€™s interestsâ€ can involve deferring to the userâ€™s beliefs in cases where they disagree with Google engineers, just as they could defer to the userâ€™s beliefs with other products. That gives us reason to be less concerned about such divergences, but eventually these evaluations do need to bottom out somewhere.)","markups":[]},{"name":"1a5f","type":1,"text":"This property becomes more problematic when we ask questions like: is there a way to seriously limit the inputs and outputs to a human while preserving universality of HCH? This causes trouble because even if limiting the human intuitively preserves universality, it will effectively eliminate some of the humanâ€™s knowledge and know-how that can only be accessed on large inputs, and hence violate universality.","markups":[{"type":3,"start":85,"end":171,"href":"https://ai-alignment.com/universality-and-security-amplification-551b314a3bab","title":"","rel":"noopener","anchorType":0},{"type":3,"start":346,"end":378,"href":"https://medium.com/@weidai/to-put-it-another-way-a-human-translator-has-learned-a-lot-of-valuable-information-much-of-it-48457f95b9bf","title":"","rel":"","anchorType":0}]},{"name":"aeac","type":1,"text":"So when investigating schemes based on this kind of impoverished human, we would need to evaluate universality from some impoverished epistemic perspective. Weâ€™d like to say that the impoverished perspective is still â€œgood enoughâ€ for us to feel safe, despite not being good enough to capture literally everything we know. But now we risk begging the question: how do we evaluate whether the impoverished perspective is good enough? I think this is probably OK, but itâ€™s definitely subtle.","markups":[]},{"name":"691f","type":1,"text":"I think that defining universality w.r.t. ğ”¼ is an artifact of this definition strategy, and Iâ€™m optimistic that a better definition wouldnâ€™t have this dependence, probably by directly attacking the notion of â€œjustifiedâ€ belief (which would likely also be useful for actually establishing universality, and may even be necessary). But thatâ€™s a hard problem. Philosophers have thought about very similar problems extensively without making the kind of progress that seems adequate for our purposes, and I donâ€™t see an immediate angle of attack.","markups":[]},{"name":"1f08","type":3,"text":"III. Which A might be universal?","markups":[]},{"name":"f5a5","type":13,"text":"Two regimes","markups":[]},{"name":"8e1b","type":1,"text":"Iâ€™m interested in universality in two distinct regimes:","markups":[]},{"name":"93c1","type":9,"text":"Universality of idealized procedures defined in terms of perfect optimization, such as debate under optimal play or max-HCH, where A[C] depends only the computational complexity of C.","markups":[{"type":3,"start":87,"end":93,"href":"https://arxiv.org/abs/1805.00899","title":"","rel":"","anchorType":0},{"type":3,"start":116,"end":123,"href":"https://ai-alignment.com/humans-consulting-hch-f893f6051455","title":"","rel":"","anchorType":0},{"type":1,"start":131,"end":132},{"type":1,"start":133,"end":134},{"type":1,"start":181,"end":182}]},{"name":"93ce","type":9,"text":"Universality of practical variants, in which the perfect optimization is replaced by a bounded optimization, which might use the computation C as a â€œhint.â€ For example, we might consider amplification or debate where the agents are allowed to inspect a transcript of C; or if C is a neural net training process, we might train them jointly with C.","markups":[{"type":1,"start":141,"end":142},{"type":1,"start":267,"end":268},{"type":1,"start":276,"end":277},{"type":1,"start":345,"end":346}]},{"name":"c90e","type":13,"text":"Idealized models","markups":[]},{"name":"a0bd","type":1,"text":"A[C] could be:","markups":[{"type":1,"start":0,"end":1},{"type":1,"start":2,"end":3}]},{"name":"55ee","type":9,"text":"A debate, judged by a human, with perfect debaters, whose length depends on the complexity of C. (Hopefully in such a way that A[A[C]] is only a slightly longer debate than A[C].)","markups":[{"type":1,"start":1,"end":2},{"type":1,"start":94,"end":95},{"type":1,"start":127,"end":128},{"type":1,"start":129,"end":130},{"type":1,"start":131,"end":132},{"type":1,"start":173,"end":174},{"type":1,"start":175,"end":176}]},{"name":"2eb2","type":9,"text":"max-HCH, using perfect optimization rather than a bounded optimizer. Again, we can impose a budget limit that depends on the complexity of C, ideally such that A[A[C]] does not involve a much larger computation than A[C].","markups":[{"type":1,"start":139,"end":140},{"type":1,"start":160,"end":161},{"type":1,"start":162,"end":163},{"type":1,"start":164,"end":165},{"type":1,"start":216,"end":217},{"type":1,"start":218,"end":219}]},{"name":"95fd","type":9,"text":"Use HCH to implement two functions, Info and Answer. Define A[C](Q) to be Answer(Q, a*), where a* = argmax Info(a, Q), and the optimization is over advice strings a of length comparable to the description of C.","markups":[{"type":1,"start":60,"end":61},{"type":1,"start":62,"end":63},{"type":1,"start":208,"end":209},{"type":2,"start":84,"end":86},{"type":2,"start":87,"end":89},{"type":2,"start":94,"end":100},{"type":2,"start":112,"end":113},{"type":2,"start":116,"end":119},{"type":2,"start":163,"end":164}]},{"name":"10ec","type":1,"text":"I donâ€™t think any of these would be universal if instantiated with an open-ended human policy â€” providing arbitrary complex inputs to a human is likely to lead to bad behavior. Instead, I imagine restricting the human to some small enough space of inputs that they can behave â€œreasonablyâ€ over the whole space, while still being rich enough to be universal. Even as such, I think additional measures might be needed to achieve security (since a team of humans may behave badly in some exotic corners of the input space, even if individual humans are sheltered from exotic inputs).","markups":[{"type":3,"start":380,"end":399,"href":"https://ai-alignment.com/universality-and-consequentialism-within-hch-c0bee00365bd","title":"","rel":"","anchorType":0}]},{"name":"c396","type":13,"text":"Practical models","markups":[]},{"name":"9ebe","type":1,"text":"In practice Iâ€™m particularly interested in the case where C has the special form described in the â€œTrusting opaque computationsâ€ section: it trains a neural net on some dataset, and then runs that neural network on new inputs.","markups":[{"type":1,"start":58,"end":59}]},{"name":"aec8","type":1,"text":"A[C] could be:","markups":[{"type":1,"start":0,"end":1},{"type":1,"start":2,"end":3}]},{"name":"b309","type":9,"text":"Debates arbitrated by a human where the debaters are trained jointly with C, and can make references to individual examples from the dataset.","markups":[{"type":1,"start":74,"end":75}]},{"name":"2394","type":9,"text":"max-HCH where the agent is trained jointly with C, and the overseer can consult individual entries from the dataset.","markups":[{"type":1,"start":48,"end":49}]},{"name":"a03c","type":9,"text":"HCH, combined with interpretability techniques that allow it to get information about what C is thinking and to relate its behavior to the dataset on which it was trained.","markups":[{"type":1,"start":91,"end":92}]},{"name":"446c","type":1,"text":"For each of these practical variants, I donâ€™t expect it to be universal unless the corresponding idealized model is. So we can think of dividing the universality question into two pieces:","markups":[]},{"name":"2a27","type":9,"text":"Would our procedure be universal if the optimization was performed perfectly?","markups":[{"type":2,"start":33,"end":76}]},{"name":"08f2","type":9,"text":"Is weight sharing / interpretability strong enough to solve the optimization problems needed for universality?","markups":[]},{"name":"11e5","type":1,"text":"I think none of the suggestions in this section are plausibly universal without some additional mechanisms, and so even if HCH or debate is universal, additional ideas are needed in order to translate those to practical universal systems. Iâ€™m still inclined to focus on the idealized case, because I want to better understand what is needed for universality without thinking about algorithmic difficulties.","markups":[]},{"name":"237f","type":3,"text":"IV. Which C are hard to epistemically dominate?","markups":[]},{"name":"ef7b","type":1,"text":"The definition in this post was somewhat informal, but the discussion in this appendix will be extremely informal. The purpose is to communicate some of my early thinking about universality and my sense for what a universal reasoner might look like.","markups":[]},{"name":"43f4","type":1,"text":"The first three examples will consider programs C with the following form:","markups":[{"type":1,"start":48,"end":49}]},{"name":"9984","type":9,"text":"Iterate over programs to find an f that successfully predicts the answers to some dataset of historical questions for which we know the answer.","markups":[{"type":2,"start":33,"end":34}]},{"name":"50cb","type":9,"text":"Use that program to predict the answers to future questions.","markups":[]},{"name":"778b","type":1,"text":"The different examples explore features of f that might make it difficult to dominate the overall computation.","markups":[{"type":2,"start":43,"end":44}]},{"name":"f14f","type":13,"text":"Deduction","markups":[]},{"name":"a24a","type":1,"text":"Suppose f takes the following form:","markups":[{"type":2,"start":8,"end":9}]},{"name":"63e7","type":9,"text":"Given a question and context, it performs a short series of deductions from facts in the context, together with some axioms (e.g. â€œBobâ€™s eyes are more likely to twitch when he is lying,â€ the rules of poker, etc.)","markups":[{"type":2,"start":207,"end":211}]},{"name":"19b9","type":9,"text":"When it arrives at answers that constrain the possible answers to the given question, it outputs an answer consistent with those constraints.","markups":[]},{"name":"cf96","type":1,"text":"We could attribute the statements produced by this process to C as beliefs. We expect this set of beliefs to have useful information because it was selected for making correct predictions.","markups":[{"type":1,"start":62,"end":64},{"type":1,"start":75,"end":76}]},{"name":"11bf","type":1,"text":"Fortunately, if HCH is able to see the training dataset then it may be able to arrive at similarly accurate views:","markups":[]},{"name":"18b2","type":9,"text":"It can compute that P(observations | axioms) is high by following along with the deductive process used by f .","markups":[{"type":2,"start":107,"end":110}]},{"name":"ae06","type":9,"text":"Bayes implies that the axioms are likely to be true (fully capturing the argument which leads us, from our current perspective, to expect them to be true).","markups":[]},{"name":"d11d","type":9,"text":"Once HCH knows the axioms are likely to be true, it can follow along with the deductive process to reproduce all of fâ€™s beliefs.","markups":[{"type":2,"start":116,"end":117}]},{"name":"d033","type":13,"text":"Modeling","markups":[]},{"name":"39df","type":1,"text":"Suppose f takes the following form:","markups":[{"type":2,"start":8,"end":9}]},{"name":"ff93","type":9,"text":"Given a question Q and context, set up a relevant physical situation.","markups":[]},{"name":"dea7","type":9,"text":"Use a dynamics model M to predict what will happen at some future time.","markups":[]},{"name":"6650","type":9,"text":"Use the resulting state to answer Q.","markups":[]},{"name":"86af","type":1,"text":"We could say that C believes everything implied by the outcome of this physical simulation. We expect these beliefs to be accurate because f has predicted well in the past.","markups":[{"type":1,"start":18,"end":19},{"type":2,"start":139,"end":140}]},{"name":"04e1","type":1,"text":"As in the deduction case, hopefully HCH can compute that P(observations|M is accurate) is high, since it can also compute the consequences of M. Then Bayes implies the dynamics model is accurate, and HCH can use that model to compute physical states.","markups":[]},{"name":"7f69","type":1,"text":"Inferring all the beliefs from a dynamics model is not trivial though. As an extreme example, if f is performing an atom-by-atom simulation of a room, and that room contains Alice and Bob, then we could ascribe extensive beliefs about Alice and Bob to the computation C.","markups":[{"type":1,"start":268,"end":269},{"type":2,"start":97,"end":99}]},{"name":"2cd4","type":1,"text":"(Here we run head on into the fuzziness about what counts as a â€œreasonableâ€ ascription procedure, but for the moment Iâ€™ll assume that some reasonable procedure ascribes beliefs about Alice and Bob to the computation.)","markups":[]},{"name":"8002","type":1,"text":"To compete with these ascriptions, HCH needs to infer those high-level beliefs about Alice and Bob from the low-level computation involving atoms. One way to do this is to search over possible â€œbridgingâ€ hypotheses that relate low-level physical facts to high-level facts about the environment. If such a hypothesis can explain additional high-level facts, then a Bayesian can learn that it is true. Similarly, if the bridging hypothesis relates facts about the model to constraints we know from the high-level interpretation, then the Bayesian can potentially use that as evidence. (This kind of reasoning will be discussed in a bit more detail in the next section.)","markups":[]},{"name":"887f","type":1,"text":"We could further hope that searching for a bridging hypothesis isnâ€™t much harder than performing the original search over low-level physics, given that the low-level physics needed to explain a bunch of high-level facts and so already must encode some part of that correspondence.","markups":[]},{"name":"fda6","type":1,"text":"(Note that the â€œdeductionâ€ example in the previous case could also involve alien concepts or models, in which case the same kind of work would be needed.)","markups":[]},{"name":"3adb","type":13,"text":"Alien reasoning","markups":[]},{"name":"d82c","type":1,"text":"In the previous section we described two styles of reasoning we already understand. But there are probably many kinds of reasoning that work well in practice but that would be more alien, and those might be more challenging. This section will explore one example in some detail to try to help anchor our reasoning about the general phenomenon. It will also elaborate on some of the reasoning about â€œbridgingâ€ hypotheses mentioned in the last section.","markups":[]},{"name":"70ed","type":1,"text":"Suppose that our predictions are always of the same form (e.g. what is the probability the stock market will go up today), and f works as follows (the details are long but not very important):","markups":[{"type":2,"start":127,"end":128}]},{"name":"4823","type":9,"text":"Find the PSD matrix A with maximum log determinant subject to the constraints in the next bullet points, then output the (0, 0) entry.","markups":[]},{"name":"47cf","type":9,"text":"There is an implicit correspondence between the rows/columns of A, and some uncertain properties X(0), X(1), X(2), â€¦. (which weâ€™ll view as 0â€“1 variables), where X(0) is the property we want to forecast.","markups":[]},{"name":"3e06","type":9,"text":"If the (i, j) entry of A represented the expectation E[X(i)X(j)], then the matrix would necessarily satisfy a bunch of constraints, which we impose A. For example:","markups":[{"type":2,"start":8,"end":9},{"type":2,"start":10,"end":12},{"type":2,"start":57,"end":58},{"type":2,"start":61,"end":62}]},{"name":"1c22","type":9,"text":"If the context implies that X(i) = 1, then E[X(i)X(j)] = E[X(j)] = E[X(j)Â²], so A(i, j) = A(j, j).","markups":[{"type":2,"start":30,"end":31},{"type":2,"start":47,"end":48},{"type":2,"start":51,"end":52},{"type":2,"start":61,"end":62},{"type":2,"start":71,"end":72},{"type":2,"start":82,"end":83},{"type":2,"start":84,"end":86},{"type":2,"start":92,"end":93},{"type":2,"start":95,"end":96}]},{"name":"9b02","type":9,"text":"If X(i) and X(j) together imply X(k), then we must have E[X(i)X(j)] â‰¤ E[X(i)X(k)] and hence A(i, j) â‰¤ A(i, k).","markups":[{"type":2,"start":5,"end":6},{"type":2,"start":14,"end":15},{"type":2,"start":34,"end":35},{"type":2,"start":60,"end":61},{"type":2,"start":64,"end":65},{"type":2,"start":74,"end":75},{"type":2,"start":78,"end":79},{"type":2,"start":94,"end":95},{"type":2,"start":97,"end":98},{"type":2,"start":104,"end":105},{"type":2,"start":107,"end":108}]},{"name":"5f2e","type":9,"text":"For any constants a, b, â€¦, E[(a X(1) + b X(2) + â€¦ )Â²] â‰¥ 0 â€” i.e., the matrix A must be PSD.","markups":[{"type":2,"start":18,"end":19},{"type":2,"start":21,"end":22},{"type":2,"start":30,"end":31},{"type":2,"start":39,"end":40}]},{"name":"3e8c","type":1,"text":"The chosen matrix A(opt) corresponds to a set of beliefs about the propositions X(i), and we can ascribe these beliefs to C. Because f predicts well, we again expect these beliefs to say something important about the world.","markups":[{"type":1,"start":122,"end":123},{"type":2,"start":82,"end":83},{"type":2,"start":133,"end":134}]},{"name":"38f1","type":1,"text":"I chose this procedure f in part because we can give a kind of argument for why the matrix A(opt) should tend to encode accurate beliefs. But I donâ€™t think that a universal reasoner can make use of that argument:","markups":[{"type":2,"start":23,"end":25}]},{"name":"768e","type":9,"text":"Finding the argument that f works is an additional problem, beyond finding f itself, which might be much harder.","markups":[{"type":2,"start":26,"end":27},{"type":2,"start":75,"end":76}]},{"name":"9240","type":9,"text":"A comprehensible version of that argument may be much larger than the strategy itself, so even in the idealized cases like debate with perfect optimization, we may need to increase the scale.","markups":[]},{"name":"a083","type":9,"text":"I donâ€™t expect that all â€œgoodâ€ reasoning strategies have clean understandable arguments in their favor (and even in this case, if it the scheme worked well it would be largely an empirical fact rather than a consequence of the simple theorems we could prove). I think this kind of example is useful because we can easily imagine a human debate judge not having the argument while still being apparently universal. This makes it a useful analogy for cases where the argument really doesnâ€™t exist.","markups":[]},{"name":"386b","type":1,"text":"Instead, I think a universal reasoner needs to be able to infer the efficacy of this reasoning procedure from its empirical success. Itâ€™s relatively easy for a Bayesian to learn the regularity â€œf makes good predictions.â€ Recovering the rest of the matrix A, and learning how to interpret and whether to trust them, is the hard part.","markups":[{"type":2,"start":194,"end":195}]},{"name":"20cb","type":1,"text":"This is going to require the same kind of bridging/identification we discussed in the last section. Letâ€™s write X(A) for the set of beliefs about the world implied by the â€œintendedâ€ identification. Searching over possible identifications to find X (or something like it) is the only way we can ever relate the rows of A to the quantities X(i). Again, we can hope that it isnâ€™t much harder than finding the original reasoning procedure.","markups":[{"type":2,"start":340,"end":341}]},{"name":"a9e4","type":1,"text":"I think that a sufficiently sophisticated Bayesian would probably be able to learn to trust X(A):","markups":[]},{"name":"d861","type":9,"text":"If f is performing well enough that we think itâ€™s more likely to be right in the future, then the Bayesian is going to end believing some claim like â€œthe predictions of f are goodâ€ (since it explains the data so well).","markups":[{"type":2,"start":3,"end":4},{"type":2,"start":169,"end":170}]},{"name":"d43a","type":9,"text":"This is a complicated statement, and without some kind of explanation this claim has a low prior probability (roughly decaying with the complexity of f). The Bayesian is motivated to find an explanation with higher prior probability.","markups":[{"type":2,"start":150,"end":152}]},{"name":"f4a1","type":9,"text":"The correspondence X can explain the constraints on the matrix A, in terms of facts that we already know about the world. This explanation may end up being simpler (or at least higher prior) than a direct enumeration of the constraints on A â€” I hope (and think itâ€™s plausible) that this happens iff weâ€™d actually believe on reflection that X(A) captures reality.\n(To the extent that we are uncertain and think Aâ€™s beliefs have a non-negligible chance of capturing reality, then hopefully we can capture that by the same mechanism by ending up with a non-degenerate posterior.)","markups":[]},{"name":"1aa8","type":9,"text":"Now the Bayesian is faced with at least two kinds of explanations:\n(a) â€œIf you use the constraints implied by correspondence X(A) + positive semidefiniteness, and then optimize log det, you get a matrix A for which X(A) makes good predictions,â€\n(b) â€œThe actual situation in the real world is described by positive semi-definite matrices with higher log determinant (under the correspondence X).â€","markups":[]},{"name":"bc6a","type":9,"text":"Explanation (b) is explaining two things at once: both why the optimization done by f respects the constraints on our beliefs, and why that optimization leads to good predictions. Hopefully this is simpler than making two separate bridging claims, one which explains f as respecting the constraints implied by X, and one which claims that f makes good predictions. Ideally, this 2-for-1 that favors (b) exactly mirrors the underlying reasoning that leads us to actually believe that X(A) is correct, rather than resembling what we know about reality and making good predictions â€œby coincidence.â€","markups":[{"type":2,"start":84,"end":85},{"type":2,"start":267,"end":268},{"type":2,"start":339,"end":340}]},{"name":"86e4","type":1,"text":"This is a pretty speculative discussion â€” itâ€™s not very careful, and itâ€™s hard to make it careful in part because I donâ€™t have a formalization of Bayesian reasoning that can even really be applied to this setting. But it seems to match my intuitions about what reasonable Bayesian reasoning â€œshouldâ€ do, which gives me a lot more optimism that a careful Bayesian would be able to epistemically dominate C.","markups":[{"type":1,"start":403,"end":404}]},{"name":"cfe4","type":13,"text":"Deliberation and self-improvement","markups":[]},{"name":"90c5","type":1,"text":"Often we expect the computation C to have accurate beliefs because it uses a strategy that appears to work in practice â€” the last 3 examples have discussed that case. But there are other reasons to trust a computation.","markups":[{"type":1,"start":32,"end":33}]},{"name":"6d74","type":1,"text":"For example, humans often write code and trust it (to some extent) even without extensive empirical testing â€” instead, we have a reason to think it will work, and need only modest testing to make sure that we havenâ€™t made an error in our implementation or reasoning. If I write an automated mathematician that works by finding proofs that pass a proof checker, I donâ€™t expect it to be correct because of the empirical record (Empirical data backs up some key assumptions, but isnâ€™t being used to directly establishing the correctness of the method.)","markups":[]},{"name":"cf51","type":1,"text":"Likewise, if we train a powerful agent, that agent might initially use strategies that work well in training, but over time it might use learned reasoning to identify other promising strategies and use those. Reasoning might allow it to totally skip empirical testing, or to adopt the method after much less testing than would have been necessary without the reasoning.","markups":[]},{"name":"32b2","type":1,"text":"To dominate the beliefs produced by such reasoning, we canâ€™t directly appeal to the kind of statistical inference made in the previous section. But in these cases I think we have access to an even more direct strategy.","markups":[]},{"name":"6a02","type":1,"text":"Concretely, consider the situation where C contains a process f that designs a new reasoning process g. Then:","markups":[{"type":1,"start":41,"end":42},{"type":2,"start":62,"end":64},{"type":2,"start":101,"end":102}]},{"name":"3ec5","type":9,"text":"From the outside, we trust g because we trust f and it trusts g.","markups":[{"type":2,"start":27,"end":28},{"type":2,"start":46,"end":48},{"type":2,"start":62,"end":64}]},{"name":"1b8f","type":9,"text":"An otherwise-universal reasoner A will dominate fâ€™s beliefs, and in particular if f is justified in thinking that g will work then A will believe that and understand why.","markups":[{"type":1,"start":32,"end":33},{"type":1,"start":131,"end":132},{"type":2,"start":48,"end":49},{"type":2,"start":82,"end":83},{"type":2,"start":114,"end":116}]},{"name":"abb7","type":9,"text":"Once we understand fâ€™s beliefs, dominating g is essentially another instance of the original ascription universality problem, but now from a slightly stronger epistemic state that involves both what ğ”¼ knows and what f knows. So unless our original approach to universality was tightly wedded to details of ğ”¼, we can probably dominate g.","markups":[{"type":2,"start":19,"end":21},{"type":2,"start":43,"end":45},{"type":2,"start":217,"end":218},{"type":2,"start":336,"end":337}]},{"name":"6019","type":1,"text":"At the end of the day weâ€™d like to put all of this together into a tight argument for universality, which will need to incorporate both statistical arguments and this kind of dynamic. But Iâ€™m tentatively optimistic about achieving universality in light of the prospect of agents designing new agents, and am much more worried about the kind of opaque computations that â€œjust workâ€ described in the last few sections.","markups":[]}],"sections":[{"name":"17dd","startIndex":0}]},"postDisplay":{"coverless":true},"metaDescription":"An attempt to formalize universality as â€œable to understand anything that any computation can understand.â€"},"virtuals":{"statusForCollection":"APPROVED","allowNotes":true,"previewImage":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"wordCount":5569,"imageCount":0,"readingTime":21.015094339622642,"subtitle":"An attempt to formalize universality as â€œable to understand anything that any computation can understand.â€","publishedInCount":1,"usersBySocialRecommends":[],"noIndex":false,"recommends":5,"socialRecommends":[],"isBookmarked":false,"tags":[{"slug":"machine-learning","name":"Machine Learning","postCount":69725,"metadata":{"postCount":69725,"coverImage":{"id":"1*LzExc8ocuimzZdJTP58g7w.png","originalWidth":4800,"originalHeight":3000,"isFeatured":true}},"type":"Tag"}],"socialRecommendsCount":0,"responsesCreatedCount":0,"links":{"entries":[{"url":"https://en.wikipedia.org/wiki/Alternating_Turing_machine","alts":[],"httpStatus":200},{"url":"https://www.lesswrong.com/posts/b3Bt9Cz4hEtR26ANX/knowledge-is-freedom","alts":[],"httpStatus":200},{"url":"https://medium.com/@weidai/to-put-it-another-way-a-human-translator-has-learned-a-lot-of-valuable-information-much-of-it-48457f95b9bf","alts":[{"type":3,"url":"medium://p/48457f95b9bf"},{"type":2,"url":"medium://p/48457f95b9bf"}],"httpStatus":200},{"url":"https://ai-alignment.com/universality-and-consequentialism-within-hch-c0bee00365bd","alts":[{"type":2,"url":"medium://p/c0bee00365bd"},{"type":3,"url":"medium://p/c0bee00365bd"}],"httpStatus":200},{"url":"https://ai-alignment.com/universality-and-security-amplification-551b314a3bab","alts":[{"type":2,"url":"medium://p/551b314a3bab"},{"type":3,"url":"medium://p/551b314a3bab"}],"httpStatus":200},{"url":"https://ai-alignment.com/humans-consulting-hch-f893f6051455","alts":[{"type":3,"url":"medium://p/f893f6051455"},{"type":2,"url":"medium://p/f893f6051455"}],"httpStatus":200},{"url":"https://ai-alignment.com/informed-oversight-18fcb5d3d1e1","alts":[{"type":2,"url":"medium://p/18fcb5d3d1e1"},{"type":3,"url":"medium://p/18fcb5d3d1e1"}],"httpStatus":200},{"url":"https://ai-alignment.com/approval-directed-algorithm-learning-bf1f8fad42cd","alts":[{"type":2,"url":"medium://p/bf1f8fad42cd"},{"type":3,"url":"medium://p/bf1f8fad42cd"}],"httpStatus":200},{"url":"https://ai-alignment.com/universality-and-model-based-rl-b08701394ddd","alts":[{"type":3,"url":"medium://p/b08701394ddd"},{"type":2,"url":"medium://p/b08701394ddd"}],"httpStatus":200},{"url":"https://arxiv.org/pdf/1810.08575.pdf","alts":[],"httpStatus":200},{"url":"https://arxiv.org/abs/1805.00899","alts":[],"httpStatus":200}],"version":"0.3","generatedAt":1547167194247},"isLockedPreviewOnly":false,"metaDescription":"An attempt to formalize universality as â€œable to understand anything that any computation can understand.â€","totalClapCount":60,"sectionCount":1,"readingList":0,"topics":[]},"coverless":true,"slug":"towards-formalizing-universality","translationSourcePostId":"","translationSourceCreatorId":"","isApprovedTranslation":false,"inResponseToPostId":"","inResponseToRemovedAt":0,"isTitleSynthesized":false,"allowResponses":true,"importedUrl":"","importedPublishedAt":0,"visibility":0,"uniqueSlug":"towards-formalizing-universality-409ab893a456","previewContent":{"bodyModel":{"paragraphs":[{"name":"previewTitle","type":3,"text":"Towards formalizing universality","alignment":1},{"name":"previewSubtitle","type":13,"text":"An attempt to formalize universality as â€œable to understand anything that any computation can understand.â€","alignment":1}],"sections":[{"startIndex":0}]},"isFullContent":false,"subtitle":"An attempt to formalize universality as â€œable to understand anything that any computation can understand.â€"},"license":0,"inResponseToMediaResourceId":"","canonicalUrl":"https://ai-alignment.com/towards-formalizing-universality-409ab893a456","approvedHomeCollectionId":"624d886c4aa4","approvedHomeCollection":{"id":"624d886c4aa4","name":"AI Alignment","slug":"ai-control","tags":[],"creatorId":"57f1a655a613","description":"Aligning AI systems with human interests.","shortDescription":"Aligning AI systems with human interests.","image":{"imageId":"1*N56Qc5-aHTcfGff0scntKQ.png","filter":"","backgroundSize":"","originalWidth":512,"originalHeight":512,"strategy":"resample","height":0,"width":0},"metadata":{"followerCount":2834,"activeAt":1548040822588},"virtuals":{"permissions":{"canPublish":false,"canPublishAll":false,"canRepublish":false,"canRemove":false,"canManageAll":false,"canSubmit":false,"canEditPosts":false,"canAddWriters":false,"canViewStats":false,"canSendNewsletter":false,"canViewLockedPosts":false,"canViewCloaked":false,"canEditOwnPosts":false,"canBeAssignedAuthor":false,"canEnrollInHightower":false,"canLockPostsForMediumMembers":false,"canLockOwnPostsForMediumMembers":false},"isSubscribed":false,"isNewsletterSubscribed":false,"isEnrolledInHightower":false,"isEligibleForHightower":false},"logo":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"collectionMastheadId":"29f3dcc2e4","domain":"ai-alignment.com","sections":[{"type":2,"collectionHeaderMetadata":{"title":"AI Alignment","backgroundImage":{},"logoImage":{},"alignment":2,"layout":4}},{"type":1,"postListMetadata":{"source":3,"layout":4,"number":3,"postIds":["157debfd1616","b49ad992940b","b959644d79c2"]}},{"type":1,"postListMetadata":{"source":1,"layout":4,"number":24,"postIds":[],"sectionHeader":"Latest"}}],"favicon":{"imageId":"1*cciPf4CUXd_Zyux0Jg0yBQ.png","filter":"","backgroundSize":"","originalWidth":400,"originalHeight":400,"strategy":"resample","height":0,"width":0},"colorPalette":{"defaultBackgroundSpectrum":{"colorPoints":[{"color":"#FF02B875","point":0},{"color":"#FF00AB6B","point":0.1},{"color":"#FF1C9963","point":0.2},{"color":"#FF092E20","point":1}],"backgroundColor":"#FFFFFFFF"},"highlightSpectrum":{"colorPoints":[{"color":"#FFFFFFFF","point":0},{"color":"#FFE9FDF0","point":0.1},{"color":"#FFE2FAEE","point":0.2},{"color":"#FFADFFCF","point":0.6},{"color":"#FF7DFFB3","point":1}],"backgroundColor":"#FFFFFFFF"}},"navItems":[],"colorBehavior":1,"instantArticlesState":0,"acceleratedMobilePagesState":0,"ampLogo":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"header":{"title":"AI Alignment","backgroundImage":{},"logoImage":{},"alignment":2,"layout":4},"paidForDomainAt":1490733089988,"type":"Collection"},"newsletterId":"","webCanonicalUrl":"https://ai-alignment.com/towards-formalizing-universality-409ab893a456","mediumUrl":"https://ai-alignment.com/towards-formalizing-universality-409ab893a456","migrationId":"","notifyFollowers":true,"notifyTwitter":false,"notifyFacebook":false,"responseHiddenOnParentPostAt":0,"isSeries":false,"isSubscriptionLocked":false,"seriesLastAppendedAt":0,"audioVersionDurationSec":0,"sequenceId":"","isNsfw":false,"isEligibleForRevenue":false,"isBlockedFromHightower":false,"deletedAt":0,"lockedPostSource":0,"hightowerMinimumGuaranteeStartsAt":0,"hightowerMinimumGuaranteeEndsAt":0,"featureLockRequestAcceptedAt":0,"mongerRequestType":1,"layerCake":0,"socialTitle":"","socialDek":"","editorialPreviewTitle":"","editorialPreviewDek":"","curationEligibleAt":0,"type":"Post"},"mentionedUsers":[],"collaborators":[],"hideMeter":false,"collectionUserRelations":[],"mode":null,"references":{"User":{"57f1a655a613":{"userId":"57f1a655a613","name":"Paul Christiano","username":"paulfchristiano","createdAt":1417286353352,"imageId":"1*BNjZCuQuRfIgcXCBMipuBw.jpeg","backgroundImageId":"","bio":"OpenAI","twitterScreenName":"","socialStats":{"userId":"57f1a655a613","usersFollowedCount":93,"usersFollowedByCount":821,"type":"SocialStats"},"social":{"userId":"lo_JuU2p2EvGepj","targetUserId":"57f1a655a613","type":"Social"},"facebookAccountId":"1167284919","allowNotes":1,"mediumMemberAt":0,"isNsfw":false,"isWriterProgramEnrolled":true,"isQuarantined":false,"type":"User"}},"Collection":{"624d886c4aa4":{"id":"624d886c4aa4","name":"AI Alignment","slug":"ai-control","tags":[],"creatorId":"57f1a655a613","description":"Aligning AI systems with human interests.","shortDescription":"Aligning AI systems with human interests.","image":{"imageId":"1*N56Qc5-aHTcfGff0scntKQ.png","filter":"","backgroundSize":"","originalWidth":512,"originalHeight":512,"strategy":"resample","height":0,"width":0},"metadata":{"followerCount":2834,"activeAt":1548040822588},"virtuals":{"permissions":{"canPublish":false,"canPublishAll":false,"canRepublish":false,"canRemove":false,"canManageAll":false,"canSubmit":false,"canEditPosts":false,"canAddWriters":false,"canViewStats":false,"canSendNewsletter":false,"canViewLockedPosts":false,"canViewCloaked":false,"canEditOwnPosts":false,"canBeAssignedAuthor":false,"canEnrollInHightower":false,"canLockPostsForMediumMembers":false,"canLockOwnPostsForMediumMembers":false},"isSubscribed":false,"isNewsletterSubscribed":false,"isEnrolledInHightower":false,"isEligibleForHightower":false},"logo":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"collectionMastheadId":"29f3dcc2e4","domain":"ai-alignment.com","sections":[{"type":2,"collectionHeaderMetadata":{"title":"AI Alignment","backgroundImage":{},"logoImage":{},"alignment":2,"layout":4}},{"type":1,"postListMetadata":{"source":3,"layout":4,"number":3,"postIds":["157debfd1616","b49ad992940b","b959644d79c2"]}},{"type":1,"postListMetadata":{"source":1,"layout":4,"number":24,"postIds":[],"sectionHeader":"Latest"}}],"favicon":{"imageId":"1*cciPf4CUXd_Zyux0Jg0yBQ.png","filter":"","backgroundSize":"","originalWidth":400,"originalHeight":400,"strategy":"resample","height":0,"width":0},"colorPalette":{"defaultBackgroundSpectrum":{"colorPoints":[{"color":"#FF02B875","point":0},{"color":"#FF00AB6B","point":0.1},{"color":"#FF1C9963","point":0.2},{"color":"#FF092E20","point":1}],"backgroundColor":"#FFFFFFFF"},"highlightSpectrum":{"colorPoints":[{"color":"#FFFFFFFF","point":0},{"color":"#FFE9FDF0","point":0.1},{"color":"#FFE2FAEE","point":0.2},{"color":"#FFADFFCF","point":0.6},{"color":"#FF7DFFB3","point":1}],"backgroundColor":"#FFFFFFFF"}},"navItems":[],"colorBehavior":1,"instantArticlesState":0,"acceleratedMobilePagesState":0,"ampLogo":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"header":{"title":"AI Alignment","backgroundImage":{},"logoImage":{},"alignment":2,"layout":4},"paidForDomainAt":1490733089988,"type":"Collection"}},"Social":{"57f1a655a613":{"userId":"lo_JuU2p2EvGepj","targetUserId":"57f1a655a613","type":"Social"}},"SocialStats":{"57f1a655a613":{"userId":"57f1a655a613","usersFollowedCount":93,"usersFollowedByCount":821,"type":"SocialStats"}}}})
// ]]></script><script id="parsely-cfg" src="//d1z2jf7jlzjs58.cloudfront.net/keys/medium.com/p.js"></script><script type="text/javascript">(function(b,r,a,n,c,h,_,s,d,k){if(!b[n]||!b[n]._q){for(;s<_.length;)c(h,_[s++]);d=r.createElement(a);d.async=1;d.src="https://cdn.branch.io/branch-latest.min.js";k=r.getElementsByTagName(a)[0];k.parentNode.insertBefore(d,k);b[n]=h}})(window,document,"script","branch",function(b,r){b[r]=function(){b._q.push([r,arguments])}},{_q:[],_v:1},"addListener applyCode autoAppIndex banner closeBanner closeJourney creditHistory credits data deepview deepviewCta first getCode init link logout redeem referrals removeListener sendSMS setBranchViewData setIdentity track validateCode trackCommerceEvent logEvent".split(" "), 0); branch.init('key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm', {'no_journeys': true, 'disable_exit_animation': true, 'disable_entry_animation': true, 'tracking_disabled':  false }, function(err, data) {});</script><div class="surface-scrollOverlay"></div><script charset="UTF-8" src="https://cdn-static-1.medium.com/_/fp/gen-js/main-common-async.bundle.qFZkgzLZ5TYXIerh_w9awQ.js"></script><script charset="UTF-8" src="https://cdn-static-1.medium.com/_/fp/gen-js/main-notes.bundle.V05mXLtyLz2Mj5DzEML26A.js"></script></body></html>